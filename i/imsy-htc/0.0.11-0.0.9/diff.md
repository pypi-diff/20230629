# Comparing `tmp/imsy_htc-0.0.11-cp39-cp39-win_amd64.whl.zip` & `tmp/imsy_htc-0.0.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,192 +1,187 @@
-Zip file size: 1902365 bytes, number of entries: 190
--rw-rw-rw-  2.0 fat    13182 b- defN 23-Jun-29 10:36 htc/__init__.py
--rw-rw-rw-  2.0 fat   318464 b- defN 23-Jun-29 10:39 htc/_cpp.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     3209 b- defN 23-Jun-29 10:36 htc/cli.py
--rw-rw-rw-  2.0 fat     2674 b- defN 23-Jun-29 10:36 htc/run_info.py
--rw-rw-rw-  2.0 fat    28439 b- defN 23-Jun-29 10:36 htc/settings.py
--rw-rw-rw-  2.0 fat     7296 b- defN 23-Jun-29 10:36 htc/settings_seg.py
--rw-rw-rw-  2.0 fat     6035 b- defN 23-Jun-29 10:36 htc/cpp/ParallelExecution.h
--rw-rw-rw-  2.0 fat    15717 b- defN 23-Jun-29 10:36 htc/cpp/__init__.py
--rw-rw-rw-  2.0 fat     2615 b- defN 23-Jun-29 10:36 htc/cpp/bindings.cpp
--rw-rw-rw-  2.0 fat    11886 b- defN 23-Jun-29 10:36 htc/cpp/colorchecker_automask.cpp
--rw-rw-rw-  2.0 fat     1743 b- defN 23-Jun-29 10:36 htc/cpp/evaluate_superpixels.cpp
--rw-rw-rw-  2.0 fat     5061 b- defN 23-Jun-29 10:36 htc/cpp/hierarchical_bootstrapping.h
--rw-rw-rw-  2.0 fat     3070 b- defN 23-Jun-29 10:36 htc/cpp/kfold_combinations.cpp
--rw-rw-rw-  2.0 fat     1084 b- defN 23-Jun-29 10:36 htc/cpp/map_label_image.cpp
--rw-rw-rw-  2.0 fat     2056 b- defN 23-Jun-29 10:36 htc/cpp/nunique.cpp
--rw-rw-rw-  2.0 fat      924 b- defN 23-Jun-29 10:36 htc/cpp/segmentation_mask.cpp
--rw-rw-rw-  2.0 fat     1681 b- defN 23-Jun-29 10:36 htc/cpp/tensor_mapping.h
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/data_exploration/__init__.py
--rw-rw-rw-  2.0 fat      939 b- defN 23-Jun-29 10:36 htc/data_processing/DatasetIteration.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/data_processing/__init__.py
--rw-rw-rw-  2.0 fat     2293 b- defN 23-Jun-29 10:36 htc/data_processing/run_l1_normalization.py
--rw-rw-rw-  2.0 fat     8966 b- defN 23-Jun-29 10:36 htc/data_processing/run_median_spectra.py
--rw-rw-rw-  2.0 fat     2569 b- defN 23-Jun-29 10:36 htc/data_processing/run_parameter_images.py
--rw-rw-rw-  2.0 fat     2229 b- defN 23-Jun-29 10:36 htc/data_processing/run_raw16.py
--rw-rw-rw-  2.0 fat     4263 b- defN 23-Jun-29 10:36 htc/data_processing/run_standardization.py
--rw-rw-rw-  2.0 fat     2102 b- defN 23-Jun-29 10:36 htc/data_processing/run_superpixel_prediction.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/evaluation/__init__.py
--rw-rw-rw-  2.0 fat     3350 b- defN 23-Jun-29 10:36 htc/evaluation/analyze_tfevents.py
--rw-rw-rw-  2.0 fat    21726 b- defN 23-Jun-29 10:36 htc/evaluation/evaluate_images.py
--rw-rw-rw-  2.0 fat     3052 b- defN 23-Jun-29 10:36 htc/evaluation/evaluate_superpixels.py
--rw-rw-rw-  2.0 fat     7232 b- defN 23-Jun-29 10:36 htc/evaluation/ranking.py
--rw-rw-rw-  2.0 fat     4761 b- defN 23-Jun-29 10:36 htc/evaluation/run_compare_runs.py
--rw-rw-rw-  2.0 fat     6675 b- defN 23-Jun-29 10:36 htc/evaluation/run_ranking_lr.py
--rw-rw-rw-  2.0 fat    20200 b- defN 23-Jun-29 10:36 htc/evaluation/run_table_generation.py
--rw-rw-rw-  2.0 fat     5907 b- defN 23-Jun-29 10:36 htc/evaluation/utils.py
--rw-rw-rw-  2.0 fat     9292 b- defN 23-Jun-29 10:36 htc/evaluation/metrics/ECE.py
--rw-rw-rw-  2.0 fat     6825 b- defN 23-Jun-29 10:36 htc/evaluation/metrics/NSDToleranceEstimation.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/evaluation/metrics/__init__.py
--rw-rw-rw-  2.0 fat     5619 b- defN 23-Jun-29 10:36 htc/evaluation/metrics/scores.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/evaluation/model_comparison/__init__.py
--rw-rw-rw-  2.0 fat     1790 b- defN 23-Jun-29 10:36 htc/evaluation/model_comparison/dataset_size.py
--rw-rw-rw-  2.0 fat     3741 b- defN 23-Jun-29 10:36 htc/evaluation/model_comparison/paper_runs.py
--rw-rw-rw-  2.0 fat     4915 b- defN 23-Jun-29 10:36 htc/evaluation/model_comparison/run_challengeR_table.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/fonts/__init__.py
--rw-rw-rw-  2.0 fat     1490 b- defN 23-Jun-29 10:36 htc/fonts/set_font.py
--rw-rw-rw-  2.0 fat     6439 b- defN 23-Jun-29 10:36 htc/model_processing/ImageConsumer.py
--rw-rw-rw-  2.0 fat     3146 b- defN 23-Jun-29 10:36 htc/model_processing/Predictor.py
--rw-rw-rw-  2.0 fat    16176 b- defN 23-Jun-29 10:36 htc/model_processing/Runner.py
--rw-rw-rw-  2.0 fat     6354 b- defN 23-Jun-29 10:36 htc/model_processing/TestLeaveOneOutPredictor.py
--rw-rw-rw-  2.0 fat     5856 b- defN 23-Jun-29 10:36 htc/model_processing/TestPredictor.py
--rw-rw-rw-  2.0 fat     5972 b- defN 23-Jun-29 10:36 htc/model_processing/ValidationPredictor.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/model_processing/__init__.py
--rw-rw-rw-  2.0 fat     4593 b- defN 23-Jun-29 10:36 htc/model_processing/run_image_figures.py
--rw-rw-rw-  2.0 fat     3263 b- defN 23-Jun-29 10:36 htc/model_processing/run_inference.py
--rw-rw-rw-  2.0 fat     3988 b- defN 23-Jun-29 10:36 htc/model_processing/run_multiple.py
--rw-rw-rw-  2.0 fat    13957 b- defN 23-Jun-29 10:36 htc/model_processing/run_tables.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/__init__.py
--rw-rw-rw-  2.0 fat     6128 b- defN 23-Jun-29 10:36 htc/models/run_generate_configs.py
--rw-rw-rw-  2.0 fat     2997 b- defN 23-Jun-29 10:36 htc/models/run_prepare_pretrained.py
--rw-rw-rw-  2.0 fat    16861 b- defN 23-Jun-29 10:36 htc/models/run_training.py
--rw-rw-rw-  2.0 fat     1016 b- defN 23-Jun-29 10:36 htc/models/common/ChannelWeights.py
--rw-rw-rw-  2.0 fat     4533 b- defN 23-Jun-29 10:36 htc/models/common/EvaluationMixin.py
--rw-rw-rw-  2.0 fat     2838 b- defN 23-Jun-29 10:36 htc/models/common/ForwardHookPromise.py
--rw-rw-rw-  2.0 fat     5543 b- defN 23-Jun-29 10:36 htc/models/common/HSI3dChannel.py
--rw-rw-rw-  2.0 fat    18281 b- defN 23-Jun-29 10:36 htc/models/common/HTCDataset.py
--rw-rw-rw-  2.0 fat     8463 b- defN 23-Jun-29 10:36 htc/models/common/HTCDatasetStream.py
--rw-rw-rw-  2.0 fat    10657 b- defN 23-Jun-29 10:36 htc/models/common/HTCLightning.py
--rw-rw-rw-  2.0 fat    35429 b- defN 23-Jun-29 10:36 htc/models/common/HTCModel.py
--rw-rw-rw-  2.0 fat     1526 b- defN 23-Jun-29 10:36 htc/models/common/Heads.py
--rw-rw-rw-  2.0 fat     6209 b- defN 23-Jun-29 10:36 htc/models/common/HierarchicalSampler.py
--rw-rw-rw-  2.0 fat    16349 b- defN 23-Jun-29 10:36 htc/models/common/MetricAggregation.py
--rw-rw-rw-  2.0 fat     6680 b- defN 23-Jun-29 10:36 htc/models/common/SharedMemoryDatasetMixin.py
--rw-rw-rw-  2.0 fat     8741 b- defN 23-Jun-29 10:36 htc/models/common/StreamDataLoader.py
--rw-rw-rw-  2.0 fat     2062 b- defN 23-Jun-29 10:36 htc/models/common/StreamImageDataLoader.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/common/__init__.py
--rw-rw-rw-  2.0 fat     2556 b- defN 23-Jun-29 10:36 htc/models/common/class_weights.py
--rw-rw-rw-  2.0 fat     5942 b- defN 23-Jun-29 10:36 htc/models/common/distance_correlation.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-29 10:36 htc/models/common/functions.py
--rw-rw-rw-  2.0 fat     3550 b- defN 23-Jun-29 10:36 htc/models/common/loss.py
--rw-rw-rw-  2.0 fat     9736 b- defN 23-Jun-29 10:36 htc/models/common/torch_helpers.py
--rw-rw-rw-  2.0 fat    20189 b- defN 23-Jun-29 10:36 htc/models/common/transforms.py
--rw-rw-rw-  2.0 fat    11886 b- defN 23-Jun-29 10:36 htc/models/common/utils.py
--rw-rw-rw-  2.0 fat    11652 b- defN 23-Jun-29 10:36 htc/models/data/DataSpecification.py
--rw-rw-rw-  2.0 fat     1231 b- defN 23-Jun-29 10:36 htc/models/data/SpecsGeneration.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/data/__init__.py
--rw-rw-rw-  2.0 fat   117848 b- defN 23-Jun-29 10:36 htc/models/data/pigs_semantic-only_5foldsV2.json
--rw-rw-rw-  2.0 fat   961680 b- defN 23-Jun-29 10:36 htc/models/data/pigs_semantic-only_dataset-size_repetitions=5V2.json
--rw-rw-rw-  2.0 fat     9003 b- defN 23-Jun-29 10:36 htc/models/data/run_pig_dataset.py
--rw-rw-rw-  2.0 fat     6889 b- defN 23-Jun-29 10:36 htc/models/data/run_size_dataset.py
--rw-rw-rw-  2.0 fat     4130 b- defN 23-Jun-29 10:36 htc/models/image/DatasetImage.py
--rw-rw-rw-  2.0 fat     4348 b- defN 23-Jun-29 10:36 htc/models/image/DatasetImageBatch.py
--rw-rw-rw-  2.0 fat     3749 b- defN 23-Jun-29 10:36 htc/models/image/DatasetImageStream.py
--rw-rw-rw-  2.0 fat     9907 b- defN 23-Jun-29 10:36 htc/models/image/LightningImage.py
--rw-rw-rw-  2.0 fat     1406 b- defN 23-Jun-29 10:36 htc/models/image/ModelImage.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/image/__init__.py
--rw-rw-rw-  2.0 fat     1779 b- defN 23-Jun-29 10:36 htc/models/image/configs/default.json
--rw-rw-rw-  2.0 fat      128 b- defN 23-Jun-29 10:36 htc/models/image/configs/default_parameters.json
--rw-rw-rw-  2.0 fat      114 b- defN 23-Jun-29 10:36 htc/models/image/configs/default_rgb.json
--rw-rw-rw-  2.0 fat      304 b- defN 23-Jun-29 10:36 htc/models/image/configs/spxs.json
--rw-rw-rw-  2.0 fat     3771 b- defN 23-Jun-29 10:36 htc/models/patch/DatasetPatchImage.py
--rw-rw-rw-  2.0 fat    10815 b- defN 23-Jun-29 10:36 htc/models/patch/DatasetPatchStream.py
--rw-rw-rw-  2.0 fat     2050 b- defN 23-Jun-29 10:36 htc/models/patch/LightningPatch.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/patch/__init__.py
--rw-rw-rw-  2.0 fat     1938 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default.json
--rw-rw-rw-  2.0 fat     1937 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default_64.json
--rw-rw-rw-  2.0 fat      131 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default_64_parameters.json
--rw-rw-rw-  2.0 fat      117 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default_64_rgb.json
--rw-rw-rw-  2.0 fat      128 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default_parameters.json
--rw-rw-rw-  2.0 fat      114 b- defN 23-Jun-29 10:36 htc/models/patch/configs/default_rgb.json
--rw-rw-rw-  2.0 fat     4941 b- defN 23-Jun-29 10:36 htc/models/pixel/DatasetPixelStream.py
--rw-rw-rw-  2.0 fat     3891 b- defN 23-Jun-29 10:36 htc/models/pixel/LightningPixel.py
--rw-rw-rw-  2.0 fat     4573 b- defN 23-Jun-29 10:36 htc/models/pixel/ModelPixel.py
--rw-rw-rw-  2.0 fat     2100 b- defN 23-Jun-29 10:36 htc/models/pixel/ModelPixelRGB.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/pixel/__init__.py
--rw-rw-rw-  2.0 fat     1785 b- defN 23-Jun-29 10:36 htc/models/pixel/configs/default.json
--rw-rw-rw-  2.0 fat      191 b- defN 23-Jun-29 10:36 htc/models/pixel/configs/default_parameters.json
--rw-rw-rw-  2.0 fat      177 b- defN 23-Jun-29 10:36 htc/models/pixel/configs/default_rgb.json
--rw-rw-rw-  2.0 fat     2615 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/DatasetSuperpixelImage.py
--rw-rw-rw-  2.0 fat     4098 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/DatasetSuperpixelStream.py
--rw-rw-rw-  2.0 fat     3493 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/LightningSuperpixelClassification.py
--rw-rw-rw-  2.0 fat     1886 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/ModelSuperpixelClassification.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/__init__.py
--rw-rw-rw-  2.0 fat     1966 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/configs/default.json
--rw-rw-rw-  2.0 fat      128 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/configs/default_parameters.json
--rw-rw-rw-  2.0 fat      114 b- defN 23-Jun-29 10:36 htc/models/superpixel_classification/configs/default_rgb.json
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/rater_variability/__init__.py
--rw-rw-rw-  2.0 fat     4492 b- defN 23-Jun-29 10:36 htc/rater_variability/rater_evaluation.py
--rw-rw-rw-  2.0 fat     3331 b- defN 23-Jun-29 10:36 htc/rater_variability/run_nsd_thresholds.py
--rw-rw-rw-  2.0 fat     2574 b- defN 23-Jun-29 10:36 htc/tissue_atlas/MetricAggregationClassification.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/tissue_atlas/__init__.py
--rw-rw-rw-  2.0 fat     2487 b- defN 23-Jun-29 10:36 htc/tissue_atlas/run_test_table_generation.py
--rw-rw-rw-  2.0 fat     5564 b- defN 23-Jun-29 10:36 htc/tissue_atlas/settings_atlas.py
--rw-rw-rw-  2.0 fat     2176 b- defN 23-Jun-29 10:36 htc/tissue_atlas/tables.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/tissue_atlas/data/__init__.py
--rw-rw-rw-  2.0 fat     3377 b- defN 23-Jun-29 10:36 htc/tissue_atlas/data/run_tissue_atlas_dataset.py
--rw-rw-rw-  2.0 fat 15656801 b- defN 23-Jun-29 10:36 htc/tissue_atlas/data/tissue-atlas_loocv_test-8_seed-0_cam-118.json
--rw-rw-rw-  2.0 fat     2419 b- defN 23-Jun-29 10:36 htc/tissue_atlas/median_pixel/DatasetMedianPixel.py
--rw-rw-rw-  2.0 fat     6207 b- defN 23-Jun-29 10:36 htc/tissue_atlas/median_pixel/LightningMedianPixel.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/tissue_atlas/median_pixel/__init__.py
--rw-rw-rw-  2.0 fat     1252 b- defN 23-Jun-29 10:36 htc/tissue_atlas/median_pixel/configs/default.json
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/tissue_atlas_open/__init__.py
--rw-rw-rw-  2.0 fat    10646 b- defN 23-Jun-29 10:36 htc/tissue_atlas_open/profiles.py
--rw-rw-rw-  2.0 fat    11507 b- defN 23-Jun-29 10:36 htc/tissue_atlas_open/run_label_profiles.py
--rw-rw-rw-  2.0 fat     3290 b- defN 23-Jun-29 10:36 htc/tissue_atlas_open/run_readme_gif.py
--rw-rw-rw-  2.0 fat    55747 b- defN 23-Jun-29 10:36 htc/tivita/DataPath.py
--rw-rw-rw-  2.0 fat     3867 b- defN 23-Jun-29 10:36 htc/tivita/DataPathMultiorgan.py
--rw-rw-rw-  2.0 fat     5372 b- defN 23-Jun-29 10:36 htc/tivita/DataPathReference.py
--rw-rw-rw-  2.0 fat     6036 b- defN 23-Jun-29 10:36 htc/tivita/DataPathSepsis.py
--rw-rw-rw-  2.0 fat     2280 b- defN 23-Jun-29 10:36 htc/tivita/DataPathTivita.py
--rw-rw-rw-  2.0 fat     7026 b- defN 23-Jun-29 10:36 htc/tivita/DatasetSettings.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/tivita/__init__.py
--rw-rw-rw-  2.0 fat     1288 b- defN 23-Jun-29 10:36 htc/tivita/colorscale.py
--rw-rw-rw-  2.0 fat     3536 b- defN 23-Jun-29 10:36 htc/tivita/hsi.py
--rw-rw-rw-  2.0 fat     5550 b- defN 23-Jun-29 10:36 htc/tivita/metadata.py
--rw-rw-rw-  2.0 fat     4033 b- defN 23-Jun-29 10:36 htc/tivita/rgb.py
--rw-rw-rw-  2.0 fat     1634 b- defN 23-Jun-29 10:36 htc/utils/AdvancedJSONEncoder.py
--rw-rw-rw-  2.0 fat    14564 b- defN 23-Jun-29 10:36 htc/utils/ColorcheckerReader.py
--rw-rw-rw-  2.0 fat     2357 b- defN 23-Jun-29 10:36 htc/utils/ColoredFileLog.py
--rw-rw-rw-  2.0 fat    19181 b- defN 23-Jun-29 10:36 htc/utils/Config.py
--rw-rw-rw-  2.0 fat    19046 b- defN 23-Jun-29 10:36 htc/utils/Datasets.py
--rw-rw-rw-  2.0 fat     2266 b- defN 23-Jun-29 10:36 htc/utils/DelayedFileHandler.py
--rw-rw-rw-  2.0 fat     7615 b- defN 23-Jun-29 10:36 htc/utils/DomainMapper.py
--rw-rw-rw-  2.0 fat      466 b- defN 23-Jun-29 10:36 htc/utils/DuplicateFilter.py
--rw-rw-rw-  2.0 fat     3549 b- defN 23-Jun-29 10:36 htc/utils/LDA.py
--rw-rw-rw-  2.0 fat    16950 b- defN 23-Jun-29 10:36 htc/utils/LabelMapping.py
--rw-rw-rw-  2.0 fat     1238 b- defN 23-Jun-29 10:36 htc/utils/MeasureTime.py
--rw-rw-rw-  2.0 fat    16469 b- defN 23-Jun-29 10:36 htc/utils/MultiPath.py
--rw-rw-rw-  2.0 fat     4984 b- defN 23-Jun-29 10:36 htc/utils/SLICWrapper.py
--rw-rw-rw-  2.0 fat     8575 b- defN 23-Jun-29 10:36 htc/utils/SpectrometerReader.py
--rw-rw-rw-  2.0 fat      112 b- defN 23-Jun-29 10:36 htc/utils/__init__.py
--rw-rw-rw-  2.0 fat     3236 b- defN 23-Jun-29 10:36 htc/utils/blosc_compression.py
--rw-rw-rw-  2.0 fat     3623 b- defN 23-Jun-29 10:36 htc/utils/colors.py
--rw-rw-rw-  2.0 fat     3165 b- defN 23-Jun-29 10:36 htc/utils/file_transfer.py
--rw-rw-rw-  2.0 fat     6338 b- defN 23-Jun-29 10:36 htc/utils/general.py
--rw-rw-rw-  2.0 fat    34672 b- defN 23-Jun-29 10:36 htc/utils/helper_functions.py
--rw-rw-rw-  2.0 fat     1748 b- defN 23-Jun-29 10:36 htc/utils/import_extra.py
--rw-rw-rw-  2.0 fat     4385 b- defN 23-Jun-29 10:36 htc/utils/parallel.py
--rw-rw-rw-  2.0 fat     5323 b- defN 23-Jun-29 10:36 htc/utils/paths.py
--rw-rw-rw-  2.0 fat     9879 b- defN 23-Jun-29 10:36 htc/utils/renderjson.js
--rw-rw-rw-  2.0 fat     2644 b- defN 23-Jun-29 10:36 htc/utils/run_system_monitor.py
--rw-rw-rw-  2.0 fat     8301 b- defN 23-Jun-29 10:36 htc/utils/sqldf.py
--rw-rw-rw-  2.0 fat     2170 b- defN 23-Jun-29 10:36 htc/utils/type_from_string.py
--rw-rw-rw-  2.0 fat     1373 b- defN 23-Jun-29 10:36 htc/utils/unify_path.py
--rw-rw-rw-  2.0 fat    89202 b- defN 23-Jun-29 10:36 htc/utils/visualization.py
--rw-rw-rw-  2.0 fat      211 b- defN 23-Jun-29 10:39 imsy_htc-0.0.11.dist-info/LICENSE.md
--rw-rw-rw-  2.0 fat    32517 b- defN 23-Jun-29 10:39 imsy_htc-0.0.11.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-29 10:39 imsy_htc-0.0.11.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       37 b- defN 23-Jun-29 10:38 imsy_htc-0.0.11.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        4 b- defN 23-Jun-29 10:38 imsy_htc-0.0.11.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    17277 b- defN 23-Jun-29 10:39 imsy_htc-0.0.11.dist-info/RECORD
-190 files, 18179118 bytes uncompressed, 1874891 bytes compressed:  89.7%
+Zip file size: 1855023 bytes, number of entries: 185
+-rw-rw-rw-  2.0 fat    13182 b- defN 23-Jan-25 22:30 htc/__init__.py
+-rw-rw-rw-  2.0 fat   280576 b- defN 23-Jan-25 22:35 htc/_cpp.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     3209 b- defN 23-Jan-25 22:30 htc/cli.py
+-rw-rw-rw-  2.0 fat     2670 b- defN 23-Jan-25 22:30 htc/run_info.py
+-rw-rw-rw-  2.0 fat    22332 b- defN 23-Jan-25 22:30 htc/settings.py
+-rw-rw-rw-  2.0 fat     6988 b- defN 23-Jan-25 22:30 htc/settings_seg.py
+-rw-rw-rw-  2.0 fat    11041 b- defN 23-Jan-25 22:30 htc/cpp/__init__.py
+-rw-rw-rw-  2.0 fat     1777 b- defN 23-Jan-25 22:30 htc/cpp/bindings.cpp
+-rw-rw-rw-  2.0 fat     1743 b- defN 23-Jan-25 22:30 htc/cpp/evaluate_superpixels.cpp
+-rw-rw-rw-  2.0 fat     2260 b- defN 23-Jan-25 22:30 htc/cpp/hierarchical_bootstrapping.h
+-rw-rw-rw-  2.0 fat     3070 b- defN 23-Jan-25 22:30 htc/cpp/kfold_combinations.cpp
+-rw-rw-rw-  2.0 fat     1084 b- defN 23-Jan-25 22:30 htc/cpp/map_label_image.cpp
+-rw-rw-rw-  2.0 fat     2056 b- defN 23-Jan-25 22:30 htc/cpp/nunique.cpp
+-rw-rw-rw-  2.0 fat      924 b- defN 23-Jan-25 22:30 htc/cpp/segmentation_mask.cpp
+-rw-rw-rw-  2.0 fat     1681 b- defN 23-Jan-25 22:30 htc/cpp/tensor_mapping.h
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/data_exploration/__init__.py
+-rw-rw-rw-  2.0 fat      939 b- defN 23-Jan-25 22:30 htc/data_processing/DatasetIteration.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/data_processing/__init__.py
+-rw-rw-rw-  2.0 fat     2289 b- defN 23-Jan-25 22:30 htc/data_processing/run_l1_normalization.py
+-rw-rw-rw-  2.0 fat     7514 b- defN 23-Jan-25 22:30 htc/data_processing/run_median_spectra.py
+-rw-rw-rw-  2.0 fat     2565 b- defN 23-Jan-25 22:30 htc/data_processing/run_parameter_images.py
+-rw-rw-rw-  2.0 fat     4297 b- defN 23-Jan-25 22:30 htc/data_processing/run_standardization.py
+-rw-rw-rw-  2.0 fat     2056 b- defN 23-Jan-25 22:30 htc/data_processing/run_superpixel_prediction.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/evaluation/__init__.py
+-rw-rw-rw-  2.0 fat     3362 b- defN 23-Jan-25 22:30 htc/evaluation/analyze_tfevents.py
+-rw-rw-rw-  2.0 fat    17944 b- defN 23-Jan-25 22:30 htc/evaluation/evaluate_images.py
+-rw-rw-rw-  2.0 fat     3052 b- defN 23-Jan-25 22:30 htc/evaluation/evaluate_superpixels.py
+-rw-rw-rw-  2.0 fat     4761 b- defN 23-Jan-25 22:30 htc/evaluation/run_compare_runs.py
+-rw-rw-rw-  2.0 fat     6675 b- defN 23-Jan-25 22:30 htc/evaluation/run_ranking_lr.py
+-rw-rw-rw-  2.0 fat    18222 b- defN 23-Jan-25 22:30 htc/evaluation/run_table_generation.py
+-rw-rw-rw-  2.0 fat     7350 b- defN 23-Jan-25 22:30 htc/evaluation/metrics/ECELoss.py
+-rw-rw-rw-  2.0 fat     6825 b- defN 23-Jan-25 22:30 htc/evaluation/metrics/NSDToleranceEstimation.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/evaluation/metrics/__init__.py
+-rw-rw-rw-  2.0 fat     5619 b- defN 23-Jan-25 22:30 htc/evaluation/metrics/scores.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/evaluation/model_comparison/__init__.py
+-rw-rw-rw-  2.0 fat     1790 b- defN 23-Jan-25 22:30 htc/evaluation/model_comparison/dataset_size.py
+-rw-rw-rw-  2.0 fat     3741 b- defN 23-Jan-25 22:30 htc/evaluation/model_comparison/paper_runs.py
+-rw-rw-rw-  2.0 fat     4915 b- defN 23-Jan-25 22:30 htc/evaluation/model_comparison/run_challengeR_table.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/fonts/__init__.py
+-rw-rw-rw-  2.0 fat     1490 b- defN 23-Jan-25 22:30 htc/fonts/set_font.py
+-rw-rw-rw-  2.0 fat     6432 b- defN 23-Jan-25 22:30 htc/model_processing/ImageConsumer.py
+-rw-rw-rw-  2.0 fat     2985 b- defN 23-Jan-25 22:30 htc/model_processing/Predictor.py
+-rw-rw-rw-  2.0 fat    12514 b- defN 23-Jan-25 22:30 htc/model_processing/Runner.py
+-rw-rw-rw-  2.0 fat     6249 b- defN 23-Jan-25 22:30 htc/model_processing/TestLeaveOneOutPredictor.py
+-rw-rw-rw-  2.0 fat     4266 b- defN 23-Jan-25 22:30 htc/model_processing/TestPredictor.py
+-rw-rw-rw-  2.0 fat     4827 b- defN 23-Jan-25 22:30 htc/model_processing/ValidationPredictor.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/model_processing/__init__.py
+-rw-rw-rw-  2.0 fat     6910 b- defN 23-Jan-25 22:30 htc/model_processing/run_add_nsd.py
+-rw-rw-rw-  2.0 fat     4087 b- defN 23-Jan-25 22:30 htc/model_processing/run_image_figures.py
+-rw-rw-rw-  2.0 fat     3569 b- defN 23-Jan-25 22:30 htc/model_processing/run_inference.py
+-rw-rw-rw-  2.0 fat     3988 b- defN 23-Jan-25 22:30 htc/model_processing/run_multiple.py
+-rw-rw-rw-  2.0 fat     9926 b- defN 23-Jan-25 22:30 htc/model_processing/run_tables.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/__init__.py
+-rw-rw-rw-  2.0 fat     5736 b- defN 23-Jan-25 22:30 htc/models/run_generate_configs.py
+-rw-rw-rw-  2.0 fat     2945 b- defN 23-Jan-25 22:30 htc/models/run_prepare_pretrained.py
+-rw-rw-rw-  2.0 fat    16151 b- defN 23-Jan-25 22:30 htc/models/run_training.py
+-rw-rw-rw-  2.0 fat     1016 b- defN 23-Jan-25 22:30 htc/models/common/ChannelWeights.py
+-rw-rw-rw-  2.0 fat     3793 b- defN 23-Jan-25 22:30 htc/models/common/EvaluationMixin.py
+-rw-rw-rw-  2.0 fat     2818 b- defN 23-Jan-25 22:30 htc/models/common/ForwardHookPromise.py
+-rw-rw-rw-  2.0 fat     5543 b- defN 23-Jan-25 22:30 htc/models/common/HSI3dChannel.py
+-rw-rw-rw-  2.0 fat    18895 b- defN 23-Jan-25 22:30 htc/models/common/HTCDataset.py
+-rw-rw-rw-  2.0 fat     8502 b- defN 23-Jan-25 22:30 htc/models/common/HTCDatasetStream.py
+-rw-rw-rw-  2.0 fat    10792 b- defN 23-Jan-25 22:30 htc/models/common/HTCLightning.py
+-rw-rw-rw-  2.0 fat    25918 b- defN 23-Jan-25 22:30 htc/models/common/HTCModel.py
+-rw-rw-rw-  2.0 fat     1526 b- defN 23-Jan-25 22:30 htc/models/common/Heads.py
+-rw-rw-rw-  2.0 fat     3745 b- defN 23-Jan-25 22:30 htc/models/common/HierarchicalSampler.py
+-rw-rw-rw-  2.0 fat    15445 b- defN 23-Jan-25 22:30 htc/models/common/MetricAggregation.py
+-rw-rw-rw-  2.0 fat     6788 b- defN 23-Jan-25 22:30 htc/models/common/SharedMemoryDatasetMixin.py
+-rw-rw-rw-  2.0 fat     8769 b- defN 23-Jan-25 22:30 htc/models/common/StreamDataLoader.py
+-rw-rw-rw-  2.0 fat     2062 b- defN 23-Jan-25 22:30 htc/models/common/StreamImageDataLoader.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/common/__init__.py
+-rw-rw-rw-  2.0 fat     2226 b- defN 23-Jan-25 22:30 htc/models/common/class_weights.py
+-rw-rw-rw-  2.0 fat     5964 b- defN 23-Jan-25 22:30 htc/models/common/distance_correlation.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jan-25 22:30 htc/models/common/functions.py
+-rw-rw-rw-  2.0 fat     3550 b- defN 23-Jan-25 22:30 htc/models/common/loss.py
+-rw-rw-rw-  2.0 fat     8035 b- defN 23-Jan-25 22:30 htc/models/common/torch_helpers.py
+-rw-rw-rw-  2.0 fat    18579 b- defN 23-Jan-25 22:30 htc/models/common/transforms.py
+-rw-rw-rw-  2.0 fat     8331 b- defN 23-Jan-25 22:30 htc/models/common/utils.py
+-rw-rw-rw-  2.0 fat    10551 b- defN 23-Jan-25 22:30 htc/models/data/DataSpecification.py
+-rw-rw-rw-  2.0 fat     1231 b- defN 23-Jan-25 22:30 htc/models/data/SpecsGeneration.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/data/__init__.py
+-rw-rw-rw-  2.0 fat   117848 b- defN 23-Jan-25 22:30 htc/models/data/pigs_semantic-only_5foldsV2.json
+-rw-rw-rw-  2.0 fat   961680 b- defN 23-Jan-25 22:30 htc/models/data/pigs_semantic-only_dataset-size_repetitions=5V2.json
+-rw-rw-rw-  2.0 fat     9003 b- defN 23-Jan-25 22:30 htc/models/data/run_pig_dataset.py
+-rw-rw-rw-  2.0 fat     6907 b- defN 23-Jan-25 22:30 htc/models/data/run_size_dataset.py
+-rw-rw-rw-  2.0 fat     3890 b- defN 23-Jan-25 22:30 htc/models/image/DatasetImage.py
+-rw-rw-rw-  2.0 fat     4473 b- defN 23-Jan-25 22:30 htc/models/image/DatasetImageBatch.py
+-rw-rw-rw-  2.0 fat     3958 b- defN 23-Jan-25 22:30 htc/models/image/DatasetImageStream.py
+-rw-rw-rw-  2.0 fat     9816 b- defN 23-Jan-25 22:30 htc/models/image/LightningImage.py
+-rw-rw-rw-  2.0 fat     1287 b- defN 23-Jan-25 22:30 htc/models/image/ModelImage.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/image/__init__.py
+-rw-rw-rw-  2.0 fat     1771 b- defN 23-Jan-25 22:30 htc/models/image/configs/default.json
+-rw-rw-rw-  2.0 fat      128 b- defN 23-Jan-25 22:30 htc/models/image/configs/default_parameters.json
+-rw-rw-rw-  2.0 fat      114 b- defN 23-Jan-25 22:30 htc/models/image/configs/default_rgb.json
+-rw-rw-rw-  2.0 fat      361 b- defN 23-Jan-25 22:30 htc/models/image/configs/spxs.json
+-rw-rw-rw-  2.0 fat     3703 b- defN 23-Jan-25 22:30 htc/models/patch/DatasetPatchImage.py
+-rw-rw-rw-  2.0 fat    11357 b- defN 23-Jan-25 22:30 htc/models/patch/DatasetPatchStream.py
+-rw-rw-rw-  2.0 fat     1802 b- defN 23-Jan-25 22:30 htc/models/patch/LightningPatch.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/patch/__init__.py
+-rw-rw-rw-  2.0 fat     1930 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default.json
+-rw-rw-rw-  2.0 fat     1929 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default_64.json
+-rw-rw-rw-  2.0 fat      131 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default_64_parameters.json
+-rw-rw-rw-  2.0 fat      117 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default_64_rgb.json
+-rw-rw-rw-  2.0 fat      128 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default_parameters.json
+-rw-rw-rw-  2.0 fat      114 b- defN 23-Jan-25 22:30 htc/models/patch/configs/default_rgb.json
+-rw-rw-rw-  2.0 fat     5686 b- defN 23-Jan-25 22:30 htc/models/pixel/DatasetPixelStream.py
+-rw-rw-rw-  2.0 fat     4264 b- defN 23-Jan-25 22:30 htc/models/pixel/LightningPixel.py
+-rw-rw-rw-  2.0 fat     4573 b- defN 23-Jan-25 22:30 htc/models/pixel/ModelPixel.py
+-rw-rw-rw-  2.0 fat     2100 b- defN 23-Jan-25 22:30 htc/models/pixel/ModelPixelRGB.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/pixel/__init__.py
+-rw-rw-rw-  2.0 fat     1777 b- defN 23-Jan-25 22:30 htc/models/pixel/configs/default.json
+-rw-rw-rw-  2.0 fat      191 b- defN 23-Jan-25 22:30 htc/models/pixel/configs/default_parameters.json
+-rw-rw-rw-  2.0 fat      177 b- defN 23-Jan-25 22:30 htc/models/pixel/configs/default_rgb.json
+-rw-rw-rw-  2.0 fat     2486 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/DatasetSuperpixelImage.py
+-rw-rw-rw-  2.0 fat     4060 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/DatasetSuperpixelStream.py
+-rw-rw-rw-  2.0 fat     3583 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/LightningSuperpixelClassification.py
+-rw-rw-rw-  2.0 fat     1886 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/ModelSuperpixelClassification.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/__init__.py
+-rw-rw-rw-  2.0 fat     2015 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/configs/default.json
+-rw-rw-rw-  2.0 fat      128 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/configs/default_parameters.json
+-rw-rw-rw-  2.0 fat      114 b- defN 23-Jan-25 22:30 htc/models/superpixel_classification/configs/default_rgb.json
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/rater_variability/__init__.py
+-rw-rw-rw-  2.0 fat     4492 b- defN 23-Jan-25 22:30 htc/rater_variability/rater_evaluation.py
+-rw-rw-rw-  2.0 fat     3331 b- defN 23-Jan-25 22:30 htc/rater_variability/run_nsd_thresholds.py
+-rw-rw-rw-  2.0 fat     2574 b- defN 23-Jan-25 22:30 htc/tissue_atlas/MetricAggregationClassification.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/tissue_atlas/__init__.py
+-rw-rw-rw-  2.0 fat     2474 b- defN 23-Jan-25 22:30 htc/tissue_atlas/run_test_table_generation.py
+-rw-rw-rw-  2.0 fat     5564 b- defN 23-Jan-25 22:30 htc/tissue_atlas/settings_atlas.py
+-rw-rw-rw-  2.0 fat     2176 b- defN 23-Jan-25 22:30 htc/tissue_atlas/tables.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/tissue_atlas/data/__init__.py
+-rw-rw-rw-  2.0 fat     3377 b- defN 23-Jan-25 22:30 htc/tissue_atlas/data/run_tissue_atlas_dataset.py
+-rw-rw-rw-  2.0 fat 15656801 b- defN 23-Jan-25 22:30 htc/tissue_atlas/data/tissue-atlas_loocv_test-8_seed-0_cam-118.json
+-rw-rw-rw-  2.0 fat     2419 b- defN 23-Jan-25 22:30 htc/tissue_atlas/median_pixel/DatasetMedianPixel.py
+-rw-rw-rw-  2.0 fat     5174 b- defN 23-Jan-25 22:30 htc/tissue_atlas/median_pixel/LightningMedianPixel.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/tissue_atlas/median_pixel/__init__.py
+-rw-rw-rw-  2.0 fat     1244 b- defN 23-Jan-25 22:30 htc/tissue_atlas/median_pixel/configs/default.json
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/tissue_atlas_open/__init__.py
+-rw-rw-rw-  2.0 fat    10646 b- defN 23-Jan-25 22:30 htc/tissue_atlas_open/profiles.py
+-rw-rw-rw-  2.0 fat    11507 b- defN 23-Jan-25 22:30 htc/tissue_atlas_open/run_label_profiles.py
+-rw-rw-rw-  2.0 fat     4208 b- defN 23-Jan-25 22:30 htc/tissue_atlas_open/run_readme_gif.py
+-rw-rw-rw-  2.0 fat    50469 b- defN 23-Jan-25 22:30 htc/tivita/DataPath.py
+-rw-rw-rw-  2.0 fat     3872 b- defN 23-Jan-25 22:30 htc/tivita/DataPathMultiorgan.py
+-rw-rw-rw-  2.0 fat     4493 b- defN 23-Jan-25 22:30 htc/tivita/DataPathReference.py
+-rw-rw-rw-  2.0 fat     6043 b- defN 23-Jan-25 22:30 htc/tivita/DataPathSepsis.py
+-rw-rw-rw-  2.0 fat     2028 b- defN 23-Jan-25 22:30 htc/tivita/DataPathTivita.py
+-rw-rw-rw-  2.0 fat     5874 b- defN 23-Jan-25 22:30 htc/tivita/DatasetSettings.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/tivita/__init__.py
+-rw-rw-rw-  2.0 fat     1288 b- defN 23-Jan-25 22:30 htc/tivita/colorscale.py
+-rw-rw-rw-  2.0 fat     3403 b- defN 23-Jan-25 22:30 htc/tivita/hsi.py
+-rw-rw-rw-  2.0 fat     5529 b- defN 23-Jan-25 22:30 htc/tivita/metadata.py
+-rw-rw-rw-  2.0 fat     4057 b- defN 23-Jan-25 22:30 htc/tivita/rgb.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 23-Jan-25 22:30 htc/utils/AdvancedJSONEncoder.py
+-rw-rw-rw-  2.0 fat    18217 b- defN 23-Jan-25 22:30 htc/utils/ColorcheckerReader.py
+-rw-rw-rw-  2.0 fat     2357 b- defN 23-Jan-25 22:30 htc/utils/ColoredFileLog.py
+-rw-rw-rw-  2.0 fat    17475 b- defN 23-Jan-25 22:30 htc/utils/Config.py
+-rw-rw-rw-  2.0 fat    18628 b- defN 23-Jan-25 22:30 htc/utils/DatasetDir.py
+-rw-rw-rw-  2.0 fat     2266 b- defN 23-Jan-25 22:30 htc/utils/DelayedFileHandler.py
+-rw-rw-rw-  2.0 fat     7620 b- defN 23-Jan-25 22:30 htc/utils/DomainMapper.py
+-rw-rw-rw-  2.0 fat      466 b- defN 23-Jan-25 22:30 htc/utils/DuplicateFilter.py
+-rw-rw-rw-  2.0 fat     3549 b- defN 23-Jan-25 22:30 htc/utils/LDA.py
+-rw-rw-rw-  2.0 fat    15529 b- defN 23-Jan-25 22:30 htc/utils/LabelMapping.py
+-rw-rw-rw-  2.0 fat     1240 b- defN 23-Jan-25 22:30 htc/utils/MeasureTime.py
+-rw-rw-rw-  2.0 fat    15488 b- defN 23-Jan-25 22:30 htc/utils/MultiPath.py
+-rw-rw-rw-  2.0 fat     7161 b- defN 23-Jan-25 22:30 htc/utils/SpectrometerReader.py
+-rw-rw-rw-  2.0 fat      112 b- defN 23-Jan-25 22:30 htc/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3236 b- defN 23-Jan-25 22:30 htc/utils/blosc_compression.py
+-rw-rw-rw-  2.0 fat     3716 b- defN 23-Jan-25 22:30 htc/utils/colors.py
+-rw-rw-rw-  2.0 fat     3165 b- defN 23-Jan-25 22:30 htc/utils/file_transfer.py
+-rw-rw-rw-  2.0 fat     6338 b- defN 23-Jan-25 22:30 htc/utils/general.py
+-rw-rw-rw-  2.0 fat    25274 b- defN 23-Jan-25 22:30 htc/utils/helper_functions.py
+-rw-rw-rw-  2.0 fat     1611 b- defN 23-Jan-25 22:30 htc/utils/import_extra.py
+-rw-rw-rw-  2.0 fat     3808 b- defN 23-Jan-25 22:30 htc/utils/parallel.py
+-rw-rw-rw-  2.0 fat     5328 b- defN 23-Jan-25 22:30 htc/utils/paths.py
+-rw-rw-rw-  2.0 fat     9879 b- defN 23-Jan-25 22:30 htc/utils/renderjson.js
+-rw-rw-rw-  2.0 fat     2644 b- defN 23-Jan-25 22:30 htc/utils/run_system_monitor.py
+-rw-rw-rw-  2.0 fat     8301 b- defN 23-Jan-25 22:30 htc/utils/sqldf.py
+-rw-rw-rw-  2.0 fat     2170 b- defN 23-Jan-25 22:30 htc/utils/type_from_string.py
+-rw-rw-rw-  2.0 fat      960 b- defN 23-Jan-25 22:30 htc/utils/unify_path.py
+-rw-rw-rw-  2.0 fat    78852 b- defN 23-Jan-25 22:30 htc/utils/visualization.py
+-rw-rw-rw-  2.0 fat      211 b- defN 23-Jan-25 22:35 imsy_htc-0.0.9.dist-info/LICENSE.md
+-rw-rw-rw-  2.0 fat    30900 b- defN 23-Jan-25 22:35 imsy_htc-0.0.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jan-25 22:35 imsy_htc-0.0.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       37 b- defN 23-Jan-25 22:33 imsy_htc-0.0.9.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        4 b- defN 23-Jan-25 22:33 imsy_htc-0.0.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    16859 b- defN 23-Jan-25 22:35 imsy_htc-0.0.9.dist-info/RECORD
+185 files, 18021262 bytes uncompressed, 1828187 bytes compressed:  89.9%
```

## zipnote {}

```diff
@@ -12,26 +12,20 @@
 
 Filename: htc/settings.py
 Comment: 
 
 Filename: htc/settings_seg.py
 Comment: 
 
-Filename: htc/cpp/ParallelExecution.h
-Comment: 
-
 Filename: htc/cpp/__init__.py
 Comment: 
 
 Filename: htc/cpp/bindings.cpp
 Comment: 
 
-Filename: htc/cpp/colorchecker_automask.cpp
-Comment: 
-
 Filename: htc/cpp/evaluate_superpixels.cpp
 Comment: 
 
 Filename: htc/cpp/hierarchical_bootstrapping.h
 Comment: 
 
 Filename: htc/cpp/kfold_combinations.cpp
@@ -63,17 +57,14 @@
 
 Filename: htc/data_processing/run_median_spectra.py
 Comment: 
 
 Filename: htc/data_processing/run_parameter_images.py
 Comment: 
 
-Filename: htc/data_processing/run_raw16.py
-Comment: 
-
 Filename: htc/data_processing/run_standardization.py
 Comment: 
 
 Filename: htc/data_processing/run_superpixel_prediction.py
 Comment: 
 
 Filename: htc/evaluation/__init__.py
@@ -84,30 +75,24 @@
 
 Filename: htc/evaluation/evaluate_images.py
 Comment: 
 
 Filename: htc/evaluation/evaluate_superpixels.py
 Comment: 
 
-Filename: htc/evaluation/ranking.py
-Comment: 
-
 Filename: htc/evaluation/run_compare_runs.py
 Comment: 
 
 Filename: htc/evaluation/run_ranking_lr.py
 Comment: 
 
 Filename: htc/evaluation/run_table_generation.py
 Comment: 
 
-Filename: htc/evaluation/utils.py
-Comment: 
-
-Filename: htc/evaluation/metrics/ECE.py
+Filename: htc/evaluation/metrics/ECELoss.py
 Comment: 
 
 Filename: htc/evaluation/metrics/NSDToleranceEstimation.py
 Comment: 
 
 Filename: htc/evaluation/metrics/__init__.py
 Comment: 
@@ -150,14 +135,17 @@
 
 Filename: htc/model_processing/ValidationPredictor.py
 Comment: 
 
 Filename: htc/model_processing/__init__.py
 Comment: 
 
+Filename: htc/model_processing/run_add_nsd.py
+Comment: 
+
 Filename: htc/model_processing/run_image_figures.py
 Comment: 
 
 Filename: htc/model_processing/run_inference.py
 Comment: 
 
 Filename: htc/model_processing/run_multiple.py
@@ -471,15 +459,15 @@
 
 Filename: htc/utils/ColoredFileLog.py
 Comment: 
 
 Filename: htc/utils/Config.py
 Comment: 
 
-Filename: htc/utils/Datasets.py
+Filename: htc/utils/DatasetDir.py
 Comment: 
 
 Filename: htc/utils/DelayedFileHandler.py
 Comment: 
 
 Filename: htc/utils/DomainMapper.py
 Comment: 
@@ -495,17 +483,14 @@
 
 Filename: htc/utils/MeasureTime.py
 Comment: 
 
 Filename: htc/utils/MultiPath.py
 Comment: 
 
-Filename: htc/utils/SLICWrapper.py
-Comment: 
-
 Filename: htc/utils/SpectrometerReader.py
 Comment: 
 
 Filename: htc/utils/__init__.py
 Comment: 
 
 Filename: htc/utils/blosc_compression.py
@@ -546,26 +531,26 @@
 
 Filename: htc/utils/unify_path.py
 Comment: 
 
 Filename: htc/utils/visualization.py
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/LICENSE.md
+Filename: imsy_htc-0.0.9.dist-info/LICENSE.md
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/METADATA
+Filename: imsy_htc-0.0.9.dist-info/METADATA
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/WHEEL
+Filename: imsy_htc-0.0.9.dist-info/WHEEL
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/entry_points.txt
+Filename: imsy_htc-0.0.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/top_level.txt
+Filename: imsy_htc-0.0.9.dist-info/top_level.txt
 Comment: 
 
-Filename: imsy_htc-0.0.11.dist-info/RECORD
+Filename: imsy_htc-0.0.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## htc/run_info.py

```diff
@@ -54,13 +54,13 @@
             print(f"{key}={os.environ[key]}")
 
     print("\n[b]Datasets:[/]")
     print(f"{settings.data_dirs}\n")
 
     print("\n[b]Other directories:[/]")
     print(repr(settings.results_dir))
-    print(repr(settings.intermediates_dir_all))
+    print(repr(settings.intermediates_dir))
     print(f"src_dir={settings.src_dir}")
     print(f"htc_package_dir={settings.htc_package_dir}")
 
     print("\n[b]System:[/]")
     collect_env.main()
```

## htc/settings.py

```diff
@@ -9,15 +9,15 @@
 from appdirs import user_config_dir
 from dotenv import load_dotenv
 from rich.console import Console
 from rich.highlighter import ReprHighlighter
 from rich.logging import RichHandler
 from rich.theme import Theme
 
-from htc.utils.Datasets import DatasetAccessor, Datasets
+from htc.utils.DatasetDir import DatasetDir
 from htc.utils.DuplicateFilter import DuplicateFilter
 from htc.utils.MultiPath import MultiPath
 
 
 class ColoredFormatter(logging.Formatter):
     def format(self, record):
         # Apply level-specific color
@@ -75,28 +75,28 @@
     Now we can access files from both results directories
     >>> (settings.results_dir / "testfile_semantic.txt").exists()
     True
     >>> (settings.results_dir / "testfile_atlas.txt").exists()
     True
 
     If we create a new file, it will be created in our semantic results directory
-    >>> _ = (settings.results_dir / "new_testfile.txt").write_text("test")
+    >>> (settings.results_dir / "new_testfile.txt").write_text("test")
     >>> files = [str(f) for f in sorted(settings.results_dir.rglob("*"))]
     >>> len(files)
     3
     >>> files  # doctest: +ELLIPSIS
     ['.../results_atlas/testfile_atlas.txt', '.../results_semantic/new_testfile.txt', '.../results_semantic/testfile_semantic.txt']
 
     The above behavior is useful for general scripts like `htc training` which can be used in any project. If you have a script or notebook which can only be used for one project, then it is also possible to access the specific results directory for this project explicitly
     >>> from htc.tissue_atlas.settings_atlas import settings_atlas
     >>> n_bytes = (settings_atlas.results_dir / "new_testfile_atlas.txt").write_text("test")
     >>> [str(f) for f in sorted(settings_atlas.results_dir.rglob("*"))]  # doctest: +ELLIPSIS
     ['.../results_atlas/new_testfile_atlas.txt', '.../results_atlas/testfile_atlas.txt']
 
-    - `PATH_E130_Projekte` or `PATH_HTC_NETWORK`: Path to the network drive of your department. The value will be accessible via `settings.datasets.network_dir`. Example: `PATH_E130_Projekte="/mnt/E130-Projekte"`
+    - `PATH_E130_Projekte` or `PATH_HTC_NETWORK`: Path to the network drive of your department. The value will be accessible via `settings.data_dirs.network_dir`. Example: `PATH_E130_Projekte="/mnt/E130-Projekte"`
     - `HTC_DOCKER_MOUNTS`: Additional mount locations for our Docker container. This is for example useful if part of the data lies on a separate disk and you link to this location in your dataset. With this environment variable, you can make the symbolic link also work in the Docker container. The syntax is `path_local1:path_docker1;path_local2:path_docker2`. Example: `HTC_DOCKER_MOUNTS="/mnt/nvme_4tb/2021_02_05_Tivita_multiorgan_masks/intermediates:/mnt/nvme_4tb/2021_02_05_Tivita_multiorgan_masks/intermediates"`
     - `PATH_HTC_DOCKER_RESULTS`: If you compute something in our Docker container, results will only be stored in the container and deleted as soon as the container exits (since the container is only intended for testing). Let this variable point to a directory of your choice to keep your Docker results. Example: `PATH_HTC_DOCKER_RESULTS="/my/results/folder"`
     - `HTC_ADD_NETWORK_ALTERNATIVES`: If set to the string `true`, will include results and intermediate directories on the network drive (default `false`). This is usually only required for testing. Example: `HTC_ADD_NETWORK_ALTERNATIVES="true"`
     - `HTC_ENV_OVERRIDE`: Whether environment variables defined in the .env file or in your user settings override existing variables (default `true`). Set this to `false` if you want that variables defined elsewhere (e.g. before the command: `ENV_NAME htc command`) have precedence. Example: `HTC_ENV_OVERRIDE="false"`
     - `HTC_MODEL_COMPARISON_TIMESTAMP`: Variable is read in settings_seg and can be used to overwrite the default comparison timestamp (e.g. used for reproducibility of the MIA2021 paper). Example: `HTC_MODEL_COMPARISON_TIMESTAMP="2022-02-03_22-58-44"`
     - `HTC_CUDA_MEM_FRACTION`: Used in run_training.py to limit the GPU memory to a fraction of the available GPU memory (e.g. to simulate GPUs with less memory). Example: `HTC_CUDA_MEM_FRACTION="0.5"`
     - `DKFZ_USERID`: Name of your AD account (DKFZ internal). This is useful for the communication with our cluster. Example: `DKFZ_USERID="a267c"`
@@ -142,17 +142,17 @@
 
         handler = RichHandler(
             markup=True,
             show_time=False,
             show_level=False,
             enable_link_path=False,  # File links don't work in notebooks and cause problems with nbval
             console=Console(
-                width=(
-                    120 if self.is_interactive else None
-                ),  # Increase the default width in notebooks (unfortunately, rich cannot detect the browser width automatically: https://github.com/Textualize/rich/issues/504)
+                width=120
+                if self.is_interactive
+                else None,  # Increase the default width in notebooks (unfortunately, rich cannot detect the browser width automatically: https://github.com/Textualize/rich/issues/504)
                 theme=Theme(
                     {
                         "var": "dim",
                         "repr.image_name": "cyan",
                         "repr.str": "bright_black",
                         "repr.number": "grey69",
                         "logging.level.debug": "bright_blue",
@@ -174,15 +174,15 @@
         self.log.setLevel(logging.INFO)
 
         self.log_once = logging.getLogger("htc.no_duplicates")
         self.log_once.addFilter(DuplicateFilter())
 
         logging.getLogger("challenger-pydocker").setLevel(logging.INFO)
 
-        if self.should_add_network_alternatives:
+        if self.add_network_alternatives:
             # This info message is not relevant for testing
             self.log_once.addFilter(
                 lambda record: "Falling back to the network drive (this may be slow)" not in record.msg
             )
 
         # Directories relative to this file
         self.htc_package_dir = Path(__file__).parent
@@ -224,23 +224,25 @@
             "liver": "#D0856D",
             "gallbladder": "#000000",
             "pancreas": "#3733F0",
             "kidney": "#54B5D4",
             "spleen": "#60F07A",
             "bladder": "#67E619",
             "omentum": "#B9D912",
+            "fat": "#E1E9AA",
             "lung": "#5267C7",
             "heart": "#0DA07C",
             "cartilage": "#0F92DB",
             "bone": "#F8B08C",
             "skin": "#2BF0F3",
             "muscle": "#CC170F",
             "peritoneum": "#98CC66",
             "aorta": "#F4D352",
             "major_vein": "#CCCCCC",
+            "major_vein": "#CCCCCC",
             "veins": "#4E2A7E",
             "kidney_with_Gerotas_fascia": "#F43E4C",
             "kidney_with_fascia": "#F43E4C",
             "lymph_nodes": "#24A90A",
             "blue_cloth": "#B38919",
             "white_compress": "#F09B2D",
             "abdominal_linen": "#5E10B7",
@@ -267,31 +269,16 @@
             "blood": "#de0d6f",
             "lymph_fluid": "#bb7af5",
             "urine": "#e1518d",
             "cauterization": "#ffbb00",
             "lig_teres_hep": "#d451e0",
             "fat_subcutaneous": "#e0536b",
             "fat_visceral": "#43c456",
-            "fat": "#a1ffca",  # Some older scripts still use fat instead of fat_subcutaneous or fat_visceral
             "meso": "#4363c4",
             "esophagus": "#4743c4",
-            "plastic": "#42e0f5",
-            "unclear_anorganic": "#a86d32",
-            "tumor": "#ff5100",
-            "reflection": "#dbb8d1",
-            "unclear_organic": "#C49505",
-            "stapler": "#C7DE12",
-            "ligasure": "#1277DE",
-            "monopolar": "#5B3478",
-            "tag_blood": "#f51505",
-            "tag_cauterization": "#9d9e9e",
-            "tag_malperfused": "#03ffff",
-            "instrument": "#636363",
-            "Exterior": "#00000000",  # Unlabeled parts in MITK
-            "network_unsure": "#AAAAAA",
         }
 
         self.known_envs = (
             "PATH_Tivita",
             "PATH_TIVITA",
             "PATH_HTC_RESULTS",
             "PATH_HTC_RESULTS_",
@@ -304,141 +291,68 @@
             "HTC_MODEL_COMPARISON_TIMESTAMP",
             "HTC_CUDA_MEM_FRACTION",
             "DKFZ_USERID",
             "CLUSTER_FOLDER",
             "SHARED_FOLDER",
         )
 
-        self._datasets = None
-        self._intermediates_dir_all = None
+        self._data_dirs = None
+        self._intermediates_dir = None
         self._results_dir = None
 
     @property
-    def should_add_network_alternatives(self) -> bool:
+    def add_network_alternatives(self) -> bool:
         # Some tests (e.g. notebooks) require result or intermediate files and it is notoriously hard to share them between users
         # As a workaround, we store them on the network drives but this can be slow so this is only done during testing
         return os.getenv("HTC_ADD_NETWORK_ALTERNATIVES", "false") == "true"
 
     @property
-    def datasets(self) -> Datasets:
-        """
-        Access datasets which are registered via environment variables.
-
-        Some default datasets (e.g. semantic, human) are registered automatically but any other dataset can be registered via environment variables starting with `PATH_TIVITA*`.
-
-        For each dataset, a dictionary with additional information is available (see `Datasets.get()` for more information).
-        >>> list(settings.datasets.semantic.keys())
-        ['path_dataset', 'path_data', 'path_intermediates', 'location', 'has_unified_paths', 'shortcut', 'env_name']
-
-        You can also access the network drive with this variable:
-        >>> str(settings.datasets.network_data)  # DOCTEST: +ELLIPSIS
-        '.../E130-Projekte/Biophotonics/Data'
-
-        Returns: `Datasets` object with accessor helpers. Please also refer to the documentation of the `Datasets` class for more information.
-        """
-        if self._datasets is None:
+    def data_dirs(self) -> DatasetDir:
+        if self._data_dirs is None:
             if path_env := os.getenv("PATH_E130_Projekte", False):
                 network_dir = Path(path_env)
             elif path_env := os.getenv("PATH_HTC_NETWORK", False):
                 network_dir = Path(path_env)
             else:
                 network_dir = None
 
-            self._datasets = Datasets(network_dir=network_dir)
-            self._datasets.add_dir(
-                "PATH_Tivita_multiorgan_human", "2021_07_26_Tivita_multiorgan_human", shortcut="human"
-            )
-            self._datasets.add_dir("PATH_Tivita_studies", "2021_03_30_Tivita_studies", shortcut="studies")
-            self._datasets.add_dir(
-                "PATH_Tivita_multiorgan_semantic", "2021_02_05_Tivita_multiorgan_semantic", shortcut="semantic"
-            )
-            self._datasets.add_dir(
-                "PATH_Tivita_multiorgan_masks", "2021_02_05_Tivita_multiorgan_masks", shortcut="masks"
-            )
-            self._datasets.add_dir("PATH_Tivita_sepsis_study", "2020_11_24_Tivita_sepsis_study", shortcut="sepsis")
-            self._datasets.add_dir("PATH_Tivita_sepsis_ICU", "2022_10_24_Tivita_sepsis_ICU", shortcut="sepsis_ICU")
-            self._datasets.add_dir(
-                "PATH_Tivita_unsorted_images", "2022_08_03_Tivita_unsorted_images", shortcut="unsorted"
-            )
+            self._data_dirs = DatasetDir(network_dir=network_dir)
 
             # Automatically add all additional datasets which start with PATH_Tivita
             for env_name in os.environ.keys():
                 if not env_name.upper().startswith("PATH_TIVITA"):
                     continue
-                if env_name in self._datasets:
+                if env_name in self._data_dirs:
                     continue
 
-                _, options = Datasets.parse_path_options(os.environ[env_name])
+                _, options = DatasetDir.parse_path_options(os.environ[env_name])
                 shortcut = options.get("shortcut", None)
 
                 # Per default, the dataset is accessible via three names. For example, for PATH_Tivita_HeiPorSPECTRAL=/my/dataset_folder_name:
                 # - PATH_Tivita_HeiPorSPECTRAL
                 # - dataset_folder_name
                 # - atlas_pigs
-                self._datasets.add_dir(
+                self._data_dirs.add_dir(
                     env_name,
                     shortcut=shortcut,
                     additional_names=[
                         env_name.removeprefix("PATH_Tivita_"),
                         env_name.upper().removeprefix("PATH_TIVITA_"),
                     ],
                 )
 
-        return self._datasets
-
-    @property
-    def data_dirs(self) -> DatasetAccessor:
-        """
-        This property can be used to access files in the data directory of a specific dataset. The usage is similar to `settings.intermediates_dirs.<dataset_name>`.
-
-        >>> str(settings.data_dirs.semantic)  # DOCTEST: +ELLIPSIS
-        '.../2021_02_05_Tivita_multiorgan_semantic/data'
-
-        Returns: `DatasetAccessor` object which can be used to access the data dir by (short) name of the dataset. None if the requested dataset is not available.
-        """
-        return DatasetAccessor(self.datasets, "path_data")
+        return self._data_dirs
 
     @property
-    def intermediates_dirs(self) -> DatasetAccessor:
-        """
-        This property can be used to access files in the intermediates directory of a specific dataset. The usage is similar to `settings.data_dirs.<dataset_name>`.
-
-        >>> str(settings.intermediates_dirs.semantic)  # DOCTEST: +ELLIPSIS
-        '.../2021_02_05_Tivita_multiorgan_semantic/intermediates'
-
-        Returns: `DatasetAccessor` object which can be used to access the intermediates dir by (short) name of the dataset. None if the requested dataset is not available.
-        """
-        return DatasetAccessor(self.datasets, "path_intermediates")
-
-    @property
-    def intermediates_dir_all(self) -> Union[MultiPath, None]:
-        """
-        This property can be used to access files in any of the registered intermediate directories (from all datasets which are available).
-
-        This is useful for reading files without knowing from which dataset this file is form (e.g. during training). It is assumed that the files inside the intermediates directories are unique across datasets, e.g. because they have different image names.
-
-        If you use this variable to create new files (not recommended, please use `settings.intermediates_dirs.<dataset_name>` instead), the files will be created in the first dataset which is available.
-
-        File from the semantic dataset:
-        >>> path1 = settings.intermediates_dir_all / "segmentations" / "P041#2019_12_14_12_00_16.blosc"
-        >>> path1.exists()
-        True
-
-        File from the masks dataset:
-        >>> path2 = settings.intermediates_dir_all / "segmentations" / "P001#2018_07_26_13_04_30.blosc"
-        >>> path2.exists()
-        True
-
-        Returns: Multi path object for the intermediates directory of all datasets. None if no dataset is set.
-        """
-        if self._intermediates_dir_all is None:
+    def intermediates_dir(self) -> Union[MultiPath, None]:
+        if self._intermediates_dir is None:
             # Combine all intermediates from all data dirs into one variable
             dirs = []
-            for name in self.datasets.dataset_names:
-                found_entry = self.datasets.get(name, local_only=not self.should_add_network_alternatives)
+            for name in self.data_dirs.dataset_names:
+                found_entry = self.data_dirs.get(name, local_only=not self.add_network_alternatives, return_entry=True)
                 if found_entry is None:
                     continue
 
                 # found_dir points to the directory for the dataset which usually contains subfolders for data and intermediates
                 found_dir = found_entry["path_intermediates"]
                 if found_dir.exists():
                     try:
@@ -450,74 +364,54 @@
                         self.log.info(
                             f"The data directory {found_dir} is available but not accessible (permission denied). The"
                             " directory will be skipped"
                         )
                         pass
 
             if len(dirs) == 0:
-                self._intermediates_dir_all = False
+                self._intermediates_dir = False
                 self.log.warning(
                     "Could not find an intermediates directory, probably because no data directory was found"
                 )
             else:
-                self._intermediates_dir_all = MultiPath(dirs[0])
+                self._intermediates_dir = MultiPath(dirs[0])
                 for d in dirs[1:]:
-                    self._intermediates_dir_all.add_alternative(d)
+                    self._intermediates_dir.add_alternative(d)
 
-        return None if not self._intermediates_dir_all else self._intermediates_dir_all
+        return None if not self._intermediates_dir else self._intermediates_dir
 
     @property
     def results_dir(self) -> Union[MultiPath, None]:
-        """
-        This property can be used to read or write to your results directories (`PATH_HTC_RESULTS` and `PATH_HTC_RESULTS_*` environment variables).
-
-        New files will be written to `PATH_HTC_RESULTS` and you can read files from all registered directories. This way you can organize your results from different projects in different directories while still being able to read files from all without always specifying the full path.
-
-        >>> (settings.results_dir / "specs_figures").exists()
-        True
-
-        Returns: Multi path object for the results directories. None if no results directory is set.
-        """
         if self._results_dir is None:
             if results_dir_path := os.getenv("PATH_HTC_RESULTS", False):
                 self._results_dir = MultiPath(results_dir_path)
 
                 # We always write to the main location
                 self._results_dir.set_default_location(str(self._results_dir.find_best_location()))
 
                 # Add additional result_dirs if available
                 for k in os.environ.keys():
                     if k.startswith("PATH_HTC_RESULTS_"):
                         path = os.getenv(k)
                         if path and path != results_dir_path:  # No duplicate paths
                             self._results_dir.add_alternative(path)
 
-                if self.should_add_network_alternatives:
+                if self.add_network_alternatives:
                     local_location_names = [l.name for l in self._results_dir.possible_locations()]
-                    for d in sorted(self.datasets.network_project.glob("results*")):
+                    for d in sorted(self.data_dirs.network_project.glob("results*")):
                         if d.name not in local_location_names:  # Do not add it if it already exists locally
                             self._results_dir.add_alternative(d)
             else:
                 self._results_dir = False
                 self.log.warning(
                     "Could not find the environment variable PATH_HTC_RESULTS so that a results directory will not be"
                     " available (scripts which use settings.results_dir will crash)"
                 )
 
         return None if not self._results_dir else self._results_dir
 
     @property
     def training_dir(self) -> Union[MultiPath, None]:
-        """
-        This property can be used to access the training directory of all registered datasets.
-
-        This is basically a subfolder of the results directory and can be used to locate a training run without the need to specify from which results directory it is.
-
-        >>> (settings.training_dir / "image" / "2023-02-08_14-48-02_organ_transplantation_0.8").exists()
-        True
-
-        Returns: Multi path object for the training directories. None if no results directory is set.
-        """
         return self.results_dir / "training" if self.results_dir is not None else None
 
 
 settings = Settings()
```

## htc/settings_seg.py

```diff
@@ -35,15 +35,15 @@
                 "stomach": 6,
                 "spleen": 7,
                 "gallbladder": 8,
                 "bladder": 9,
                 "omentum": 10,
                 "peritoneum": 11,
                 "skin": 12,
-                "fat_subcutaneous": 13,
+                "fat": 13,
                 "pancreas": 14,
                 "muscle": 15,
                 "kidney": 16,
                 "major_vein": 17,
                 "kidney_with_Gerotas_fascia": 18,
                 # These classes are ignored
                 "unlabeled": settings.label_index_thresh,
@@ -68,31 +68,27 @@
                 "ureter": settings.label_index_thresh + 18,
                 "blood": settings.label_index_thresh + 19,
                 "lymph_fluid": settings.label_index_thresh + 20,
                 "urine": settings.label_index_thresh + 21,
                 # Ignored because they occur only the humans dataset
                 "cauterization": settings.label_index_thresh + 22,
                 "lig_teres_hep": settings.label_index_thresh + 23,
+                "fat_subcutaneous": settings.label_index_thresh + 24,
                 "fat_visceral": settings.label_index_thresh + 25,
                 "meso": settings.label_index_thresh + 26,
                 "esophagus": settings.label_index_thresh + 27,
-                "unclear_organic": settings.label_index_thresh + 28,
-                "stapler": settings.label_index_thresh + 29,
-                "ligasure": settings.label_index_thresh + 30,
-                "monopolar": settings.label_index_thresh + 31,
             }
         )
         self.labels = self.label_mapping.label_names()
 
         self.model_comparison_timestamp = os.getenv("HTC_MODEL_COMPARISON_TIMESTAMP", "2022-02-03_22-58-44")
         self.dataset_size_timestamp = "2022-02-16_23-01-31"
         self.lr_experiment_timestamp = "2022-02-04_22-05-49"
         self.seed_experiment_timestamp = "2022-02-15_20-00-11"
         self.nsd_aggregation = "surface_dice_metric_image_mean"
-        self.nsd_aggregation_short = self.nsd_aggregation.replace("_image", "")
         self.lr_default = 0.001
         self.lr_higher = 0.01
         self.lr_lower = 0.0001
 
         # Also used for ordering of the labels
         self.label_colors_paper = {
             "invalid": "#BBBBBB",
@@ -105,26 +101,26 @@
             "liver": "#ff7f0e",
             "gallbladder": "#ff9896",
             "pancreas": "#f7b6d2",
             "spleen": "#d62728",
             "kidney": "#dbdb8d",
             "kidney_with_Gerotas_fascia": "#9edae5",
             "bladder": "#9467bd",
-            "fat_subcutaneous": "#e377c2",
+            "fat": "#e377c2",
             "skin": "#c49c94",
             "muscle": "#bcbd22",
             "omentum": "#c5b0d5",
             "peritoneum": "#8c564b",
             "major_vein": "#17becf",
         }
         self.labels_paper_renaming = {
             "invalid": "ignore",
             "major_vein": "major vein",
             "kidney_with_Gerotas_fascia": "kidney with<br>Gerota's fascia",
-            "fat_subcutaneous": "subcutaneous fat",
+            "fat": "subcutaneous fat",
             "small_bowel": "small intestine",
         }
         self.modality_names = {
             "hsi": "HSI",
             "param": "TPI",  # Tissue Parameter Images (or Tivita Parameter Images xD)
             "rgb": "RGB",
         }
```

## htc/cpp/__init__.py

```diff
@@ -127,16 +127,16 @@
         tensor: The input tensor.
         mapping: The remapping dictionary.
 
     Returns: Same as the input tensor with the remapping applied in-place.
     """
     assert len(mapping) > 0, "Empty mapping"
     assert all(
-        type(k) == type(v) for k, v in mapping.items()
-    ), "All keys and values of the mapping must have the same type"
+        [type(k) == type(v) for k, v in mapping.items()]
+    ), "All keys and values of the mapping must have the same time"
     first_value = next(iter(mapping.values()))
 
     if isinstance(first_value, int):
         assert not tensor.is_floating_point(), f"The tensor must have an integer type ({tensor.dtype = })"
         return htc._cpp.tensor_mapping_integer(tensor, mapping)
     elif isinstance(first_value, float):
         assert tensor.is_floating_point(), f"The tensor must have an floating type ({tensor.dtype = })"
@@ -223,139 +223,39 @@
     return htc._cpp.map_label_image(label_image, label_color_mapping)
 
 
 def hierarchical_bootstrapping(
     mapping: dict[int, dict[int, list[int]]], n_subjects: int, n_images: int, n_bootstraps: int = 1000
 ) -> torch.Tensor:
     """
-    Creates bootstrap samples based on a three-level hierarchy (domain_name, subject_name, image_name) while always selecting all domains in every bootstrap and the specified number of subjects and images (both with resampling).
+    Creates bootstrap samples based on a two-level hierarchy (cam_name, subject_name, image_name) while always selecting all cameras in every bootstrap and the specified number of subjects and images (both with resampling).
 
     Note: This function is not deterministic but you can set a seed.
 
-    >>> from lightning import seed_everything
+    >>> from pytorch_lightning import seed_everything
     >>> print('ignore_line'); seed_everything(0)  # doctest: +ELLIPSIS
     ignore_line...
     >>> mapping = {
     ...     0: {0: [10]},              # First camera, one subject with one image
     ...     1: {1: [20, 30], 2: [40]}  # Second camera, two subjects with two and one image each
     ... }
     >>> hierarchical_bootstrapping(mapping, n_subjects=2, n_images=1, n_bootstraps=4)
     tensor([[30, 20, 10, 10],
             [30, 20, 10, 10],
             [40, 40, 10, 10],
             [30, 40, 10, 10]])
 
     Args:
-        mapping: Domain to subjects to images mapping.
+        mapping: Camera to subjects to images mapping.
         n_subjects: Number of subjects to draw with replacement.
         n_images: Number of images to draw with replacement.
         n_bootstraps: Total number of bootstraps.
 
-    Returns: Matrix of shape (n_bootstraps, n_domains * n_subjects * n_images) with the bootstraps. It contains the values provided for the images (final layer in the mapping).
+    Returns: Matrix of shape (n_bootstraps, n_cams * n_subjects * n_images) with the bootstraps. It contains the values provided for the images (final lay in the mapping).
     """
     # We are generating a random number which will be used as seed during bootstraping
     # This produces different bootstraps when the user calls this function multiple times while still allowing to set a seed
     seed = torch.randint(0, torch.iinfo(torch.int32).max, (1,), dtype=torch.int32).item()
     bootstraps = htc._cpp.hierarchical_bootstrapping(mapping, n_subjects, n_images, n_bootstraps, seed)
     assert bootstraps.shape == (n_bootstraps, len(set(mapping.keys())) * n_subjects * n_images)
 
     return bootstraps
-
-
-def hierarchical_bootstrapping_labels(
-    domain_mapping: dict[int, dict[int, list[int]]],
-    label_mapping: dict[int, dict[int, list[int]]],
-    n_labels: int,
-    n_bootstraps: int = 1000,
-) -> torch.Tensor:
-    """
-    Creates bootstrap samples based on a three-level hierarchy (domain_name, subject_name, image_name) while always selecting all domains equally often in every bootstrap. Compared to `hierarchical_bootstrapping()`, this function takes the labels into account and always selects images with the same label for each domain tuple. For each domain and label, one subject and one image is selected, i.e. selection of different subjects is preferred over selecting many images per subject.
-
-    Using this function, a batch with a size of 4 may look like this:
-    | domain | subject | image | label |
-    |---|---|---|---|
-    | D1 | S1 | I1 | liver |
-    | D2 | S2 | I2 | liver |
-    | D1 | S1 | I3 | colon |
-    | D2 | S3 | I4 | colon |
-
-    Note: This function is not deterministic but you can set a seed.
-
-    >>> from lightning import seed_everything
-    >>> print('ignore_line'); seed_everything(0)  # doctest: +ELLIPSIS
-    ignore_line...
-    >>> domain_mapping = {
-    ...     0: {0: [10, 11]},          # First camera, one subject with two images
-    ...     1: {1: [20, 30], 2: [40]}  # Second camera, two subjects with two and one image each
-    ... }
-    >>> label_mapping = {
-    ...     100: {0: [10, 11], 1: [20]},      # Images 10, 11 and 20 have label 100
-    ...     200: {0: [10], 1: [30], 2: [40]}  # Images 10, 30 and 40 have label 200
-    ... }
-    >>> hierarchical_bootstrapping_labels(domain_mapping, label_mapping, n_labels=2, n_bootstraps=4)
-    tensor([[20, 10, 30, 10],
-            [20, 10, 20, 11],
-            [20, 11, 20, 11],
-            [30, 10, 20, 11]])
-
-    Args:
-        domain_mapping: Domain to subjects to images mapping.
-        label_mapping: Label to subjects to images mapping.
-        n_labels: Number of labels to draw with replacement. For each label, images from n_domains will be selected.
-        n_bootstraps: Total number of bootstraps.
-
-    Returns: Matrix of shape (n_bootstraps, n_domains * n_labels) with the bootstraps. It contains the values provided for the images (final layer in the mappings).
-    """
-    n_domains = len(set(domain_mapping.keys()))
-    subjects2domain = {s: d for d, subjects in domain_mapping.items() for s in subjects}
-    for label, subjects in label_mapping.items():
-        assert (
-            len({subjects2domain[s] for s in subjects}) == n_domains
-        ), f"Label {label} is not present in all domains (only the subjects {subjects} have this label)"
-
-    # We are generating a random number which will be used as seed during bootstraping
-    # This produces different bootstraps when the user calls this function multiple times while still allowing to set a seed
-    seed = torch.randint(0, torch.iinfo(torch.int32).max, (1,), dtype=torch.int32).item()
-    bootstraps = htc._cpp.hierarchical_bootstrapping_labels(domain_mapping, label_mapping, n_labels, n_bootstraps, seed)
-    assert bootstraps.shape == (n_bootstraps, n_domains * n_labels)
-
-    return bootstraps
-
-
-@automatic_numpy_conversion
-def colorchecker_automask(
-    rot_image: torch.Tensor,
-    cc_board: str,
-    square_size: int,
-    safety_margin: int,
-    square_dist_vertical: int,
-    square_dist_horizontal: int,
-) -> dict[str, dict[str, int]]:
-    """
-    Automatically detect colorchecker chips. See the ColorcheckerReader class for usage of this function.
-    """
-    assert rot_image.ndim == 3, "The image must be three-dimensional"
-    assert rot_image.dtype == torch.float32, "The image must be of type torch.float32"
-
-    return htc._cpp.colorchecker_automask(
-        rot_image, cc_board, square_size, safety_margin, square_dist_vertical, square_dist_horizontal
-    )
-
-
-@automatic_numpy_conversion
-def colorchecker_automask_search_area(
-    rot_image: torch.Tensor,
-    cc_board: str,
-    square_size: int,
-    safety_margin: int,
-    square_dist_vertical: int,
-    square_dist_horizontal: int,
-) -> torch.Tensor:
-    """
-    Visualize the search are of the automatic colorchecker detection algorithm. See the ColorcheckerReader class for usage of this function.
-    """
-    assert rot_image.ndim == 3, "The image must be three-dimensional"
-    assert rot_image.dtype == torch.float32, "The image must be of type torch.float32"
-
-    return htc._cpp.colorchecker_automask_search_area(
-        rot_image, cc_board, square_size, safety_margin, square_dist_vertical, square_dist_horizontal
-    )
```

## htc/cpp/bindings.cpp

```diff
@@ -6,23 +6,18 @@
 #include "hierarchical_bootstrapping.h"
 
 std::tuple<torch::Tensor, torch::Tensor> spxs_predictions(torch::Tensor& spxs, torch::Tensor& labels, torch::Tensor& mask, int n_classes);
 torch::Tensor segmentation_mask(const torch::Tensor& label_image, std::map<std::tuple<int, int, int>, int>& color_mapping);
 std::vector<std::vector<int>> kfold_combinations(const std::vector<int>& subject_indices, const std::map<int, std::vector<int>>& subject_labels, int min_labels, int n_groups = 5);
 torch::Tensor nunique(const torch::Tensor& in, int64_t dim);
 torch::Tensor map_label_image(const torch::Tensor& label_image, std::unordered_map<int64_t, std::tuple<float, float, float, float>>& label_color_mapping);
-std::map<std::string, std::map<std::string, int64_t>> colorchecker_automask(const at::Tensor& rot_image, const std::string& cc_board, int square_size, int safety_margin, int square_dist_vertical, int square_dist_horizontal);
-at::Tensor colorchecker_automask_search_area(const at::Tensor& rot_image, const std::string& cc_board, int square_size, int safety_margin, int square_dist_vertical, int square_dist_horizontal);
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
   m.def("spxs_predictions", &spxs_predictions, "Superpixel prediction based on the modal value.");
   m.def("segmentation_mask", &segmentation_mask, "Creates a segmentation mask based on a label image and a corresponding color mapping.");
   m.def("tensor_mapping_integer", &tensor_mapping_integer, "Remaps values of a tensor based on a dict.");
   m.def("tensor_mapping_floating", &tensor_mapping_floating, "Remaps values of a tensor based on a dict.");
   m.def("kfold_combinations", &kfold_combinations, "Evaluate kfold combinations.");
   m.def("nunique", &nunique, "Counts unique elements along dim.");
   m.def("map_label_image", &map_label_image, "Create color images based on label images (map label ids to colors).");
-  m.def("hierarchical_bootstrapping", &hierarchical_bootstrapping, "Create hierarchical bootstrapping combinations.");
-  m.def("hierarchical_bootstrapping_labels", &hierarchical_bootstrapping_labels, "Create hierarchical bootstrapping combinations including label information.");
-  m.def("colorchecker_automask", &colorchecker_automask, "Automatically find a mask for colorchecker images.");
-  m.def("colorchecker_automask_search_area", &colorchecker_automask_search_area, "Visualize the search area of the colorchecker automask.");
+  m.def("hierarchical_bootstrapping", &hierarchical_bootstrapping, "Create hierarchical_bootstrapping combinations.");
 }
```

## htc/cpp/hierarchical_bootstrapping.h

```diff
@@ -1,107 +1,48 @@
 // SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 // SPDX-License-Identifier: MIT
 
 #include <torch/extension.h>
 #include <random>
-#include <unordered_map>
 
-using Domain2Subjects = std::unordered_map<int64_t, std::vector<int64_t>>;
+using Cam2Subjects = std::unordered_map<int64_t, std::vector<int64_t>>;
 using Subject2Images = std::unordered_map<int64_t, std::vector<int64_t>>;
-using Domain2Subjects2Images = std::unordered_map<int64_t, Subject2Images>;
-using Label2Subjects2Images = std::unordered_map<int64_t, Subject2Images>;
+using Cam2Subjects2Images = std::unordered_map<int64_t, Subject2Images>;
 
-torch::Tensor hierarchical_bootstrapping(Domain2Subjects2Images& mapping, int n_subjects, int n_images, int n_bootstraps, unsigned int seed) {
+torch::Tensor hierarchical_bootstrapping(Cam2Subjects2Images mapping, int n_subjects, int n_images, int n_bootstraps, unsigned int seed) {
     std::mt19937 gen(seed); // Offers a good uniform distribution (https://www.boost.org/doc/libs/1_61_0/doc/html/boost_random/reference.html#boost_random.reference.generators)
     
-    auto n_domains = mapping.size();
-    auto bootstraps = torch::empty({n_bootstraps, static_cast<int64_t>(n_domains * n_subjects * n_images)}, torch::kInt64);
+    auto n_cams = mapping.size();
+    auto bootstraps = torch::empty({n_bootstraps, static_cast<int64_t>(n_cams * n_subjects * n_images)}, torch::kInt64);
     auto bootstraps_a = bootstraps.accessor<int64_t, 2>();
     
-    // Cache domain2subjects vector mapping for later use (we don't want to do this all over again inside the bootstrap loop)
-    Domain2Subjects domain2subjects;
-    for (const auto &[domain_index, subject2images]: mapping) {
-        domain2subjects[domain_index].reserve(subject2images.size());
-        for (auto const& p: subject2images) {
-            domain2subjects[domain_index].push_back(p.first);
+    // Cache cam_to_subjects vector mapping for later use (we don't want to do this all over again inside the bootstrap loop)
+    Cam2Subjects cam_to_subjects;
+    for (const auto &[camera_index, subject_to_images]: mapping) {
+        cam_to_subjects[camera_index].reserve(subject_to_images.size());
+        for (auto const& p: subject_to_images) {
+            cam_to_subjects[camera_index].push_back(p.first);
         }
     }
     
     for (int b = 0; b < n_bootstraps; ++b) {
         int col = 0;
 
-        for (auto &[domain_index, subject2images]: mapping) {
-            std::vector<int64_t>& subjects = domain2subjects[domain_index];
+        for (auto &[camera_index, subject_to_images]: mapping) {
+            std::vector<int64_t>& subjects = cam_to_subjects[camera_index];
             
             std::uniform_int_distribution<> random_subject(0, subjects.size() - 1);
             
             for (int subject_index = 0; subject_index < n_subjects; ++subject_index) {
                 auto& subject = subjects[random_subject(gen)];
-                auto& images = subject2images[subject];
+                auto& images = subject_to_images[subject];
                 std::uniform_int_distribution<> random_image(0, images.size() - 1);
                 
                 for (int image_index = 0; image_index < n_images; ++image_index) {
                     bootstraps_a[b][col++] = images[random_image(gen)];
                 }
             }
         }
     }
     
     return bootstraps;
 }
-
-torch::Tensor hierarchical_bootstrapping_labels(Domain2Subjects2Images& domain_mapping, Label2Subjects2Images& label_mapping, int n_labels, int n_bootstraps, unsigned int seed) {
-    std::mt19937 gen(seed); // Offers a good uniform distribution (https://www.boost.org/doc/libs/1_61_0/doc/html/boost_random/reference.html#boost_random.reference.generators)
-
-    auto n_domains = domain_mapping.size();
-    auto bootstraps = torch::empty({ n_bootstraps, static_cast<int64_t>(n_domains * n_labels) }, torch::kInt64);
-    auto bootstraps_a = bootstraps.accessor<int64_t, 2>();
-
-    // Cache domain2subjects vector mapping for later use (we don't want to do this all over again inside the bootstrap loop)
-    Domain2Subjects domain2subjects;
-    for (const auto& [domain_index, subject2images] : domain_mapping) {
-        domain2subjects[domain_index].reserve(subject2images.size());
-        for (auto const& p : subject2images) {
-            domain2subjects[domain_index].push_back(p.first);
-        }
-    }
-
-    // List of possible labels
-    std::vector<int64_t> labels;
-    labels.reserve(label_mapping.size());
-    for (auto& item : label_mapping) {
-        labels.push_back(item.first);
-    }
-    std::uniform_int_distribution<> random_label(0, labels.size() - 1);
-
-    for (int b = 0; b < n_bootstraps; ++b) {
-        int col = 0;
-
-        // For each label, we select per domain one subject and one image and repeat this process n_labels times
-        while (col < bootstraps.size(1)) {
-            auto label = labels[random_label(gen)];
-            auto& label_subjects = label_mapping[label];
-
-            for (auto& [domain_index, subject2images] : domain_mapping) {
-                std::vector<int64_t>& subjects_domain = domain2subjects[domain_index];
-
-                // Select the subjects which have images of the current label
-                std::vector<int64_t> subjects;
-                subjects.reserve(subjects_domain.size());
-                std::copy_if(subjects_domain.begin(), subjects_domain.end(), std::back_inserter(subjects), [&](auto s) {
-                    return label_subjects.contains(s);
-                });
-
-                // Select random subject
-                std::uniform_int_distribution<> random_subject(0, subjects.size() - 1);
-                auto subject = subjects[random_subject(gen)];
-
-                // Select random image
-                auto& images = label_subjects[subject];
-                std::uniform_int_distribution<> random_image(0, images.size() - 1);
-                bootstraps_a[b][col++] = images[random_image(gen)];
-            }
-        }
-    }
-
-    return bootstraps;
-}
```

## htc/data_processing/run_l1_normalization.py

```diff
@@ -17,15 +17,15 @@
 
 class L1Normalization(DatasetIteration):
     def __init__(self, paths: list[DataPath], file_type: str, output_dir: Path = None, regenerate: bool = False):
         super().__init__(paths)
         self.file_type = file_type
 
         if output_dir is None:
-            self.output_dir = settings.intermediates_dir_all / "preprocessing" / "L1"
+            self.output_dir = settings.intermediates_dir / "preprocessing" / "L1"
         else:
             self.output_dir = output_dir / "L1"
         self.output_dir.mkdir(exist_ok=True, parents=True)
 
         config = Config(
             {
                 "input/normalization": "L1",
```

## htc/data_processing/run_median_spectra.py

```diff
@@ -1,14 +1,13 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import itertools
 from pathlib import Path
 
-import numpy as np
 import pandas as pd
 import torch
 
 from htc.data_processing.DatasetIteration import DatasetIteration
 from htc.models.image.DatasetImage import DatasetImage
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
@@ -24,15 +23,15 @@
         # It does not make any sense to load images where we don't have any labels as we then also can't compute any median spectra
         paths = [p for p in paths if filter_min_labels(p)]
         assert len(paths) > 0, "No paths left for median spectra computation"
         super().__init__(paths)
         self.dataset_name = dataset_name
 
         if output_dir is None:
-            self.output_dir = settings.intermediates_dir_all / "tables"
+            self.output_dir = settings.intermediates_dir / "tables"
         else:
             self.output_dir = output_dir
         self.output_dir.mkdir(exist_ok=True, parents=True)
 
         config = Config(
             {
                 "input/n_channels": 100,
@@ -49,22 +48,14 @@
         annotation_meta = path.read_annotation_meta()
         if annotation_meta is None:
             annotation_meta = {}
 
         features_normalized = sample["features"] / torch.linalg.norm(sample["features"], ord=1, dim=-1, keepdim=True)
         features_normalized.nan_to_num_()
 
-        cube = sample["features"].numpy()
-        sto2_img = path.compute_sto2(cube)
-        nir_img = path.compute_nir(cube)
-        twi_img = path.compute_twi(cube)
-        ohi_img = path.compute_ohi(cube)
-        thi_img = path.compute_thi(cube)
-        tli_img = path.compute_tli(cube)
-
         rows = []
         for label_key in sample.keys():
             if not label_key.startswith("labels"):
                 continue
 
             for label_index, counts in zip(*sample[label_key].unique(return_counts=True)):
                 label_index = label_index.item()
@@ -82,44 +73,25 @@
                         selection = selection[valid_dim].squeeze(dim=0)
                     else:
                         selection = sample[label_key] == label_index
 
                     spectra = sample["features"][selection]
                     spectra_normalized = features_normalized[selection]
 
-                    selected_sto2 = sto2_img[selection]
-                    selected_nir = nir_img[selection]
-                    selected_twi = twi_img[selection]
-                    selected_ohi = ohi_img[selection]
-                    selected_thi = thi_img[selection]
-                    selected_tli = tli_img[selection]
-
                     current_row = {"image_name": path.image_name()}
                     current_row |= path.image_name_typed()
 
                     current_row |= {
                         "label_index": label_index,
                         "label_name": label_name,
                         "median_spectrum": spectra.quantile(q=0.5, dim=0).numpy(),  # Same as np.median
                         "std_spectrum": spectra.std(dim=0).numpy(),
                         "median_normalized_spectrum": spectra_normalized.quantile(q=0.5, dim=0).numpy(),
                         "std_normalized_spectrum": spectra_normalized.std(dim=0).numpy(),
                         "n_pixels": counts.item(),
-                        "median_sto2": np.median(selected_sto2.data),
-                        "std_sto2": np.std(selected_sto2.data),
-                        "median_nir": np.median(selected_nir.data),
-                        "std_nir": np.std(selected_nir.data),
-                        "median_twi": np.median(selected_twi.data),
-                        "std_twi": np.std(selected_twi.data),
-                        "median_ohi": np.median(selected_ohi.data),
-                        "std_ohi": np.std(selected_ohi.data),
-                        "median_thi": np.median(selected_thi.data),
-                        "std_thi": np.std(selected_thi.data),
-                        "median_tli": np.median(selected_tli.data),
-                        "std_tli": np.std(selected_tli.data),
                     }
 
                     if label_key != "labels":
                         current_row["annotation_name"] = label_key.removeprefix("labels_")
 
                     # Already add additional meta labels if we have it
                     for k, v in annotation_meta.items():
@@ -166,14 +138,14 @@
             df.to_feather(target_file)
             settings.log.info(f"Wrote median table to {target_file}")
 
 
 if __name__ == "__main__":
     prep = ParserPreprocessing(description="Calculate median spectra per pig, organ and image")
     paths = prep.get_paths()
-    assert prep.args.spec is None, (
+    assert prep.args.specs is None, (
         "Median spectra can only be calculated per dataset and not for a specification file. Otherwise, the label_index"
         " would not be correct (the same label_index can refer to different organs across datasets)."
     )
 
     settings.log.info(f"Computing median spectra for {len(paths)} paths...")
     MedianSpectra(paths, prep.args.dataset_name, output_dir=prep.args.output_path).run()
```

## htc/data_processing/run_parameter_images.py

```diff
@@ -15,15 +15,15 @@
 
 class ParameterImages(DatasetIteration):
     def __init__(self, paths: list[DataPath], file_type: str, output_dir: Path = None, regenerate: bool = False):
         super().__init__(paths)
         self.file_type = file_type
 
         if output_dir is None:
-            self.output_dir = settings.intermediates_dir_all / "preprocessing" / "parameter_images"
+            self.output_dir = settings.intermediates_dir / "preprocessing" / "parameter_images"
         else:
             self.output_dir = output_dir / "parameter_images"
         self.output_dir.mkdir(exist_ok=True, parents=True)
 
         if regenerate:
             clear_directory(self.output_dir)
```

## htc/data_processing/run_standardization.py

```diff
@@ -79,24 +79,24 @@
     return results
 
 
 def calc_standardization_folds(specs: DataSpecification) -> dict[str, dict[str, float]]:
     fold_datasets = [specs.folds[f] for f in specs.fold_names()]
     results = p_map(calc_standardization, fold_datasets)
 
-    return dict(zip(specs.fold_names(), results))
+    return {fold_name: data for fold_name, data in zip(specs.fold_names(), results)}
 
 
 if __name__ == "__main__":
     prep = ParserPreprocessing(description="Precomputes a filter for all images")
     paths = prep.get_paths()  # Must always be called
     assert (
-        prep.args.spec is not None
-    ), "The --spec argument must be supplied so that the standardization parameters can be calculated per fold"
+        prep.args.specs is not None
+    ), "The --specs argument must be supplied so that the standardization parameters can be calculated per fold"
 
-    specs = DataSpecification(prep.args.spec)
+    specs = DataSpecification(prep.args.specs)
     assert paths == specs.paths()
     results = calc_standardization_folds(specs)
 
-    target_dir = settings.intermediates_dir_all / "data_stats"
+    target_dir = settings.intermediates_dir / "data_stats"
     target_dir.mkdir(parents=True, exist_ok=True)
     pickle.dump(results, open(target_dir / f"{specs.name()}#standardization.pkl", "wb"))
```

## htc/data_processing/run_superpixel_prediction.py

```diff
@@ -34,17 +34,15 @@
         "nsd": metrics["surface_dice_metric_image"],
         "subject_name": subject_name,
         "timestamp": timestamp,
     }
 
 
 if __name__ == "__main__":
-    config = Config.from_model_name("default", "superpixel_classification")
-    config["input/no_features"] = True
-
+    config = Config.load_config("default", "superpixel_classification")
     paths = list(DataPath.iterate(settings.data_dirs.semantic))
     dataset_all = DatasetImage(paths, train=False, config=config)
     tolerances = get_nsd_thresholds(settings_seg.label_mapping)
 
     with threadpool_limits(1):
         rows = p_map(aggregate_results, range(len(dataset_all)))
```

## htc/evaluation/analyze_tfevents.py

```diff
@@ -48,15 +48,15 @@
             assert loss_tags == current_loss_tags, f"Each fold must have the same loss tag (run_dir={run_dir})"
 
         assert len(loss_tags) > 0, f"At least one loss key required for {run_dir}"
 
         # Map steps to epochs
         steps_to_epoch = {}
         for e in acc.Scalars("epoch"):
-            # There might be more than step for the same epoch in which case we only take the first value (https://github.com/Lightning-AI/lightning/issues/12851)
+            # There might be more than step for the same epoch in which case we only take the first value (https://github.com/PyTorchLightning/pytorch-lightning/issues/12851)
             if e.step not in steps_to_epoch:
                 steps_to_epoch[e.step] = int(e.value)
 
         # First create a mapping from steps to values for each tag (the steps may differ between the tags)
         steps = set()
         tag_values = {}
         for tag in loss_tags:
```

## htc/evaluation/evaluate_images.py

```diff
@@ -1,20 +1,19 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import math
 import warnings
 
-import numpy as np
 import torch
 import torch.nn.functional as F
 from monai.metrics import SurfaceDistanceMetric, compute_dice, compute_surface_dice
 from torchmetrics.functional import confusion_matrix
 
-from htc.evaluation.metrics.ECE import ECE
+from htc.evaluation.metrics.ECELoss import ECELoss
 from htc.settings import settings
 from htc.settings_seg import settings_seg
 
 
 def calc_surface_dice(
     predictions_labels: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor, tolerances: list[float]
 ) -> list[dict]:
@@ -33,20 +32,15 @@
         predictions_labels.shape == labels.shape and predictions_labels.shape == mask.shape
     ), "All input tensors must have the same shape"
     assert predictions_labels.dim() == 3, "Each tensor must have three dimensions (batch, height, width)"
     assert (
         predictions_labels.dtype == torch.int64 and labels.dtype == torch.int64
     ), "Predictions and labels must be label index values"
     assert mask.dtype == torch.bool, "The mask must be a boolean tensor"
-    assert all(t >= 0 for t in tolerances), "The tolerance values must be non-negative"
-
-    # Unfortunately, the NSD can only be computed on the CPU
-    predictions_labels = predictions_labels.cpu()
-    labels = labels.cpu()
-    mask = mask.cpu()
+    assert all([t >= 0 for t in tolerances]), "The tolerance values must be non-negative"
 
     # Copy the tensors since we need to modify them for the masking
     predictions_labels = predictions_labels.clone()
     labels = labels.clone()
 
     # The invalid labels are assigned a new dummy class which does not influence the calculation
     invalid_label_index = max(predictions_labels[mask].max(), labels[mask].max()) + 1
@@ -160,19 +154,14 @@
     ), "All input tensors must have the same shape"
     assert predictions_labels.dim() == 3, "Each tensor must have three dimensions (batch, height, width)"
     assert (
         predictions_labels.dtype == torch.int64 and labels.dtype == torch.int64
     ), "Predictions and labels must be label index values"
     assert mask.dtype == torch.bool, "The mask must be a boolean tensor"
 
-    # Unfortunately, the ASD can only be computed on the CPU
-    predictions_labels = predictions_labels.cpu()
-    labels = labels.cpu()
-    mask = mask.cpu()
-
     # Copy the tensors since we need to modify them for the masking
     predictions_labels = predictions_labels.clone()
     labels = labels.clone()
 
     # The invalid labels are assigned a new dummy class which does not influence the calculation
     invalid_label_index = max(predictions_labels[mask].max(), labels[mask].max()) + 1
     predictions_labels[~mask] = invalid_label_index
@@ -247,37 +236,27 @@
 def evaluate_images(
     predictions: torch.Tensor,
     labels: torch.Tensor,
     mask: torch.Tensor,
     metrics: list = None,
     n_classes: int = None,
     tolerances: list[float] = None,
-    confidence_thresholds: list[float] = None,
 ) -> list[dict]:
     """
     Evaluate all images in the batch dimension of the tensors.
 
     The goal is to provide a function which can be used by every model (even when validating during training) so that the validation is always the same. The metrics calculated by this function are the most common one and should always be included in the validation.
 
     Args:
         predictions: Predictions of the model either as labels (batch, height, width) or softmax (of logits) (batch, channel, height, width). ece loss is only calculated when providing the softmax.
         labels: Target labels (batch, height, width).
         mask: Pixels to include (batch, height, width). You can for example also use it to exclude specific classes (e.g. the background=0) in the calculation by setting the mask for these pixels to False.
         n_classes: Number of classes in the dataset. This value should be the same for every image in the dataset and is e.g. necessary for the confusion matrix where the shape is determined by the number of classes (and is only comparable if this number is the same for all images).
-        metrics: A list of metrics to be calculated, this list can contain the following:
-        - DSC = Dice Similarity Coefficient
-        - CM = Confusion Matrix
-        - ECE = Expected Calibration Error
-        - NSD = Normalized Surface Dice
-        - ASD = Average Surface Distance
-        - DSC_confidences = DSC for different confidence thresholds. For each threshold t, the pixels with confidence < t are ignored in the DSC calculation (invalid pixels). As higher the threshold, as more pixels are ignored. The result for each image is a dictionary with the threshold as key and the DSC and area as values. If a class is removed completely, the area is set to 0 and the DSC to nan. The `aggregated_confidences_table()` function can be used to get aggregated results for all thresholds (based on a validation or test table).
-
-        If None, DSC, ECE (if softmaxes are provided) and CM will be calculated.
+        metrics: A list of metrics to be calculated, this list can contain the following: DSC = dice similarity coefficient, CM = confusion matrix, ECE = expected calibration error, NSD = normalized surface dice, ASD = average surface distance. If None, DSC, ECE (if softmaxes are provided) and CM will be calculated.
         tolerances: Parameter for the NSD metric: the tolerance threshold in pixels for each class. All pixels in the distance of this threshold will count as correct. Higher numbers yield better results.
-        confidence_thresholds: Confidence thresholds to use for the DSC_confidences metric. If None, the thresholds [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] will be used.
 
     Returns: All calculated metrics as a dict for each element in the batch.
     """
     if metrics is None:
         metrics = ["DSC", "ECE", "CM"]
     assert (
         len(predictions.shape) == 4 or len(predictions.shape) == 3
@@ -318,50 +297,51 @@
 
     # All metrics should only consider valid pixels
     valid_labels = [labels[b, mask[b]] for b in range(n_batch)]
     valid_predictions_labels = [predictions_labels[b, mask[b]] for b in range(n_batch)]
 
     result_batch = {}
 
-    # This is important later when calculating the dice per image (to know which values to exclude)
-    used_labels = [valid_labels[b].unique() for b in range(n_batch)]
+    used_labels = [
+        valid_labels[b].unique() for b in range(n_batch)
+    ]  # This is important later when calculating the dice per image (to know which values to exclude)
 
-    # Dice Similarity Coefficient
+    # dice similarity coefficient
     if "DSC" in metrics:
         dice = calc_dice_metric(predictions_labels, labels, mask)
         assert len(used_labels) == len(dice)
-        assert all(torch.all(used_labels[b] == dice[b]["used_labels"]) for b in range(n_batch))
+        assert all([torch.all(used_labels[b] == dice[b]["used_labels"]) for b in range(n_batch)])
 
         result_batch |= {
             "dice_metric": [b["dice_metric"] for b in dice],
             "dice_metric_image": [b["dice_metric_image"] for b in dice],
         }
 
     # confusion matrix
     if "CM" in metrics:
         conf_mat = [
             confusion_matrix(valid_predictions_labels[b], valid_labels[b], task="multiclass", num_classes=n_classes)
             for b in range(n_batch)
         ]
-        assert all(c.shape == (n_classes, n_classes) for c in conf_mat), "The confusion matrix has the wrong shape"
+        assert all([c.shape == (n_classes, n_classes) for c in conf_mat]), "The confusion matrix has the wrong shape"
 
         result_batch |= {
             "confusion_matrix": conf_mat,
         }
 
     # expected calibration error
     if predictions_softmaxes is not None and "ECE" in metrics:
         # The losses can only be calculated if we have the softmaxes
         valid_predictions_softmaxes = [predictions_softmaxes[b, :, mask[b]] for b in range(n_batch)]
         assert all(
-            len(t.shape) == 2 for t in valid_predictions_softmaxes
+            [len(t.shape) == 2 for t in valid_predictions_softmaxes]
         ), "Invalid shape of the valid predicted softmaxes"
 
         ece = []
-        ece_model = ECE()
+        ece_model = ECELoss()
         for b in range(n_batch):
             ece_result = ece_model(valid_predictions_softmaxes[b], valid_labels[b])
             ece.append(ece_result)
 
         result_batch["ece"] = ece
 
     # normalized surface dice
@@ -381,63 +361,16 @@
         asd = calc_surface_distance(predictions_labels, labels, mask)
 
         result_batch |= {
             "surface_distance_metric": [b["surface_distance_metric"] for b in asd],
             "surface_distance_metric_image": [b["surface_distance_metric_image"] for b in asd],
         }
 
-    if predictions_softmaxes is not None and "DSC_confidences" in metrics:
-        confidences = predictions.max(dim=1).values
-        thresholds = np.arange(0, 1, 0.1) if confidence_thresholds is None else confidence_thresholds
-        conf_results = [{} for _ in range(n_batch)]
-
-        for t in thresholds:
-            # We repeat the DSC calculation for each threshold basically exchanging the mask
-            # Shrink the existing mask by the values which are excluded by the confidence threshold
-            mask_t = mask.clone()
-            mask_t.masked_fill_(confidences < t, False)
-            valid_images = mask_t.any(dim=1).any(dim=1)
-            if valid_images.any():
-                dice_results = calc_dice_metric(predictions_labels, labels, mask_t)
-
-            for b in range(n_batch):
-                if not valid_images[b]:
-                    # If an image does not contain any valid pixels anymore, fill with 0 area and nan DSC
-                    res = {
-                        "areas": torch.zeros(len(used_labels[b]), device=used_labels[b].device),
-                        "dice_metric": torch.full((len(used_labels[b]),), torch.nan, device=used_labels[b].device),
-                    }
-                else:
-                    # The length of the areas and DSC values remain the same for all thresholds even if a label does not exist anymore
-                    areas = []
-                    dice_values = []
-                    for l in used_labels[b]:
-                        if l in dice_results[b]["used_labels"]:
-                            current_labels_all = labels[b] == l
-                            current_labels_conf = labels[b][mask_t[b]] == l
-                            n_total = torch.count_nonzero(current_labels_all)
-                            n_remaining = torch.count_nonzero(current_labels_conf)
-
-                            areas.append(n_remaining / n_total)
-                            dice_values.append(
-                                dice_results[b]["dice_metric"][dice_results[b]["used_labels"] == l].squeeze()
-                            )
-                        else:
-                            areas.append(torch.tensor(0, device=used_labels[b].device))
-                            dice_values.append(torch.tensor(torch.nan, device=used_labels[b].device))
-
-                    res = {
-                        "areas": torch.stack(areas),
-                        "dice_metric": torch.stack(dice_values),
-                    }
-
-                conf_results[b][t] = res
-        result_batch["DSC_confidences"] = conf_results
-
     result_batch |= {
         "used_labels": used_labels,
     }
 
-    # Convert from dict of lists to list of dicts (https://stackoverflow.com/a/33046935/2762258)
-    results_batch = [dict(zip(result_batch, t)) for t in zip(*result_batch.values())]
+    results_batch = [
+        dict(zip(result_batch, t)) for t in zip(*result_batch.values())
+    ]  # Convert from dict of lists to list of dicts (https://stackoverflow.com/a/33046935/2762258)
 
     return results_batch
```

## htc/evaluation/run_table_generation.py

```diff
@@ -11,15 +11,15 @@
 import numpy as np
 import pandas as pd
 
 from htc.models.data.DataSpecification import DataSpecification
 from htc.models.image.DatasetImage import DatasetImage
 from htc.settings import settings
 from htc.utils.Config import Config
-from htc.utils.helper_functions import checkpoint_path, execute_notebook, get_valid_run_dirs
+from htc.utils.helper_functions import checkpoint_path, get_valid_run_dirs, run_experiment_notebook
 from htc.utils.parallel import p_map
 
 
 def merge_fold_tables(run_dir: Path, table_filename: str) -> pd.DataFrame:
     fold_dirs = sorted(run_dir.glob("fold*"))
     table_files = sorted(run_dir.rglob(table_filename))
     assert len(fold_dirs) == len(table_files), (
@@ -48,44 +48,44 @@
                 )
 
         all_dfs.append(df)
 
     return pd.concat(all_dfs)
 
 
-def generate_validation_table(run_dir: Path, table_stem: str = "validation_results") -> pd.DataFrame:
+def generate_validation_table(run_dir: Path) -> pd.DataFrame:
     def validation_table_npz(run_dir: Path) -> pd.DataFrame:
         # First collect all possible metric names
         metric_names = set()
 
         for fold_dir in sorted(run_dir.glob("fold*")):
-            results_path = fold_dir / f"{table_stem}.npz"
+            results_path = fold_dir / "validation_results.npz"
             assert results_path.exists(), f"The run {fold_dir} does not contain any results"
 
             data = np.load(results_path, allow_pickle=True)["data"]
             for epoch_index, epoch_data in enumerate(data):
                 for dataset_index, dataset in enumerate(epoch_data.values()):
                     for img_data in dataset.values():
                         metric_names.update(img_data.keys())
 
         # Then collect the actual result values
         rows = []
         metric_names = sorted(metric_names)
 
         for fold_dir in sorted(run_dir.glob("fold*")):
-            results_path = fold_dir / f"{table_stem}.npz"
+            results_path = fold_dir / "validation_results.npz"
             config = Config(fold_dir / "config.json")
             _, best_epoch_index = checkpoint_path(fold_dir)
 
             # Generate a nice table based on the validation data structure
             data = np.load(results_path, allow_pickle=True)["data"]
             if len(data) < config["trainer_kwargs/max_epochs"]:
                 settings.log.warning(
                     f"The number of epochs in the validation data ({len(data)}) is smaller than the epoch length in the"
-                    f" config file {config.path_config} ({config['trainer_kwargs/max_epochs']}) for the run {run_dir}"
+                    f' config file {config.path_config} ({config["trainer_kwargs/max_epochs"]}) for the run {run_dir}'
                 )
 
             for epoch_index, epoch_data in enumerate(data):
                 for dataset_index, dataset in enumerate(epoch_data.values()):
                     for image_name, img_data in dataset.items():
                         current_row = [epoch_index, best_epoch_index, dataset_index, fold_dir.name, image_name]
                         for metric_name in metric_names:
@@ -96,20 +96,20 @@
 
                         rows.append(current_row)
 
         return pd.DataFrame(
             rows, columns=["epoch_index", "best_epoch_index", "dataset_index", "fold_name", "image_name"] + metric_names
         )
 
-    if len(sorted(run_dir.rglob(f"{table_stem}.npz"))) > 0:
-        # E.g. old segmentation tasks
+    if len(sorted(run_dir.rglob("validation_results.npz"))) > 0:
+        # E.g. segmentation tasks
         return validation_table_npz(run_dir)
     else:
         # E.g. camera problem
-        return merge_fold_tables(run_dir, f"{table_stem}.pkl.xz")
+        return merge_fold_tables(run_dir, "validation_results.pkl.xz")
 
 
 def save_validation_table(run_dir: Path) -> None:
     """
     Saves a generated table containing the validation results from all folds.
 
     Args:
@@ -119,22 +119,14 @@
     if table_path.exists():
         # Skip run if results are already aggregated
         return None
 
     df = generate_validation_table(run_dir)
     df.to_pickle(table_path)
 
-    # Also merge additional validation tables in case they are available
-    additional_results = sorted(run_dir.rglob("*validation_results_*"))
-    if len(additional_results) > 0:
-        for stem in {f.name.split(".")[0] for f in additional_results}:
-            print(stem)
-            df = generate_validation_table(run_dir, table_stem=stem)
-            df.to_pickle(run_dir / f"{stem.replace('results', 'table')}.pkl.xz")
-
 
 def save_test_table(run_dir: Path) -> None:
     """
     Saves a generated table containing the test results from all folds.
 
     Args:
         run_dir: Path to the run folder with subfolders for each fold.
@@ -153,15 +145,15 @@
     # Check whether necessary files are available
     necessary_files = [
         "config.json",
         "log.txt",
         "*ckpt",
         "events.out.tfevents*",
         "system_log*.json",
-        "validation_results.*",
+        "validation_results*",
     ]
     error_occurred = False
 
     fold_dirs = []
     for d in sorted(run_dir.iterdir()):
         if d.is_file():
             continue
@@ -181,31 +173,28 @@
         for fold_dir in fold_dirs:
             if len(list(fold_dir.glob(f"{wildcard}"))) != 1:
                 settings.log.warning(f"Not exactly one {wildcard} file found in the run directory {fold_dir}")
                 error_occurred = True
 
     # Check log files
     for fold_dir in fold_dirs:
-        log_path = fold_dir / "log.txt"
-        if not log_path.exists():
-            settings.log.error(f"Could not find the log file for the fold {fold_dir}")
+        with (fold_dir / "log.txt").open() as f:
+            log_text = f.read()
+
+        if "WARNING" in log_text:
+            settings.log.warning(f"The log of the fold {fold_dir} contains warnings")
+        if "ERROR" in log_text or "CRITICAL" in log_text:
+            settings.log.error(f"The log of the fold {fold_dir} contains errors")
             error_occurred = True
-        else:
-            with log_path.open() as f:
-                log_text = f.read()
-
-            if "WARNING" in log_text:
-                settings.log.warning(f"The log of the fold {fold_dir} contains warnings")
-            if "ERROR" in log_text or "CRITICAL" in log_text:
-                settings.log.error(f"The log of the fold {fold_dir} contains errors")
-                error_occurred = True
 
     # Check config files
     try:
-        config = Config.from_fold(run_dir)  # This also checks that the config files inside the folds are identical
+        config = Config.load_config_fold(
+            run_dir
+        )  # This also checks that the config files inside the folds are identical
         config.save_config(run_dir / "config.json")
     except AssertionError:
         settings.log.exception("Error in config loading")
         error_occurred = True
 
     # Check data files
     for fold in fold_dirs:
@@ -246,15 +235,15 @@
         )
         error = True
 
     # Find columns where we can compute unique
     hashable_columns = []
     for c in df_val.columns:
         try:
-            df_val[c].unique()
+            df_val.iloc[:2][c].unique()
             hashable_columns.append(c)
         except TypeError:
             pass
 
     # The ID columns should be identical across epochs
     unique_counts = df_val[hashable_columns].groupby("epoch_index").nunique()
     unique_counts = unique_counts.drop(columns=[c for c in unique_counts.columns if not c.endswith("_id")])
@@ -311,21 +300,15 @@
             assert len(fold_paths) > 0
 
             if "fold_name" in df:
                 table_image_names = set(df.query("fold_name == @fold_name")["image_name"].unique())
             else:
                 table_image_names = set(df["image_name"].unique())
 
-            if (df["subject_name"] == df["image_name"]).all() and len(df) == df["subject_name"].nunique():
-                # For some tasks (e.g. tissue atlas), we do not store image-level but only subject-level metrics
-                # In these cases, we only check that the subject names match
-                fold_image_names = {p.subject_name for p in fold_paths}
-            else:
-                fold_image_names = {p.image_name() for p in fold_paths}
-
+            fold_image_names = {p.image_name() for p in fold_paths}
             if fold_image_names != table_image_names:
                 settings.log.error(
                     f"The [var]{set_name}[/] table for the run {run_dir} misses"
                     f" [var]{len(fold_image_names - table_image_names)}[/] paths which are defined in the data"
                     f" specification file (fold_name = [var]{fold_name}[/])"
                 )
                 error = True
@@ -378,36 +361,37 @@
 
     return error
 
 
 def create_experiment_notebooks(run_dir: Path, base_notebook: str) -> None:
     possible_paths = [
         Path(base_notebook),
+        Path.cwd() / base_notebook,
         Path(__file__).parent / base_notebook,
         settings.htc_package_dir / base_notebook,
-        settings.src_dir / base_notebook,
     ]
     input_path = None
     for path in possible_paths:
         if path.exists():
             input_path = path
             break
 
     assert input_path is not None, (
         f"Cannot find the base notebook {base_notebook} which is necessary for the experiment visualizations. Tried at"
         f" the following locations: {possible_paths}"
     )
 
-    # We always use the same output name so that the check for the required files below works
-    output_path = run_dir / "ExperimentAnalysis.html"
+    output_path = run_dir / input_path.name
     if output_path.exists() and output_path.with_suffix(".html").exists():
         return None
 
-    settings.log_once.info(f"Using the notebook {input_path}")
-    execute_notebook(notebook_path=input_path, output_path=output_path, parameters={"run_dir": str(run_dir)})
+    settings.log.info(f"Using the notebook {input_path}")
+    run_experiment_notebook(
+        notebook_path=input_path, output_path=output_path, parameters={"run_dir": str(run_dir)}, html_only=True
+    )
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(
         description=(
             "Run the validation on the trained models. This includes aggregation of the fold information and a"
             " corresponding notebook generation in the results folder"
@@ -418,58 +402,40 @@
         "--notebook",
         default="ExperimentAnalysis.ipynb",
         help=(
             "Relative or absolute path to the notebook for the experiment visualizations (if set to an empty string, no"
             " notebook will be created)."
         ),
     )
-    parser.add_argument(
-        "--input-path",
-        default=None,
-        type=Path,
-        help=(
-            "Explicitly set the path to a run directory or a folder which contains one or more run directories. Files"
-            " will be generated for all found run directories and always re-generated in case tables already exist. A"
-            " run directory is defined as a directory which contains one or more subdirectory with the name fold_. If"
-            " not set, all run directories in the results directory which are missing all required files"
-            " (validation_table.pkl.xz and ExperimentAnalysis.html) are used."
-        ),
-    )
     args = parser.parse_args()
 
-    if args.input_path is None:
-        # Find runs which do not have all of the following required files
-        required_files = ["validation_table.pkl.xz", "ExperimentAnalysis.html"]
-        run_dirs = []
-        for r in get_valid_run_dirs():
-            # If a run should be computed again, delete all of the required files
-            if not all((r / f).exists() for f in required_files):
-                run_dirs.append(r)
-    else:
-        run_dirs = set()
-        for r in sorted(args.input_path.rglob("fold_*")):
-            run_dirs.add(r.parent)
-        run_dirs = sorted(run_dirs)
+    # Find runs which do not have any of the following files
+    required_files = ["validation_table.pkl.xz", "ExperimentAnalysis.html"]
+    run_dirs = []
+    for r in get_valid_run_dirs():
+        # If a run should be computed again, delete all of the required files
+        if not any([(r / f).exists() for f in required_files]):
+            run_dirs.append(r)
 
     if len(run_dirs) > 0:
         settings.log.info("Will generate results for the following runs:")
         for run_dir in run_dirs:
             settings.log.info(f"{run_dir.parent.name}/{run_dir.name}")
 
-        errors = p_map(check_run, run_dirs, task_name="Check for necessary files")
+        errors = p_map(check_run, run_dirs)
         assert not any(errors), "At least one run folder misses some files. Aborting..."
 
-        p_map(save_validation_table, run_dirs, task_name="Create validation table")
-        p_map(save_test_table, run_dirs, task_name="Create test table")
+        p_map(save_validation_table, run_dirs)
+        p_map(save_test_table, run_dirs)
 
-        errors = p_map(check_tables, run_dirs, task_name="Validate tables")
+        errors = p_map(check_tables, run_dirs)
         assert not any(errors), "Something wrong with the validation and/or test tables"
 
         if args.notebook != "":
             settings.log.info("Creating notebooks...")
             p_map(
-                partial(create_experiment_notebooks, base_notebook=args.notebook),
-                run_dirs,
-                task_name="Generate notebooks",
-            )
+                partial(create_experiment_notebooks, base_notebook=args.notebook), run_dirs, num_cpus=12
+            )  # If you encounter errors, please run the execution sequentially (unfortunately, errors are not shown when run in parallel)
+            # for run in run_dirs:
+            #     create_experiment_notebooks(run, base_notebook=args.notebook)
     else:
         settings.log.info("All runs complete. Nothing to do")
```

## htc/evaluation/model_comparison/paper_runs.py

```diff
@@ -45,15 +45,15 @@
         model = run_dir.parent.name
 
         if prev_data_spec is None:
             prev_data_spec = config["input/data_spec"]
         else:
             assert prev_data_spec == config["input/data_spec"], (
                 f"All runs must use the same data specification file (The {run_dir} has the specification"
-                f" {config['input/data_spec']} instead of {prev_data_spec})"
+                f' {config["input/data_spec"]} instead of {prev_data_spec})'
             )
 
         main_loss = "train/dice_loss_epoch"
         if model == "pixel":
             main_loss = "train/ce_loss_epoch"
         elif model == "superpixel_classification":
             main_loss = "train/kl_loss_epoch"
```

## htc/model_processing/ImageConsumer.py

```diff
@@ -82,15 +82,15 @@
                             predictions_dir = self.target_dir / image_data["fold_name"] / "reconstructions"
                             predictions_data = image_data["reconstructions"]
                         elif "predictions" in image_data.keys():
                             predictions_dir = self.target_dir / "predictions"
                             predictions_data = image_data["predictions"]
 
                         predictions_dir.mkdir(parents=True, exist_ok=True)
-                        compress_file(predictions_dir / f'{image_data["path"].image_name()}.blosc', predictions_data)
+                        compress_file(predictions_dir / f'{image_data["image_name"]}.blosc', predictions_data)
 
                     self.handle_image_data(image_data)
 
                 self.task_queue.task_done()
         except Exception as e:
             # Unfortunately, there is no built-in mechanism for exception handling between producers and consumers
             # So, we handle all incoming tasks, do nothing with them and then at the end return the error
```

## htc/model_processing/Predictor.py

```diff
@@ -12,30 +12,26 @@
 from htc.utils.Config import Config
 
 
 class Predictor:
     def __init__(
         self,
         run_dir: Path,
-        use_predictions: bool = False,
-        store_predictions: bool = False,
+        use_predictions: bool,
+        store_predictions: bool,
         config: Config = None,
         mode: str = "predictions",
         **kwargs,
     ):
         self.run_dir = run_dir
         self.use_predictions = use_predictions
         self.store_predictions = store_predictions
         self.mode = mode
         self.config = Config(self.run_dir / "config.json") if config is None else config
         self.config["dataloader_kwargs/num_workers"] = 1  # One worker process is usually enough for inference tasks
-        self.name_path_mapping = {}
-
-        # We usually don't need labels for the prediction
-        self.config["input/no_labels"] = True
 
         if self.mode == "activations" or self.mode == "reconstructions":
             assert self.fold_name is not None, (
                 f"The fold name has to specified when calculating {self.mode}. "
                 "As, for predictions an ensemble of fold models is used but this is "
                 f"not possible for {self.mode}, so they are calculated per-fold."
             )
```

## htc/model_processing/Runner.py

```diff
@@ -2,27 +2,23 @@
 # SPDX-License-Identifier: MIT
 
 import argparse
 import copy
 import multiprocessing
 import queue
 import sys
-from functools import cached_property
 from pathlib import Path
-from typing import Union
 
 import psutil
 import torch
 
 from htc.model_processing.ImageConsumer import ImageConsumer
 from htc.model_processing.Predictor import Predictor
 from htc.models.common.HTCModel import HTCModel
-from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
-from htc.tivita.DataPath import DataPath
 
 
 class Runner:
     def __init__(self, description: str) -> None:
         r"""
         Helper class to start the producer and the consumers to operate on predictions of images.
 
@@ -102,54 +98,28 @@
         self.parser.add_argument(
             "--hide-progressbar",
             default=False,
             action="store_true",
             help="If set, no progress bar is shown for the predictor.",
         )
 
+        self._args = None
         self._used_args = []
 
-    @cached_property
+    @property
     def args(self) -> argparse.Namespace:
-        return self.parser.parse_args()
+        if self._args is None:
+            self._args = self.parser.parse_args()
+
+        return self._args
 
     @property
     def run_dir(self) -> Path:
         return HTCModel.find_pretrained_run(self.args.model, self.args.run_folder)
 
-    @cached_property
-    def paths(self) -> Union[list[DataPath], None]:
-        """
-        Returns: List of paths either collected based on the --input-dir or the --spec argument. None if no path could be found.
-        """
-        dargs = vars(self.args)
-
-        if dargs.get("input_dir") is not None:
-            assert (
-                dargs.get("spec") is None and dargs.get("spec_split") is None and dargs.get("spec_fold") is None
-            ), "--spec, --spec-split and --spec-fold can only be used if --input-dir is not used"
-
-            input_dir = dargs.get("input_dir")
-            assert input_dir.exists(), "Directory for which inference should be computed does not exist."
-
-            return list(DataPath.iterate(input_dir, annotation_name=dargs.get("annotation_name")))
-        elif dargs.get("spec") is not None:
-            assert dargs.get("input_dir") is None, "--input-dir cannot be used together with the --spec argument"
-            spec = DataSpecification(dargs.get("spec"))
-            if dargs.get("spec_split") is not None and "test" in dargs.get("spec_split"):
-                spec.activate_test_set()
-                settings.log.info("Activating the test set of the data specification file")
-
-            if dargs.get("spec_fold") is not None:
-                return spec.fold_paths(dargs.get("spec_fold"), dargs.get("spec_split"))
-            else:
-                return spec.paths(dargs.get("spec_split"))
-        else:
-            return None
-
     def add_argument(self, name: str, **kwargs) -> None:
         """
         Add a custom argument to the runner. If the name is known (e.g. --test), then some defaults will be applied to the argument. It is always possible to overwrite the defaults. The value of all arguments is automatically passed on to the producer and the consumers.
 
         Args:
             name: Name of the argument (e.g. --test).
             kwargs: All additional keyword arguments passed to parser.add_argument()
@@ -197,53 +167,14 @@
             kwargs.setdefault(
                 "help",
                 (
                     "The target domain for hyper_diva activations plotting. If this parameter is specified then the"
                     " target_domain from the config files is overridden."
                 ),
             )
-        elif name == "--spec":
-            kwargs.setdefault("type", str)
-            kwargs.setdefault("default", None)
-            kwargs.setdefault(
-                "help",
-                (
-                    "If the inference needs to be carried out on a specific data spec file. This argument results in"
-                    " all unique paths from that data spec file to be used for inference. This argument override the"
-                    " --input-dir argument."
-                ),
-            )
-        elif name == "--spec-fold":
-            kwargs.setdefault("type", str)
-            kwargs.setdefault("default", None)
-            kwargs.setdefault(
-                "help",
-                (
-                    "If the inference-spec argument has been set, then use this argument to specify a particular fold"
-                    " within the data spec file. All unique paths in that fold inside that data spec file will be used"
-                    " for inference. This argument can only be used if inference-spec argument has been set."
-                    " This argument overrides the --input-dir argument."
-                ),
-            )
-        elif name == "--spec-split":
-            kwargs.setdefault("type", str)
-            kwargs.setdefault("default", None)
-            kwargs.setdefault(
-                "help",
-                (
-                    "If the inference-spec has been set, then use this argument to specify a split name inside"
-                    " the data spec file. All unique paths in that split from that data spec files will"
-                    " be used for inference. This argument can only be used if inference-spec has been set."
-                    " This argument overrides the --input-dir argument."
-                ),
-            )
-        elif name == "--annotation-name":
-            kwargs.setdefault("type", str)
-            kwargs.setdefault("default", None)
-            kwargs.setdefault("help", "Filter the paths by this annotation name (default is no filtering).")
 
         self.parser.add_argument(name, **kwargs)
         self._used_args.append(name.removeprefix("--").replace("-", "_"))
 
     def start(self, PredictorClass: type[Predictor], ConsumerClass: type[ImageConsumer]) -> None:
         """
         Start the producer and the consumers. If you need to pass additional parameters (not CLI arguments, they are automatically available in the producer and the consumers) to your consumers or producer, please use the functools.partial method.
@@ -290,18 +221,18 @@
             )
 
             exit_code = 0
             try:
                 with torch.autocast(device_type="cuda"):
                     predictor.start(task_queue, self.args.hide_progressbar)
 
-                # Wait until all consumers are finished with this run
-                task_queue.join()
-                # Let one consumer do the final work (the run_dir serves as a dummy to indicate that all predictions are done)
-                task_queue.put(self.run_dir)
+                task_queue.join()  # Wait until all consumers are finished with this run
+                task_queue.put(
+                    self.run_dir
+                )  # Let one consumer do the final work (the run_dir serves as a dummy to indicate that all predictions are done)
             except Exception:
                 settings.log.exception("Error occurred in the producer")
                 exit_code = 1
             finally:
                 # Add a poison pill for each consumer to shut down
                 for _ in range(num_consumers):
                     task_queue.put(None)
```

## htc/model_processing/TestLeaveOneOutPredictor.py

```diff
@@ -21,15 +21,16 @@
         self,
         *args,
         paths: Union[list[DataPath], dict[str, list[DataPath]]] = None,
         fold_names: list[str] = None,
         outputs: list[str] = None,
         **kwargs,
     ):
-        """Compared to the TestPredictor, this class expects a leave-one-out structure of the test set and hence does not use ensembling."""
+        """Compared to the TestPredictor, this class expects a leave-one-out structure of the test set and hence does not use ensembling.
+        """
         super().__init__(*args, **kwargs)
         if outputs is None:
             outputs = ["predictions"]
         self.outputs = outputs
         assert len(self.outputs), "At least one output should be provided"
         self.feature_names = [name for name in self.outputs if name != "predictions"]
 
@@ -69,15 +70,14 @@
             model = LightningClass.load_from_checkpoint(
                 ckpt_file, dataset_train=None, datasets_val=[dataset], config=self.config
             )
             model.eval()
             model.cuda()
 
             self.models[fold_dir] = model
-            self.name_path_mapping |= {p.image_name(): p for p in fold_paths}
 
     @torch.no_grad()
     def start(self, task_queue: multiprocessing.JoinableQueue, hide_progressbar: bool) -> None:
         with Progress(*Progress.get_default_columns(), TimeElapsedColumn(), disable=hide_progressbar) as progress:
             task_models = progress.add_task("Models", total=len(self.models))
 
             for fold_dir, model in self.models.items():
@@ -87,15 +87,15 @@
                 for batch in dataloader:
                     remaining_image_names = []
                     for b, image_name in enumerate(batch["image_name"]):
                         predictions = self.load_predictions(image_name)
                         if predictions is not None:
                             task_queue.put(
                                 {
-                                    "path": self.name_path_mapping[image_name],
+                                    "image_name": image_name,
                                     "fold_name": fold_dir.name,
                                     "predictions": predictions,
                                 }
                             )
                         else:
                             remaining_image_names.append(image_name)
 
@@ -120,15 +120,15 @@
                         for name in self.feature_names:
                             batch_outputs[name] = features[name].cpu().numpy()
 
                         for b in range(batch["features"].size(0)):
                             image_name = batch["image_name"][b]
                             if image_name in remaining_image_names:
                                 data = {
-                                    "path": self.name_path_mapping[image_name],
+                                    "image_name": image_name,
                                     "fold_name": fold_dir.name,
                                 }
                                 for name in self.outputs:
                                     data[name] = batch_outputs[name][b, ...]
 
                                 task_queue.put(data)
```

## htc/model_processing/TestPredictor.py

```diff
@@ -1,147 +1,96 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import multiprocessing
-from pathlib import Path
 
 import torch
-import torch.nn as nn
 from rich.progress import Progress, TimeElapsedColumn
-from torch.utils.data import DataLoader
-from typing_extensions import Self
 
 from htc.model_processing.Predictor import Predictor
 from htc.models.common.HTCLightning import HTCLightning
 from htc.models.common.torch_helpers import move_batch_gpu
 from htc.models.data.DataSpecification import DataSpecification
 from htc.tivita.DataPath import DataPath
-from htc.utils.Config import Config
 from htc.utils.helper_functions import checkpoint_path
 
 
-class TestEnsemble(nn.Module):
-    def __init__(self, model_paths: list[Path], paths: list[DataPath], config: Config):
-        super().__init__()
-        self.config = config
-
-        self.models: dict[Path, HTCLightning] = {}
-        for fold_dir in model_paths:
-            ckpt_file, _ = checkpoint_path(fold_dir)
-
-            # Load dataset and lightning class based on model name
-            LightningClass = HTCLightning.class_from_config(self.config)
-            dataset = LightningClass.dataset(paths=paths, train=False, config=self.config, fold_name=fold_dir.stem)
-            model = LightningClass.load_from_checkpoint(
-                ckpt_file, dataset_train=None, datasets_val=[dataset], config=self.config
-            )
-
-            self.models[fold_dir] = model
-
-    def eval(self) -> Self:
-        for model in self.models.values():
-            model.eval()
-        return self
-
-    def cuda(self, *args, **kwargs) -> Self:
-        for model in self.models.values():
-            model.cuda(*args, **kwargs)
-        return self
-
-    def to(self, *args, **kwargs) -> Self:
-        for model in self.models.values():
-            model.to(*args, **kwargs)
-        return self
-
-    def dataloader(self) -> DataLoader:
-        # All models have the same dataset, so we just take the first dataloader and use the data for all models
-        return next(iter(self.models.values())).val_dataloader()[0]
-
-    def predict_step(self, batch: dict[str, torch.Tensor], batch_idx: int = None) -> dict[str, torch.Tensor]:
-        fold_predictions = []
-        for model in self.models.values():
-            out = model.predict_step(batch)["class"]
-            if self.config["model/activations"] != "sigmoid":
-                out = out.softmax(dim=1)
-            fold_predictions.append(out)
-
-        # Ensembling over the softmax values (or logits in case of sigmoid)
-        return {"class": torch.stack(fold_predictions).mean(dim=0)}
-
-
 class TestPredictor(Predictor):
     def __init__(self, *args, paths: list[DataPath] = None, fold_names: list[str] = None, **kwargs):
         super().__init__(*args, **kwargs)
 
         if fold_names is None:
-            model_paths = sorted(self.run_dir.glob("fold*"))  # All folds per default
+            model_folds = sorted(self.run_dir.glob("fold*"))  # All folds per default
         else:
-            model_paths = [self.run_dir / f for f in fold_names]
-        assert len(model_paths) > 0, "At least one fold required"
+            model_folds = [self.run_dir / f for f in fold_names]
+        assert len(model_folds) > 0, "At least one fold required"
 
-        # We do not need pretrained model during testing
+        # Do not need pretrained model during testing
         self.config["model/pretrained_model"] = None
 
         if paths is None:
             specs = DataSpecification(self.run_dir / "data.json")
             specs.activate_test_set()
             paths = specs.paths("^test")
 
-        self.name_path_mapping = {p.image_name(): p for p in paths}
-        self.model = TestEnsemble(model_paths, paths, self.config)
-        self.model.eval()
-        self.model.cuda()
+        # Load models from all folds (we'll do ensembling later)
+        self.models = {}
+        for fold_dir in model_folds:
+            ckpt_file, _ = checkpoint_path(fold_dir)
+
+            # Load dataset and lightning class based on model name
+            LightningClass = HTCLightning.class_from_config(self.config)
+            dataset = LightningClass.dataset(paths=paths, train=False, config=self.config, fold_name=fold_dir.stem)
+            model = LightningClass.load_from_checkpoint(
+                ckpt_file, dataset_train=None, datasets_val=[dataset], config=self.config
+            )
+            model.eval()
+            model.cuda()
+
+            self.models[fold_dir] = model
 
     @torch.no_grad()
     def start(self, task_queue: multiprocessing.JoinableQueue, hide_progressbar: bool) -> None:
-        dataloader = self.model.dataloader()
-
+        dataloader = next(iter(self.models.values())).val_dataloader()[
+            0
+        ]  # All models have the same dataset, so we just take the first dataloader and use the data for all models
         with Progress(*Progress.get_default_columns(), TimeElapsedColumn(), disable=hide_progressbar) as progress:
             task_loader = progress.add_task("Dataloader", total=len(dataloader))
 
             for batch in dataloader:
                 remaining_image_names = []
                 for b, image_name in enumerate(batch["image_name"]):
                     predictions = self.load_predictions(image_name)
                     if predictions is not None:
                         task_queue.put(
                             {
-                                "path": self.name_path_mapping[image_name],
+                                "image_name": image_name,
                                 "predictions": predictions,
                             }
                         )
                     else:
                         remaining_image_names.append(image_name)
 
                 if len(remaining_image_names) > 0:
                     if not batch["features"].is_cuda:
                         batch = move_batch_gpu(batch)
 
-                    self.produce_predictions(
-                        task_queue=task_queue,
-                        model=self.model,
-                        batch=batch,
-                        remaining_image_names=remaining_image_names,
-                    )
+                    fold_predictions = []
+                    for model in self.models.values():
+                        fold_predictions.append(model.predict_step(batch)["class"].softmax(dim=1))
+
+                    batch_predictions = (
+                        torch.stack(fold_predictions).mean(dim=0).cpu().numpy()
+                    )  # Ensembling over the softmax values
+                    for b in range(batch_predictions.shape[0]):
+                        image_name = batch["image_name"][b]
+                        if image_name in remaining_image_names:
+                            predictions = batch_predictions[b, ...]
+
+                            task_queue.put(
+                                {
+                                    "image_name": image_name,
+                                    "predictions": predictions,
+                                }
+                            )
 
                 progress.advance(task_loader)
-
-    def produce_predictions(
-        self,
-        task_queue: multiprocessing.JoinableQueue,
-        model: HTCLightning,
-        batch: dict[str, torch.Tensor],
-        remaining_image_names: list[str],
-    ) -> None:
-        batch_predictions = model.predict_step(batch)["class"].cpu().numpy()
-
-        for b in range(batch_predictions.shape[0]):
-            image_name = batch["image_name"][b]
-            if image_name in remaining_image_names:
-                predictions = batch_predictions[b, ...]
-
-                task_queue.put(
-                    {
-                        "path": self.name_path_mapping[image_name],
-                        "predictions": predictions,
-                    }
-                )
```

## htc/model_processing/ValidationPredictor.py

```diff
@@ -1,11 +1,10 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import gc
 import multiprocessing
 
 import torch
 from rich.progress import Progress, TimeElapsedColumn
 
 from htc.model_processing.Predictor import Predictor
 from htc.models.common.HTCLightning import HTCLightning
@@ -24,15 +23,15 @@
         LightningClass = HTCLightning.class_from_config(self.config)
 
         with Progress(*Progress.get_default_columns(), TimeElapsedColumn(), disable=hide_progressbar) as progress:
             fold_dirs = sorted(self.run_dir.glob("fold*"))
             task_folds = progress.add_task("Folds", total=len(fold_dirs))
 
             for fold_dir in fold_dirs:
-                ckpt_file, best_epoch_index = checkpoint_path(fold_dir)
+                ckpt_file, _ = checkpoint_path(fold_dir)
 
                 # All paths for the respective fold
                 fold_data = specs.folds[fold_dir.name]
                 split_name = [name for name in fold_data.keys() if name.startswith("val")][0]  # Only the first dataset
                 assert not split_name.endswith("_known"), "Predictions should not be done on the known dataset"
 
                 if (
@@ -44,85 +43,55 @@
                     assert not self.use_predictions and not self.store_predictions, (
                         "Found duplicate image_names across folds. Neither the --use-predictions nor the"
                         " --store-predictions switch is allowed to be set. Otherwise, subsequent runs of the same"
                         " script would only ever use the predictions of the first fold"
                     )
 
                 paths = fold_data[split_name]
-                self.name_path_mapping = {p.image_name(): p for p in paths}
                 dataset = LightningClass.dataset(paths=paths, train=False, config=self.config, fold_name=fold_dir.stem)
 
                 # Load dataset and lightning class based on model name
                 model = LightningClass.load_from_checkpoint(
                     ckpt_file, dataset_train=None, datasets_val=[dataset], config=self.config
                 )
                 model.eval()
                 model.cuda()
                 dataloader = model.val_dataloader()[0]
                 task_loader = progress.add_task(f"Dataloader (fold={fold_dir.name})", total=len(dataloader))
 
                 for batch in dataloader:
                     remaining_image_names = []
-                    for image_name in batch["image_name"]:
+                    for b, image_name in enumerate(batch["image_name"]):
                         predictions = self.load_predictions(image_name)
                         if predictions is not None:
                             task_queue.put(
                                 {
-                                    "path": self.name_path_mapping[image_name],
+                                    "image_name": image_name,
                                     "fold_name": fold_dir.name,
-                                    "best_epoch_index": best_epoch_index,
                                     "predictions": predictions,
                                 }
                             )
                         else:
                             remaining_image_names.append(image_name)
 
                     # It is quite unusual that in one sample some images are already predicted and some not, so we just predict them all and yield only new samples
                     if len(remaining_image_names) > 0:
-                        if not batch["features"].is_cuda:
+                        if not batch["labels"].is_cuda:
                             batch = move_batch_gpu(batch)
 
-                        self.produce_predictions(
-                            task_queue=task_queue,
-                            model=model,
-                            batch=batch,
-                            remaining_image_names=remaining_image_names,
-                            fold_name=fold_dir.name,
-                            best_epoch_index=best_epoch_index,
-                        )
+                        batch_predictions = model.predict_step(batch)["class"].softmax(dim=1).cpu().numpy()
 
-                    progress.advance(task_loader)
-
-                # Make sure the shared memory buffer is cleared before the next iteration
-                del dataloader
-                gc.collect()
+                        for b in range(batch_predictions.shape[0]):
+                            image_name = batch["image_name"][b]
+                            if image_name in remaining_image_names:
+                                predictions = batch_predictions[b, ...]
+
+                                task_queue.put(
+                                    {
+                                        "image_name": image_name,
+                                        "fold_name": fold_dir.name,
+                                        "predictions": predictions,
+                                    }
+                                )
 
+                    progress.advance(task_loader)
                 progress.advance(task_folds)
-
-    def produce_predictions(
-        self,
-        task_queue: multiprocessing.JoinableQueue,
-        model: HTCLightning,
-        batch: dict[str, torch.Tensor],
-        remaining_image_names: list[str],
-        fold_name: str,
-        best_epoch_index: int,
-    ) -> None:
-        batch_predictions = model.predict_step(batch)["class"]
-        if self.config["model/activations"] != "sigmoid":
-            # For the sigmoid activation, we need to pass the logits
-            batch_predictions = batch_predictions.softmax(dim=1)
-        batch_predictions = batch_predictions.cpu().numpy()
-
-        for b in range(batch_predictions.shape[0]):
-            image_name = batch["image_name"][b]
-            if image_name in remaining_image_names:
-                predictions = batch_predictions[b, ...]
-
-                task_queue.put(
-                    {
-                        "path": self.name_path_mapping[image_name],
-                        "fold_name": fold_name,
-                        "best_epoch_index": best_epoch_index,
-                        "predictions": predictions,
-                    }
-                )
```

## htc/model_processing/run_image_figures.py

```diff
@@ -1,72 +1,64 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import numpy as np
 import pandas as pd
-import torch
 
 from htc.model_processing.ImageConsumer import ImageConsumer
 from htc.model_processing.Runner import Runner
 from htc.model_processing.TestLeaveOneOutPredictor import TestLeaveOneOutPredictor
 from htc.model_processing.TestPredictor import TestPredictor
 from htc.model_processing.ValidationPredictor import ValidationPredictor
-from htc.models.common.utils import multi_label_condensation
+from htc.tivita.DataPath import DataPath
 from htc.utils.LabelMapping import LabelMapping
 from htc.utils.visualization import (
     compress_html,
     create_confusion_figure,
     create_image_scores_figure,
     prediction_figure_html,
 )
 
 
 class ImageFigureConsumer(ImageConsumer):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
-        if self.target_dir == self.run_dir:
-            self.target_dir = self.target_dir / "prediction_figures"
-        else:
-            self.target_dir = self.target_dir / self.run_dir.parent.name / self.run_dir.name
+        self.target_dir = self.run_dir / "prediction_figures"
         self.target_dir.mkdir(parents=True, exist_ok=True)
 
     def handle_image_data(self, image_data: dict) -> None:
         predictions = image_data["predictions"]
-        if self.config["model/activations"] == "sigmoid":
-            res = multi_label_condensation(torch.from_numpy(predictions).float().unsqueeze(dim=0), self.config)
-            confidences = res["confidences"].squeeze(dim=0).numpy()
-            predictions = res["predictions"].squeeze(dim=0).numpy()
-        else:
-            confidences = np.max(predictions, axis=0)
-            predictions = np.argmax(predictions, axis=0)
+        confidence = np.max(predictions, axis=0)
+        predictions = np.argmax(predictions, axis=0)
 
-        path = image_data["path"]
+        image_name = image_data["image_name"]
+        path = DataPath.from_image_name(image_name)
 
         # Find the appropriate table where the evaluation results are stored for this image
         df_val = pd.read_pickle(self.run_dir / "validation_table.pkl.xz").query("dataset_index == 0")
         if self.test:
             df = pd.read_pickle(self.run_dir / "test_table.pkl.xz")
-            df = df.query("image_name == @path.image_name()")
+            df = df.query("image_name == @image_name")
             assert len(df) == 1, "There is more than one result for the image"
             df = df.iloc[0]
         else:
-            df = df_val.query(f'epoch_index == best_epoch_index and image_name == "{path.image_name()}"')
+            df = df_val.query(f'epoch_index == best_epoch_index and image_name == "{image_name}"')
             assert len(df) == 1, "There is more than one result for the image"
             df = df.iloc[0]
 
         # Some metric data to display for the image
         dice = df["dice_metric_image"].item()
         title = f": dice={dice:0.2f}"
         if "surface_distance_metric_image" in df:
             surface = df["surface_distance_metric_image"].item()
             title += f", surface={surface:0.2f}"
 
         # Create figures
-        html_prediction = prediction_figure_html(predictions, confidences, path, self.config, title_suffix=title)
+        html_prediction = prediction_figure_html(predictions, confidence, path, self.config, title_suffix=title)
 
         mapping = LabelMapping.from_config(self.config)
         label_names = [mapping.index_to_name(i) for i in df["used_labels"]]
         fig_image_scores = create_image_scores_figure(label_names, df["dice_metric"])
 
         if "confusion_matrix" in df:
             mapping = LabelMapping.from_config(self.config)
@@ -76,33 +68,32 @@
 
         # Combine all figures in one html file
         html = f"""
 <!DOCTYPE html>
 <html lang="en">
     <head>
         <meta charset="utf-8">
-        <title>Results for image {path.image_name()}</title>
+        <title>Results for image {image_data["image_name"]}</title>
     </head>
     <body>
         {html_prediction}
         {fig_image_scores.to_html(full_html=False, include_plotlyjs='cdn', div_id='dice_scores')}
         {fig_confusion.to_html(full_html=False, include_plotlyjs='cdn', div_id='confusion_matrix') if fig_confusion is not None else ''}
     </body>
 </html>"""
 
-        target_file = self.target_dir / f"{path.image_name()}_dice={dice:.02f}.html"
+        target_file = self.target_dir / f'{image_data["image_name"]}_dice={dice:.02f}.html'
         compress_html(target_file, html)
 
 
 if __name__ == "__main__":
     runner = Runner(
         description="Create prediction figures (interactive HTML files) for every image in the validation or test set."
     )
     runner.add_argument("--test")
     runner.add_argument("--test-looc")
-    runner.add_argument("--output-dir")
 
     if runner.args.test:
         TestClass = TestLeaveOneOutPredictor if runner.args.test_looc else TestPredictor
         runner.start(TestClass, ImageFigureConsumer)
     else:
         runner.start(ValidationPredictor, ImageFigureConsumer)
```

## htc/model_processing/run_inference.py

```diff
@@ -5,79 +5,81 @@
 
 import numpy as np
 
 from htc.model_processing.ImageConsumer import ImageConsumer
 from htc.model_processing.Runner import Runner
 from htc.model_processing.TestPredictor import TestPredictor
 from htc.settings import settings
+from htc.tivita.DataPath import DataPath
 from htc.utils.blosc_compression import compress_file
 from htc.utils.Config import Config
 from htc.utils.visualization import compress_html, prediction_figure_html
 
 
 class InferenceConsumer(ImageConsumer):
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args, paths, **kwargs):
         super().__init__(*args, **kwargs)
 
-        if self.target_dir == self.run_dir:
-            self.target_dir = self.target_dir / "predictions"
+        if hasattr(self, "output_dir") and self.output_dir is not None:
+            self.target_folder = self.output_dir / self.run_dir.name
         else:
-            self.target_dir = self.target_dir / self.run_dir.parent.name / self.run_dir.name
-        self.target_dir.mkdir(parents=True, exist_ok=True)
+            self.target_folder = settings.results_dir / "predictions" / self.run_dir.name
+        self.target_folder.mkdir(parents=True, exist_ok=True)
 
-        self.config.save_config(self.target_dir / "config.json")
+        self.config.save_config(self.target_folder / "config.json")
 
-    def handle_image_data(self, image_data: dict) -> None:
-        path = image_data["path"]
+        # We need a custom mapping from image_name to paths since this script should work with arbitrary images even without unique image names
+        self.path_mapping = {p.image_name(): p for p in paths}
 
+    def handle_image_data(self, image_data: dict) -> None:
         confidence = np.max(image_data["predictions"], axis=0)
         predictions = np.argmax(image_data["predictions"], axis=0)
         predictions_save = predictions if self.predictions_type == "labels" else image_data["predictions"]
-        compress_file(self.target_dir / f"{path.image_name()}.blosc", predictions_save)
+        compress_file(self.target_folder / f'{image_data["image_name"]}.blosc', predictions_save)
 
+        path = self.path_mapping[image_data["image_name"]]
         html_prediction = prediction_figure_html(predictions, confidence, path, self.config)
         html = f"""
 <!DOCTYPE html>
 <html lang="en">
     <head>
         <meta charset="utf-8">
-        <title>Results for image {path.image_name()}</title>
+        <title>Results for image {image_data["image_name"]}</title>
     </head>
     <body>
         {html_prediction}
     </body>
 </html>"""
-        compress_html(self.target_dir / f"{path.image_name()}.html", html)
+        compress_html(self.target_folder / f'{image_data["image_name"]}.html', html)
 
 
 if __name__ == "__main__":
     runner = Runner(
         description=(
             "General inference based on the TestPredictor. For each image, the softmax predictions and an accompanying"
             " HTML file which visualizes the result will be stored either in your results directory or in the specified"
             " output directory."
         )
     )
-    runner.add_argument("--input-dir")
+    runner.add_argument("--input-dir", required=True)
     runner.add_argument("--output-dir")
-    runner.add_argument("--annotation-name")
-    runner.add_argument("--spec")
-    runner.add_argument("--spec-fold")
-    runner.add_argument("--spec-split")
     runner.add_argument("--predictions-type", type=str, choices=["softmax", "labels"], default="labels")
 
-    paths = runner.paths
-    assert paths is not None, "Either --input-dir or --spec must be provided"
-    settings.log.info(f"Compute the prediction for {len(paths)} images")
+    input_dir = runner.args.input_dir
+    assert input_dir.exists(), "Directory for which inference should be computed does not exist."
+    paths = list(DataPath.iterate(input_dir))
 
     config = Config(runner.run_dir / "config.json")
 
     # This is a general script which should work for arbitrary images so we might not have access to the intermediate files
     # Hence, we compute the L1 normalization on the fly
     if config["input/preprocessing"] == "L1":
         config["input/preprocessing"] = None
         config["input/normalization"] = "L1"
 
+    # We don't need labels for the inference
+    config["input/no_labels"] = True
+
     runner.start(
         partial(TestPredictor, paths=paths, config=config),
-        partial(InferenceConsumer),
+        partial(InferenceConsumer, paths=paths),
     )
```

## htc/model_processing/run_tables.py

```diff
@@ -1,172 +1,58 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import copy
-from pathlib import Path
-from typing import Union
 
 import numpy as np
 import pandas as pd
 import torch
 
 from htc.evaluation.evaluate_images import evaluate_images
 from htc.model_processing.ImageConsumer import ImageConsumer
 from htc.model_processing.Runner import Runner
 from htc.model_processing.TestLeaveOneOutPredictor import TestLeaveOneOutPredictor
 from htc.model_processing.TestPredictor import TestPredictor
 from htc.model_processing.ValidationPredictor import ValidationPredictor
-from htc.models.common.EvaluationMixin import EvaluationMixin
-from htc.models.common.HTCLightning import HTCLightning
 from htc.models.image.DatasetImage import DatasetImage
 from htc.settings import settings
 from htc.settings_seg import settings_seg
+from htc.tivita.DataPath import DataPath
 from htc.utils.general import apply_recursive
 from htc.utils.helper_functions import get_nsd_thresholds
 from htc.utils.LabelMapping import LabelMapping
 
 
-def _save_validation_table(
-    df_results: pd.DataFrame, target_dir: Path, metrics: list[str], tolerance_name: Union[str, None], run_dir: Path
-) -> None:
-    # Adding the new results to the validation table is a bit complicated since we only have results for the best epoch and the first dataset (but still want to keep the per-epoch results)
-    assert "image_name" in df_results and "fold_name" in df_results
-
-    # Add the distance aggregation of the nsd to the key
-    if "NSD" in metrics:
-        key_nsd = f"surface_dice_metric_{tolerance_name}"
-        key_nsd_image = f"surface_dice_metric_image_{tolerance_name}"
-        df_results.rename(
-            columns={"surface_dice_metric": key_nsd, "surface_dice_metric_image": key_nsd_image}, inplace=True
-        )
-
-    df_results.sort_values(by=["fold_name", "image_name"], inplace=True, ignore_index=True)
-
-    # We load and extend the validation table. This is a bit tricky since we only want to extend the best epoch per fold and only for the first dataset
-    df_val = pd.read_pickle(run_dir / "validation_table.pkl.xz")
-    df_val_best = df_val.query("epoch_index == best_epoch_index and dataset_index == 0").copy()
-    df_val_best.sort_values(by=["fold_name", "image_name"], inplace=True, ignore_index=True)
-    assert (
-        df_val_best.index.identical(df_results.index)
-        and np.all(df_val_best["image_name"].values == df_results["image_name"].values)
-        and np.all(df_val_best["fold_name"].values == df_results["fold_name"].values)
-    ), "Results and validation table are not aligned"
-    if "used_labels" in df_results:
-        assert np.all(
-            np.concatenate(np.equal(df_results["used_labels"].values, df_val_best["used_labels"].values, dtype=object))
-        ), "Used labels do not match between tables"
-
-    if "dice_metric_image" in df_results:
-        # The dice is always calculated during training, here we just check if we get a similar result as during training
-        dice_old = df_val_best["dice_metric_image"].values
-        dice_new = df_results["dice_metric_image"].values
-        dice_abs_diff = np.abs(dice_new - dice_old)
-        threshold = 0.01
-        if not np.all(dice_abs_diff <= threshold):
-            settings.log.warning(
-                "Differences between old (calculated during training) and new (calculated via predictions) dice"
-                f" scores are larger than {threshold} for {np.sum(dice_abs_diff > threshold)} out of"
-                f" {len(dice_abs_diff)} images (mean abs diff: {np.mean(dice_abs_diff)})"
-            )
-
-    # We need to copy all id columns to the results table so that the merge works
-    id_columns = ["epoch_index", "best_epoch_index", "dataset_index"]
-    for copy_column in id_columns:
-        df_results[copy_column] = df_val_best[copy_column].values
-
-    # These columns are already available in the results table but we still need them for the join
-    id_columns += ["fold_name", "image_name"]
-
-    # Merge results into existing validation table, will result in duplicate columns
-    df_val_new = df_val.merge(df_results, how="left", on=id_columns, validate="one_to_one", suffixes=("_old", "_new"))
-
-    # Overwrite _old columns with _new ones but only for the best epoch and the first dataset (for the remaining rows we keep the old data)
-    for c in df_results.columns:
-        if c not in id_columns and f"{c}_new" in df_val_new and f"{c}_old" in df_val_new:
-            df_val_new[c] = df_val_new[f"{c}_new"].fillna(df_val_new[f"{c}_old"])
-
-    df_val_new.drop(columns=df_val_new.filter(regex="_(?:old|new)$").columns.tolist(), inplace=True)
-    assert not any(c.endswith(("_old", "_new")) for c in df_val_new.columns), "Merge columns are still present"
-    assert all(c in df_val_new for c in df_results.columns), "Some columns are missing"
-
-    # Some checks that the merge actually worked
-    df_changed = df_val_new.query("epoch_index == best_epoch_index and dataset_index == 0")
-    df_unchanged = df_val_new.query("not (epoch_index == best_epoch_index and dataset_index == 0)")
-    assert len(df_changed) + len(df_unchanged) == len(df_val_new), "Tables do not add up"
-
-    assert (
-        pd.isna(df_unchanged[[c for c in df_results.columns if c not in df_val]]).all().all()
-    ), "Old values for non-existing columns must be nan"
-    assert not pd.isna(df_changed[df_results.columns]).all().all(), "All new values must be non-nan"
-    assert len(df_val) == len(df_val_new), "The validation table must not change in length"
-
-    # Finally, overwrite existing validation table
-    df_val_new.reset_index(inplace=True, drop=True)
-    df_val_new.to_pickle(target_dir / "validation_table.pkl.xz")
-
-
-def _save_test_table(
-    df_results: pd.DataFrame,
-    target_dir: Path,
-    metrics: list[str],
-    tolerance_name: Union[str, None],
-    test_table_name: str,
-) -> None:
-    # For the test table it is easier, as we create it from scratch (and overwrite an existing one, if available)
-    assert len(df_results) == len(df_results["image_name"].unique()), "There must be exactly one row per image"
-
-    # Add the distance aggregation of the nsd to the key
-
-    if "NSD" in metrics:
-        key_nsd = f"surface_dice_metric_{tolerance_name}"
-        key_nsd_image = f"surface_dice_metric_image_{tolerance_name}"
-        df_results.rename(
-            columns={"surface_dice_metric": key_nsd, "surface_dice_metric_image": key_nsd_image}, inplace=True
-        )
-
-    df_results.to_pickle(target_dir / f"{test_table_name}.pkl.xz")
-
-
 class ImageTableConsumer(ImageConsumer):
-    def __init__(self, *args, test_table_name: str = "test_table", **kwargs):
+    def __init__(self, *args, **kwargs):
         """Adds all post-hoc metrics (e.g. ASD, NSD) to the validation or test table."""
         super().__init__(*args, **kwargs)
 
-        if "NSD" in self.metrics:
-            label_mapping = LabelMapping.from_config(self.config)
-            if self.NSD_threshold is None:
-                # Default MIA2021 thresholds
-                self.tolerances = get_nsd_thresholds(label_mapping)
-                self.tolerance_name = settings_seg.nsd_aggregation.split("_")[-1]
-            else:
-                # The same threshold for all classes
-                self.tolerances = [self.NSD_threshold] * len(label_mapping)
-                self.tolerance_name = str(self.NSD_threshold)
+        # Get tolerance values for the NSD with the label mapping of the run
 
-            settings.log.info(f"Using the following NSD thresholds: {self.tolerances}")
+        if "NSD" in self.metrics:
+            self.tolerances = get_nsd_thresholds(LabelMapping.from_config(self.config))
         else:
             self.tolerances = None
-            self.tolerance_name = None
-
-        self.test_table_name = test_table_name
 
     def handle_image_data(self, image_data: dict) -> None:
         config = copy.copy(self.config)
         config["input/preprocessing"] = None
         config["input/no_features"] = True  # As we only need labels from the sample
 
-        path = image_data["path"]
+        path = DataPath.from_image_name(image_data["image_name"])
         sample = DatasetImage([path], train=False, config=config)[0]
 
         predictions = torch.from_numpy(image_data["predictions"]).unsqueeze(dim=0)
+        predictions_labels = predictions.argmax(dim=1)
         labels = sample["labels"].unsqueeze(dim=0)
         valid_pixels = sample["valid_pixels"].unsqueeze(dim=0)
 
         if valid_pixels.sum() == 0:
-            settings.log.info(f"The image {path.image_name()} is skipped because it contains no valid pixels")
+            settings.log.info(f'The image {image_data["image_name"]} is skipped because it contains no valid pixels')
             return
 
         metric_data = {}
 
         # this script can also be used without setting the metrics parameter i.e. in case the metrics don't have to
         # computed, but only the predictions are to be stored
         if len(self.metrics) > 0:
@@ -175,122 +61,137 @@
                 labels,
                 valid_pixels,
                 n_classes=predictions.shape[1],
                 tolerances=self.tolerances,
                 metrics=self.metrics + ["ECE", "CM"],
             )[0]
 
-        metric_data["image_name"] = path.image_name()
+        metric_data["image_name"] = image_data["image_name"]
         metric_data |= path.image_name_typed()
 
         if "fold_name" in image_data:
             metric_data["fold_name"] = image_data["fold_name"]
 
         apply_recursive(lambda x: x.cpu().numpy() if type(x) == torch.Tensor else x, metric_data)
 
         self.results_list.append(metric_data)
 
     def run_finished(self) -> None:
+        if self.test:
+            self._save_test_table()
+        else:
+            self._save_validation_table()
+
+    def _save_validation_table(self) -> None:
+        # Adding the new results to the validation table is a bit complicated since we only have results for the best epoch and the first dataset (but still want to keep the per-epoch results)
         df_results = pd.DataFrame(list(self.results_list))
         assert len(df_results) > 0, "No results calculated"
+        assert "image_name" in df_results and "fold_name" in df_results
 
-        if self.test:
-            _save_test_table(df_results, self.target_dir, self.metrics, self.tolerance_name, self.test_table_name)
-        else:
-            _save_validation_table(df_results, self.target_dir, self.metrics, self.tolerance_name, self.run_dir)
+        # Add the distance aggregation of the nsd to the key
+        if "NSD" in self.metrics:
+            tolerance_name = settings_seg.nsd_aggregation.split("_")[-1]
+            key_nsd = f"surface_dice_metric_{tolerance_name}"
+            key_nsd_image = f"surface_dice_metric_image_{tolerance_name}"
+            df_results.rename(
+                columns={"surface_dice_metric": key_nsd, "surface_dice_metric_image": key_nsd_image}, inplace=True
+            )
 
+        df_results.sort_values(by=["fold_name", "image_name"], inplace=True, ignore_index=True)
 
-class TableValidationPredictor(EvaluationMixin, ValidationPredictor):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.rows = []
-        self.config["input/no_labels"] = False
-        self.evaluation_kwargs = {"metrics": self.metrics}
-
-    def produce_predictions(
-        self, model: HTCLightning, batch: dict[str, torch.Tensor], fold_name: str, best_epoch_index: int, **kwargs
-    ) -> None:
-        self.model = model
-
-        rows = self._validate_batch(batch, dataloader_idx=0)
-        rows = apply_recursive(lambda x: x.cpu().numpy() if type(x) == torch.Tensor else x, rows)
-        for r in rows:
-            r["fold_name"] = fold_name
-            r["epoch_index"] = best_epoch_index  # The validation predictor only uses the best epoch
-            r["best_epoch_index"] = best_epoch_index
-        self.rows += rows
-
-    def predict_step(self, batch: dict[str, torch.Tensor], batch_idx: int = None) -> dict[str, torch.Tensor]:
-        return self.model.predict_step(batch)
-
-    def save_table(self, output_dir: Path) -> None:
-        df = pd.DataFrame(self.rows)
-        _save_validation_table(df, output_dir, self.metrics, self.run_dir)
+        # We load and extend the validation table. This is a bit tricky since we only want to extend the best epoch per fold and only for the first dataset
+        df_val = pd.read_pickle(self.run_dir / "validation_table.pkl.xz")
+        df_val_best = df_val.query("epoch_index == best_epoch_index and dataset_index == 0").copy()
+        df_val_best.sort_values(by=["fold_name", "image_name"], inplace=True, ignore_index=True)
+        assert (
+            df_val_best.index.identical(df_results.index)
+            and np.all(df_val_best["image_name"].values == df_results["image_name"].values)
+            and np.all(df_val_best["fold_name"].values == df_results["fold_name"].values)
+        ), "Results and validation table are not aligned"
+        if "used_labels" in df_results:
+            assert np.all(
+                np.concatenate(
+                    np.equal(df_results["used_labels"].values, df_val_best["used_labels"].values, dtype=object)
+                )
+            ), "Used labels do not match between tables"
+
+        if "dice_metric_image" in df_results:
+            # The dice is always calculated during training, here we just check if we get a similar result as during training
+            dice_old = df_val_best["dice_metric_image"].values
+            dice_new = df_results["dice_metric_image"].values
+            dice_abs_diff = np.abs(dice_new - dice_old)
+            threshold = 0.01
+            if not np.all(dice_abs_diff <= threshold):
+                settings.log.warning(
+                    "Differences between old (calculated during training) and new (calculated via predictions) dice"
+                    f" scores are larger than {threshold} for {np.sum(dice_abs_diff > threshold)} out of"
+                    f" {len(dice_abs_diff)} images (mean abs diff: {np.mean(dice_abs_diff)})"
+                )
+
+        # We need to copy all id columns to the results table so that the merge works
+        id_columns = ["epoch_index", "best_epoch_index", "dataset_index"]
+        for copy_column in id_columns:
+            df_results[copy_column] = df_val_best[copy_column].values
+
+        # These columns are already available in the results table but we still need them for the join
+        id_columns += ["fold_name", "image_name"]
+
+        # Merge results into existing validation table, will result in duplicate columns
+        df_val_new = df_val.merge(
+            df_results, how="left", on=id_columns, validate="one_to_one", suffixes=("_old", "_new")
+        )
 
+        # Overwrite _old columns with _new ones but only for the best epoch and the first dataset (for the remaining rows we keep the old data)
+        for c in df_results.columns:
+            if c not in id_columns and f"{c}_new" in df_val_new and f"{c}_old" in df_val_new:
+                df_val_new[c] = df_val_new[f"{c}_new"].fillna(df_val_new[f"{c}_old"])
+
+        df_val_new.drop(columns=df_val_new.filter(regex="_(?:old|new)$").columns.tolist(), inplace=True)
+        assert not any([c.endswith(("_old", "_new")) for c in df_val_new.columns]), "Merge columns are still present"
+        assert all([c in df_val_new for c in df_results.columns]), "Some columns are missing"
+
+        # Some checks that the merge actually worked
+        df_changed = df_val_new.query("epoch_index == best_epoch_index and dataset_index == 0")
+        df_unchanged = df_val_new.query("not (epoch_index == best_epoch_index and dataset_index == 0)")
+        assert len(df_changed) + len(df_unchanged) == len(df_val_new), "Tables do not add up"
+
+        assert (
+            pd.isna(df_unchanged[[c for c in df_results.columns if c not in df_val]]).all().all()
+        ), "Old values for non-existing columns must be nan"
+        assert not pd.isna(df_changed[df_results.columns]).all().all(), "All new values must be non-nan"
+        assert len(df_val) == len(df_val_new), "The validation table must not change in length"
+
+        # Finally, overwrite existing validation table
+        df_val_new.reset_index(inplace=True, drop=True)
+        df_val_new.to_pickle(self.target_dir / "validation_table.pkl.xz")
 
-class TableTestPredictor(EvaluationMixin, TestPredictor):
-    def __init__(self, *args, test_table_name: str = "test_table", **kwargs):
-        super().__init__(*args, **kwargs)
-        self.rows = []
-        self.config["input/no_labels"] = False
-        self.evaluation_kwargs = {"metrics": self.metrics}
-        self.test_table_name = test_table_name
-
-    def produce_predictions(self, model: HTCLightning, batch: dict[str, torch.Tensor], **kwargs) -> None:
-        self.model = model
-
-        rows = self._validate_batch(batch, dataloader_idx=0)
-        rows = apply_recursive(lambda x: x.cpu().numpy() if type(x) == torch.Tensor else x, rows)
-        self.rows += rows
-
-    def predict_step(self, batch: dict[str, torch.Tensor], batch_idx: int = None) -> dict[str, torch.Tensor]:
-        return self.model.predict_step(batch)
-
-    def save_table(self, output_dir: Path) -> None:
-        df = pd.DataFrame(self.rows)
-        _save_test_table(df, output_dir, self.metrics, self.test_table_name)
+    def _save_test_table(self) -> None:
+        # For the test table it is easier, as we create it from scratch (and overwrite an existing one, if available)
+        df_results = pd.DataFrame(list(self.results_list))
+        assert len(df_results) > 0, "No results calculated"
+        assert len(df_results) == len(df_results["image_name"].unique()), "There must be exactly one row per image"
+
+        # Add the distance aggregation of the nsd to the key
+
+        if "NSD" in self.metrics:
+            tolerance_name = settings_seg.nsd_aggregation.split("_")[-1]
+            key_nsd = f"surface_dice_metric_{tolerance_name}"
+            key_nsd_image = f"surface_dice_metric_image_{tolerance_name}"
+            df_results.rename(
+                columns={"surface_dice_metric": key_nsd, "surface_dice_metric_image": key_nsd_image}, inplace=True
+            )
+
+        df_results.to_pickle(self.target_dir / "test_table.pkl.xz")
 
 
 if __name__ == "__main__":
     runner = Runner(description="Re-create the test or validation table for a run with all requested metrics.")
     runner.add_argument("--test")
     runner.add_argument("--test-looc")
     runner.add_argument("--metrics")
     runner.add_argument("--output-dir")
-    runner.add_argument(
-        "--gpu-only",
-        action="store_true",
-        default=False,
-        help=(
-            "If set, the producer/consumer infrastructure will not be used. Instead, everything will be computed on the"
-            " GPU (similar as during training). This makes only sense for metrics which work efficiently on the GPU"
-            " (like DSC)."
-        ),
-    )
-    runner.add_argument(
-        "--NSD_threshold",
-        default=None,
-        type=float,
-        help=(
-            "The threshold which is used for the NSD computation. In the resulting table, a column with the name"
-            " surface_dice_metric_VALUE will be inserted. Only a global threshold is supported at the moment.If None,"
-            " the default (class-wise) thresholds from the MIA2021 paper will be used."
-        ),
-    )
-
-    if runner.args.gpu_only:
-        if runner.args.test:
-            predictor = TableTestPredictor(runner.run_dir, metrics=runner.args.metrics)
-        else:
-            predictor = TableValidationPredictor(runner.run_dir, metrics=runner.args.metrics)
 
-        with torch.autocast(device_type="cuda"):
-            predictor.start(task_queue=None, hide_progressbar=False)
-
-        target_dir = runner.args.output_dir if runner.args.output_dir is not None else runner.run_dir
-        predictor.save_table(target_dir)
+    if runner.args.test:
+        TestClass = TestLeaveOneOutPredictor if runner.args.test_looc else TestPredictor
+        runner.start(TestClass, ImageTableConsumer)
     else:
-        if runner.args.test:
-            TestClass = TestLeaveOneOutPredictor if runner.args.test_looc else TestPredictor
-            runner.start(TestClass, ImageTableConsumer)
-        else:
-            runner.start(ValidationPredictor, ImageTableConsumer)
+        runner.start(ValidationPredictor, ImageTableConsumer)
```

## htc/models/run_generate_configs.py

```diff
@@ -20,30 +20,29 @@
     params: dict = None,
     test: bool = False,
     return_config_only: bool = False,
     tuning: bool = False,
     memory: str = "10.7G",
 ) -> None:
     timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-    config = Config.from_model_name(config_name_or_path, model_name)
+    config = Config.load_config(config_name_or_path, model_name)
 
     if params is None:
         params = {
             "seed": [settings.default_seed],
             # 'model/architecture_name': ["Model3D2DSeg", "DynUNet"],
             # 'dataloader_kwargs/batch_size': [8],
             # 'dataloader_kwargs/num_workers': [8],
         }
 
-    bashstring = f"""\
-#!/bin/bash
-
-# Run jobs
-ssh {settings.dkfz_userid}@bsub01.lsf.dkfz.de <<"BASH"
-"""
+    bashstring = (
+        '#!/bin/bash\n\nscript_path=$(realpath "$BASH_SOURCE")\nscript_path=$(dirname "$script_path")\n\n# Execute'
+        " everything in the root of the repository\ncd $script_path/../../../.. || exit\n\n# Make environment variables"
+        ' available\nsource .env\n\n# Run jobs\nssh $DKFZ_USERID@bsub01.lsf.dkfz.de <<"BASH"\n'
+    )
 
     configs = []
     for parameters in itertools.product(*params.values()):
         new_config = copy.copy(config)
 
         # Disable logging for cluster runs
         new_config["trainer_kwargs/enable_progress_bar"] = False
@@ -65,30 +64,23 @@
 
         n_gpus = new_config.get("trainer_kwargs/devices", 1)
 
         # Use all available CPUs
         new_config["dataloader_kwargs/num_workers"] = new_config["dataloader_kwargs/num_workers"] * n_gpus
 
         # Store the config in the configs folder
-        parts_str = ",".join(filename_parts)
-        if parts_str == f"seed={settings.default_seed}":
-            # Setting just the default seed can be used to generate a run for the given config (without further changes)
-            new_config["config_name"] = "generated_" + config["config_name"]
-        else:
-            new_config["config_name"] = "generated_" + config["config_name"] + "_" + parts_str
+        new_config["config_name"] = "generated_" + config["config_name"] + "_" + ",".join(filename_parts)
         new_config["config_name"] = new_config["config_name"].replace(
             "<", "LT"
         )  # Avoid special symbols which may break the cluster
         filename = new_config["config_name"] + ".json"
         config_dir = config.path_config.parent
         new_config.save_config(config_dir / filename)
 
-        # To keep the run folder names short, we remove the "generated_" part from the config
-        config_name_short = new_config["config_name"].removeprefix("generated_")
-        run_name = datetime.now().strftime(f"{timestamp}_{config_name_short}")
+        run_name = datetime.now().strftime(f'{timestamp}_{new_config["config_name"]}')
         config_rel_path = Path(config_name_or_path).parent / filename
 
         if single_submit:
             bashstring += cluster_command(
                 f'--model {model_name} --config "{config_rel_path}"', n_gpus=n_gpus, memory=memory
             )
             if test:
@@ -119,16 +111,14 @@
         bashstring += "BASH\n"
         bashsavepath = config_dir / f'submit_jobs_{config["config_name"]}.sh'
         with bashsavepath.open("wb") as f:
             f.write(bashstring.encode())
 
 
 if __name__ == "__main__":
-    # Generate a submission script for all folds:
-    # htc generate_configs --model image --config context/models/configs/glove_organ_transplantation_0.8.json
     parser = argparse.ArgumentParser(
         description="Generate configs for a model", formatter_class=argparse.ArgumentDefaultsHelpFormatter
     )
     parser.add_argument("--model", type=str, required=True, help="Name of the model to train (e.g. image or pixel).")
     parser.add_argument(
         "--config",
         default="default.json",
```

## htc/models/run_prepare_pretrained.py

```diff
@@ -1,14 +1,16 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import hashlib
 from pathlib import Path
 from zipfile import ZipFile
 
+from rich.progress import track
+
 from htc.evaluation.model_comparison.paper_runs import collect_comparison_runs
 from htc.settings import settings
 from htc.settings_seg import settings_seg
 from htc.utils.file_transfer import upload_file_s3
 from htc.utils.general import sha256_file
 
 
@@ -57,21 +59,19 @@
     run_dirs = []
     for i, row in collect_comparison_runs(settings_seg.model_comparison_timestamp).iterrows():
         for model_type in ["rgb", "param", "hsi"]:
             run_dir = settings.training_dir / row["model"] / row[f"run_{model_type}"]
             run_dirs.append(run_dir)
 
     known_models = {}
-    for run_dir in run_dirs:
-        print(run_dir)
+    for run_dir in track(run_dirs):
         name = f"{run_dir.parent.name}@{run_dir.name}"
         known_models[name] = {
             "sha256": compress_run(run_dir, output_path=target_dir / f"{name}.zip"),
             "url": upload_file_s3(local_path=target_dir / f"{name}.zip", remote_path=f"models/{name}.zip"),
         }
 
-    # Update known_models of HTCModel with the output of this script
     print(known_models)
 
 
 if __name__ == "__main__":
     compress_model_comparison_runs()
```

## htc/models/run_training.py

```diff
@@ -8,17 +8,18 @@
 import subprocess
 import sys
 import warnings
 from datetime import datetime
 
 import numpy as np
 import torch
-from lightning import Trainer, seed_everything
-from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichProgressBar, StochasticWeightAveraging
-from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger
+from pytorch_lightning import Trainer, seed_everything
+from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, RichProgressBar, StochasticWeightAveraging
+from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
+from rich.progress import track
 from threadpoolctl import threadpool_limits
 
 from htc.models.common.HTCLightning import HTCLightning
 from htc.models.common.utils import adjust_num_workers, infer_swa_lr
 from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
 from htc.utils.Config import Config
@@ -26,20 +27,20 @@
 from htc.utils.DuplicateFilter import DuplicateFilter
 from htc.utils.MeasureTime import MeasureTime
 
 
 class FoldTrainer:
     def __init__(self, model_name: str, config_name: str):
         self.model_name = model_name
-        self.config = Config.from_model_name(config_name, model_name, use_shared_dict=True)
+        self.config = Config.load_config(config_name, model_name, use_shared_dict=True)
 
         adjust_num_workers(self.config)
 
         # There must be a label mapping defined (class names to label ids)
-        if not self.config["input/no_labels"] and "label_mapping" not in self.config:
+        if "label_mapping" not in self.config:
             settings.log.warning(
                 "No label mapping specified in the config file. The default mapping from the images will be used which"
                 " may not be what you want (e.g. it is different across datasets). Best practice is to explicitly"
                 " specify the label mapping in the config"
             )
 
         self.data_specs = DataSpecification.from_config(self.config)
@@ -111,15 +112,15 @@
 
             if name.startswith("train"):
                 train_paths += paths
             elif name.startswith("val"):
                 dataset = self.LightningClass.dataset(paths=paths, train=False, config=self.config, fold_name=fold_name)
                 datasets_val.append(dataset)
             else:
-                settings.log_once.info(f"The split {name} is not used for training (neither starts with train nor val)")
+                raise ValueError(f"Invalid dataset name {name}")
 
         if test:
             # To avoid potential errors, we activate the test set only temporarily to get the paths
             # If other classes access the specs, they cannot accidentally access the test set
             with self.data_specs.activated_test_set():
                 test_paths = self.data_specs.fold_paths(fold_name, "^test")
 
@@ -131,22 +132,22 @@
         # Set some defaults if missing the config
         if "validation/checkpoint_metric" not in self.config:
             self.config["validation/checkpoint_metric"] = "dice_metric"
             settings.log.warning(
                 "No value set for validation/checkpoint_metric in the config. This should be the name of the metric"
                 " which will be used to determine the best model. Please note that this does not specify the actual"
                 " calculation of the metric but just the name of the metric (e.g. used in the checkpoint filename)."
-                f" Defaulting to \"{self.config['validation/checkpoint_metric']}\""
+                f' Defaulting to "{self.config["validation/checkpoint_metric"]}"'
             )
         if "validation/dataset_index" not in self.config:
             self.config["validation/dataset_index"] = 0
             settings.log.warning(
                 "No value set for validation/dataset_index in the config. This specifies the main validation dataset,"
                 " e.g. used for checkpointing. Currently, only one validation dataset can be used. Defaulting to"
-                f" \"{self.config['validation/dataset_index']}\""
+                f' "{self.config["validation/dataset_index"]}"'
             )
 
         checkpoint_saving = self.config.get("validation/checkpoint_saving", "best")
         if checkpoint_saving == "best":
             save_top_k = 1
             save_last = False
             test_ckpt_path = "best"
@@ -162,59 +163,48 @@
 
         checkpoint_callback = ModelCheckpoint(
             dirpath=model_dir,
             filename="{epoch:02d}-{" + self.config["validation/checkpoint_metric"] + ":.2f}",
             save_top_k=save_top_k,
             save_last=save_last,
             monitor=self.config["validation/checkpoint_metric"],
-            mode=self.config.get("validation/checkpoint_mode", "max"),
+            mode="max",
         )
         lr_monitor = LearningRateMonitor(logging_interval="epoch")
 
         lightning_kwargs = {}
         if test and len(test_paths) > 0:
             dataset_test = self.LightningClass.dataset(
                 paths=test_paths, train=False, config=self.config, fold_name=fold_name
             )
             lightning_kwargs["dataset_test"] = dataset_test
 
-        module = self.LightningClass(dataset_train, datasets_val, self.config, fold_name=fold_name, **lightning_kwargs)
+        module = self.LightningClass(dataset_train, datasets_val, self.config, **lightning_kwargs)
         callbacks = [checkpoint_callback, lr_monitor]
 
         if self.config["trainer_kwargs/enable_progress_bar"] is not False:
             callbacks.append(RichProgressBar(leave=True))
 
         # Stochastic Weight Averaging
         if self.config["swa_kwargs"]:
             self.config["swa_kwargs/swa_lrs"] = infer_swa_lr(self.config)
             swa = StochasticWeightAveraging(**self.config["swa_kwargs"])
             callbacks.append(swa)
 
-        # May be faster on a 3090 and should not hurt (https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision)
-        torch.set_float32_matmul_precision("high")
-
         # Sanity check is disabled since it only leads to problems (duplicate epoch number, incomplete dataset)
         trainer = Trainer(logger=logger, callbacks=callbacks, num_sanity_val_steps=0, **self.config["trainer_kwargs"])
 
         # There are some problems on the cluster if too many threads are used because then more CPUs are used as available for the job
         # Hopefully, this with block limits the issue (right now there was only an issue on the main process)
         with threadpool_limits(2), warnings.catch_warnings():
             # We store the logs in the run folder, so it will never be empty
             warnings.filterwarnings(
                 "ignore", message="Checkpoint directory.*exists and is not empty", category=UserWarning
             )
             warnings.filterwarnings("ignore", message=".*`IterableDataset` has `__len__` defined", category=UserWarning)
-            warnings.filterwarnings(
-                "ignore",
-                message=(
-                    ".*from an ambiguous collection. The batch size we found is"
-                    f" {self.config['dataloader_kwargs/batch_size']}.*"
-                ),
-                category=UserWarning,
-            )
 
             if self.config["wandb_kwargs"]:
                 wandb_logger = WandbLogger(save_dir=model_dir, **self.config["wandb_kwargs"])
                 wandb_logger.watch(module.model, log="all", log_freq=10)
 
             trainer.fit(module)
             if test and len(test_paths) > 0:
@@ -235,15 +225,15 @@
         shutil.copy2(self.data_specs.path, model_dir / "data.json")
 
 
 def train_all_folds(
     model_name: str, config_name: str, run_folder: str, test: bool, file_log_handler: DelayedFileHandler
 ) -> None:
     with MeasureTime("training_all", silent=True) as mt:
-        config = Config.from_model_name(config_name, model_name)
+        config = Config.load_config(config_name, model_name)
 
         # Unique folder name per run
         if run_folder is None:
             run_folder = datetime.now().strftime(f'%Y-%m-%d_%H-%M-%S_{config["config_name"]}')
         run_folder_tmp = (  # The results are first written to a temporary directory and later renamed back. This helps to easily detect incomplete runs
             f"running_{run_folder}"
         )
@@ -252,24 +242,24 @@
         run_path = settings.training_dir / model_name / run_folder_tmp
         run_path.mkdir(exist_ok=True, parents=True)
         file_log_handler.set_filename(run_path / "log.txt", mode="w")
 
         # Start the training script for each fold
         data_specs = DataSpecification.from_config(config)
         error_occurred = False
-        for i, fold_name in enumerate(data_specs.fold_names()):
+        for fold_name in track(data_specs.fold_names(), description="folds"):
             # We start the training of a fold in a new process so that we can start fresh, i.e. this makes sure that all resources like RAM are freed
             command = (
                 f"{sys.executable} {__file__} --model {model_name} --config {config_name} --fold-name"
                 f' {fold_name} --run-folder "{run_folder_tmp}"'
             )
             if test:
                 command += " --test"
 
-            settings.log.info(f"Starting training of the fold {fold_name} [{i + 1}/{len(data_specs.fold_names())}]")
+            settings.log.info(f"Starting training of the fold {fold_name}")
             ret = subprocess.run(command, shell=True)
             if ret.returncode != 0:
                 settings.log.error(f"Training of the fold {fold_name} was not successful (returncode={ret.returncode}")
                 error_occurred = True
 
         if error_occurred:
             settings.log.error("Some folds were not successful (see error messages above)")
```

## htc/models/common/EvaluationMixin.py

```diff
@@ -1,75 +1,34 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import itertools
 from pathlib import Path
-from typing import Any
 
 import pandas as pd
 import torch
 import torch.nn.functional as F
 
 from htc.evaluation.evaluate_images import evaluate_images
 from htc.models.common.MetricAggregation import MetricAggregation
-from htc.models.common.utils import multi_label_condensation
 from htc.tivita.DataPath import DataPath
 
 
 class EvaluationMixin:
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.evaluation_kwargs = {}  # Additional arguments for the evaluate_images function (e.g. additional metrics)
         self.df_validation_results = pd.DataFrame()
-        self.validation_results_epoch = []  # Also used for storing the test results
 
-    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int, dataloader_idx: int = 0) -> None:
-        if batch_idx == 0 and dataloader_idx == 0:
-            assert len(self.validation_results_epoch) == 0, "Validation results are not properly cleared"
-
-        self.validation_results_epoch.append(self._validate_batch(batch, dataloader_idx))
-
-    def on_validation_epoch_end(self) -> None:
-        # First level (list): batches
-        # Second level (list): images
-        # Third level (dict): results per image
-        df_epoch = pd.DataFrame(list(itertools.chain.from_iterable(self.validation_results_epoch)))
-
-        agg = MetricAggregation(df_epoch, config=self.config)
-        self.log_checkpoint_metric(agg.checkpoint_metric())
-        self.df_validation_results = pd.concat([self.df_validation_results, df_epoch])
-        self.df_validation_results.to_pickle(Path(self.logger.save_dir) / "validation_results.pkl.xz")
-
-        # Start clean for the next validation round
-        self.validation_results_epoch = []
-
-    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:
-        self.validation_step(batch, batch_idx)
-
-    def on_test_epoch_end(self) -> None:
-        df_test = pd.DataFrame(list(itertools.chain.from_iterable(self.validation_results_epoch)))
-        df_test.drop(columns=["epoch_index"], inplace=True)
-        df_test.to_pickle(Path(self.logger.save_dir) / "test_results.pkl.xz")
-        self.validation_results_epoch = []
-
-    def _validate_batch(self, batch: dict[str, torch.Tensor], dataloader_idx: int) -> list[dict[str, Any]]:
+    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int, dataset_idx: int = 0) -> list[dict]:
         batch_clean = {k: v for k, v in batch.items() if not k.startswith("labels")}
 
         logits = self.predict_step(batch_clean)
-        if self.config["model/activations"] == "sigmoid":
-            predictions = multi_label_condensation(logits["class"], self.config)["predictions"]
-        else:
-            predictions = F.softmax(logits["class"], dim=1)
-
+        softmaxes = F.softmax(logits["class"], dim=1)
         batch_results_class = evaluate_images(
-            predictions,
-            batch["labels"],
-            batch["valid_pixels"],
-            n_classes=logits["class"].shape[1],
-            **self.evaluation_kwargs,
+            softmaxes, batch["labels"], batch["valid_pixels"], n_classes=softmaxes.shape[1]
         )
 
         domains = self.config.get("input/target_domain", [])
         predicted_domains = {}
         for domain in domains:
             if domain in logits:
                 # We may use the domain but to not necessarily also predict it
@@ -77,19 +36,19 @@
                 assert len(batch_results_class) == len(predicted_domains[domain])
 
         rows = []
         for b in range(len(batch_results_class)):
             image_name = batch["image_name"][b]
             path = DataPath.from_image_name(image_name)
 
-            current_row = {}
-            if hasattr(self, "current_epoch"):
-                current_row["epoch_index"] = self.current_epoch
-            current_row["dataset_index"] = dataloader_idx
-            current_row["image_name"] = image_name
+            current_row = {
+                "epoch_index": self.current_epoch,
+                "dataset_index": dataset_idx,
+                "image_name": image_name,
+            }
             current_row |= path.image_name_typed()
 
             for key, value in batch_results_class[b].items():
                 if type(value) == torch.Tensor:
                     current_row[key] = value.cpu().numpy()
                 else:
                     current_row[key] = value
@@ -98,7 +57,35 @@
                 current_row[domain] = batch[domain][b].item()
                 if domain in predicted_domains:
                     current_row[f"{domain}_predicted"] = predicted_domains[domain][b].item()
 
             rows.append(current_row)
 
         return rows
+
+    def validation_epoch_end(self, outputs: list[list[list[dict]]]) -> None:
+        # First list: datasets
+        # Second list: batches
+        # Third list: images
+        if len(self.datasets_val) > 1:
+            df_epoch = pd.concat([pd.DataFrame(list(itertools.chain(*output))) for output in outputs])
+        else:
+            df_epoch = pd.DataFrame(list(itertools.chain(*outputs)))
+
+        agg = MetricAggregation(df_epoch, config=self.config)
+        self.log_checkpoint_metric(agg.checkpoint_metric())
+        self.df_validation_results = pd.concat([self.df_validation_results, df_epoch])
+
+    def on_validation_epoch_end(self) -> None:
+        assert self.checkpoint_metric_logged[self.current_epoch], (
+            "log_checkpoint_metric was not called! This is strictly necessary so that Lightning knows which model"
+            " should be saved"
+        )
+        self.df_validation_results.to_pickle(Path(self.logger.save_dir) / "validation_results.pkl.xz")
+
+    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict:
+        return self.validation_step(batch, batch_idx)
+
+    def test_epoch_end(self, outputs: list[dict]) -> None:
+        df_test = pd.DataFrame(list(itertools.chain(*outputs)))
+        df_test = df_test.drop(columns=["epoch_index"])
+        df_test.to_pickle(Path(self.logger.save_dir) / "test_results.pkl.xz")
```

## htc/models/common/ForwardHookPromise.py

```diff
@@ -1,15 +1,14 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 from typing import Any, Callable
 
 import torch
 import torch.nn as nn
-from typing_extensions import Self
 
 
 class ForwardHookPromise:
     def __init__(self, module: nn.Module, hook: Callable = None):
         """
         This class can be used to safely register forward hooks on modules. It returns a promise for the forward pass which can be used to read the data afterwards. The promise is meant to be used in a with block and the data is only accessible inside this block. This ensures that the data is freed afterwards avoiding out of memory issues.
 
@@ -56,15 +55,15 @@
 
     def _hook(self, module: nn.Module, module_in: tuple, module_out: torch.Tensor) -> None:
         if self.hook is None:
             self._data = module_out
         else:
             self._data = self.hook(module, module_in, module_out)
 
-    def __enter__(self) -> Self:
+    def __enter__(self) -> "ForwardHookPromise":
         self._in_context = True
         return self
 
     def __exit__(self, type: Any, value: Any, tb: Any) -> None:
         self._handle.remove()
         del self._data
         self._in_context = False
```

## htc/models/common/HTCDataset.py

```diff
@@ -3,14 +3,15 @@
 
 import re
 from abc import ABC, abstractmethod
 from typing import Union
 
 import numpy as np
 import torch
+from PIL import Image
 from torch.utils.data import Dataset
 
 from htc.models.common.transforms import HTCTransformation, ToType
 from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
 from htc.settings_seg import settings_seg
 from htc.tivita.DataPath import DataPath
@@ -35,35 +36,31 @@
             config = Config({})
 
         self.train = train
         self.paths = paths
         self.config = config
         self.fold_name = fold_name
         self.image_names = [p.image_name() for p in self.paths]
-        self.features_dtype = (
-            torch.float16 if self.config["trainer_kwargs/precision"] in [16, "16-mixed"] else torch.float32
-        )
+        self.features_dtype = torch.float16 if self.config["trainer_kwargs/precision"] == 16 else torch.float32
         self.n_channels_loading = self.config["input/n_channels"]  # Value before any channel selection
 
         # Data transformations
         if self.train and self.config["input/transforms_cpu"]:
             self.transforms = HTCTransformation.parse_transforms(
                 self.config["input/transforms_cpu"],
                 initial_dtype=self.features_dtype,
                 config=self.config,
                 fold_name=self.fold_name,
-                paths=self.paths,
             )
         elif not self.train and self.config["input/test_time_transforms_cpu"]:
             self.transforms = HTCTransformation.parse_transforms(
                 self.config["input/test_time_transforms_cpu"],
                 initial_dtype=self.features_dtype,
                 config=self.config,
                 fold_name=self.fold_name,
-                paths=self.paths,
             )
         else:
             self.transforms = HTCTransformation.parse_transforms(initial_dtype=self.features_dtype)
 
         if self.config["input/channel_selection"] and self.n_channels_loading == 100:
             assert (
                 type(self.config["input/channel_selection"]) == list
@@ -151,14 +148,18 @@
         # We use the median tables to calculate the label count information
         df = median_table(image_names=image_names, label_mapping=mapping)
         df = df.groupby("label_index_mapped", as_index=False)["n_pixels"].sum()
         df.sort_values(by="label_index_mapped", inplace=True)
 
         return torch.from_numpy(df["label_index_mapped"].values), torch.from_numpy(df["n_pixels"].values)
 
+    def from_image_name(self, name: str) -> dict[str, torch.Tensor]:
+        index = self.image_names.index(name)
+        return self[index]
+
     def read_labels(self, path: DataPath) -> Union[dict[str, torch.Tensor], None]:
         """
         Read the labels for the data path, compute the valid pixels and apply the label mapping.
 
         Args:
             path: Data path to the image.
 
@@ -258,14 +259,28 @@
                 assert sample["features"].shape[-1] == self.config["input/n_channels"], (
                     f'Number of feature channels ({sample["features"].shape = }) does not correspond to the number of'
                     f' channels in the config {self.config["input/n_channels"] = }'
                 )
             else:
                 assert "features" not in sample
 
+        if self.config["input/specs_threshold"]:
+            # Read the specular highlight masks
+            spec_path = (
+                settings.intermediates_dir
+                / "specs"
+                / f'specular_highlight_masks_from_rgb2lab_{self.config["input/specs_threshold"]}'
+                / f"{path.image_name()}.png"
+            )
+            assert spec_path.exists(), f"Cannot find specular highlights for {path.image_name()} (path={spec_path})"
+            spec = np.asarray(Image.open(spec_path))
+            spec[spec == 255] = 1
+
+            sample["specs"] = torch.from_numpy(spec)  # True = spec
+
         for name, tensor in sample.items():
             if name.startswith(("labels", "valid_pixels")):
                 # May be CHW
                 spatial_shape = tensor.shape[-2:]
             else:
                 # May be HWC
                 spatial_shape = tensor.shape[:2]
@@ -308,15 +323,15 @@
         assert not torch.isnan(weights).any(), f"Some dataset sampling weights contain NAN values: {weights}"
 
         sample_weights = weights[dataset_indices]
         return sample_weights
 
     def _load_preprocessed(self, image_name: str, folder_name: str) -> Union[np.ndarray, None]:
         # Load a preprocessed HSI cube
-        files_dir = settings.intermediates_dir_all / "preprocessing" / folder_name
+        files_dir = settings.intermediates_dir / "preprocessing" / folder_name
 
         extensions = [
             (".blosc", decompress_file),
             (".npy", np.load),
             (".npz", lambda p: np.load(p, allow_pickle=True)["data"]),
         ]
         data = None
@@ -325,25 +340,24 @@
             if file_path.exists():
                 data = load_func(file_path)
                 break
 
         assert data is not None, (
             f"Could not ind the image {image_name}. This probably means that you have not registered the dataset where"
             " this image is from, i.e. you need to set the corresponding environment variable. The following"
-            f" intermediate directories are registered: {settings.intermediates_dir_all} and the following file"
-            f" extensions were tried: {[e[0] for e in extensions]}"
+            f" intermediate directories are registered: {settings.intermediates_dir} and the following file extensions"
+            f" were tried: {[e[0] for e in extensions]}"
         )
 
-        if self.features_dtype != torch.float16:
-            data_dtype = next(iter(data.values())).dtype if type(data) == dict else data.dtype
-            if data_dtype == np.float16:
-                settings.log_once.warning(
-                    f"You have set the precision to {self.features_dtype} but the preprocessed data has a precision of"
-                    f" {data_dtype} which means that you will not work with the full precision of the original data"
-                )
+        precision = self.config["trainer_kwargs/precision"]
+        if precision is not None and precision != 16 and data.dtype == np.float16:
+            settings.log.warning(
+                f"You have set the precision to {precision} but the preprocessed data has a precision of"
+                f" {data.dtype} which means that you will not work with the full precision of the original data"
+            )
 
         if folder_name == "parameter_images":
             names = self.config.get("input/parameter_names", ["StO2", "NIR", "TWI", "OHI"])
             assert len(names) > 0, "At least the name of one parameter is required"
             assert all(n in data for n in names), "Not all parameter names are stored in the preprocessed file"
 
             # Store all parameters in one array
```

## htc/models/common/HTCDatasetStream.py

```diff
@@ -48,16 +48,16 @@
             message="Named tensors and all their associated APIs are an experimental feature and subject to change",
         )
 
     def __iter__(self) -> Iterator[dict[str, Union[int, bool]]]:
         assert len(self.shared_dict) > 0, "Shared dictionary is not initialized. Did you forget to call init_shared()?"
         assert self.batch_part_size > 0, (
             f"batch_part_size must not be {self.batch_part_size}. Incompatible batch size"
-            f" ({self.config['dataloader_kwargs/batch_size']}) or number of workers"
-            f" ({self.config['dataloader_kwargs/num_workers']})"
+            f' ({self.config["dataloader_kwargs/batch_size"]}) or number of workers'
+            f' ({self.config["dataloader_kwargs/num_workers"]})'
         )
 
         worker_base_index = self._get_worker_index() * self.batch_part_size
         i = 0
         buffer_index = 0
         expected_keys = set(self.shared_dict.keys())
 
@@ -122,15 +122,16 @@
 
     @property
     def buffer_size(self) -> int:
         return self.prefetch_factor + 1  # Shared ring buffer
 
     @abstractmethod
     def iter_samples(self) -> Iterator[dict[str, torch.Tensor]]:
-        """This method must be implemented by all child classes and yields one sample at a time (e.g. one patch at a time) or complete batch parts (e.g. pixel dataset)."""
+        """This method must be implemented by all child classes and yields one sample at a time (e.g. one patch at a time) or complete batch parts (e.g. pixel dataset).
+        """
         pass
 
     def _iter_paths(self) -> tuple[int, DataPath]:
         worker_index = self._get_worker_index()
 
         if self.sampler is None:
             path_indices = np.array_split(
@@ -150,31 +151,31 @@
                 len(path_indices[worker_index]),
                 replace=False,
                 p=path_indices_weights[worker_index] / np.sum(path_indices_weights[worker_index]),
             )
             if self.single_pass:
                 # normalize the weights so the probability sums to 1
                 for path_index in indices:
-                    yield worker_index, path_index
+                    yield worker_index, self.paths[path_index]
             else:
                 for path_index in cycle(indices):
-                    yield worker_index, path_index
+                    yield worker_index, self.paths[path_index]
         else:
             i = 0
             while True:
                 # Split indices across workers, e.g. with 3 workers:
                 # w1: 0, 3, 6
                 # w2: 1, 4, 7
                 # w3. 2, 5, 8
                 sampler_index = worker_index + i * self.config["dataloader_kwargs/num_workers"]
                 if sampler_index >= len(self.path_indices_worker):
                     break
 
                 path_index = self.path_indices_worker[sampler_index]
-                yield worker_index, path_index.item()
+                yield worker_index, self.paths[path_index]
                 i += 1
 
     def _sample_pixel_indices(self, labels: torch.Tensor, n_samples: int = None) -> Sampler[int]:
         if n_samples is None:
             n_samples = len(labels)
 
         if self.config["input/oversampling"]:
```

## htc/models/common/HTCLightning.py

```diff
@@ -1,57 +1,50 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import copy
+import importlib
 from abc import abstractmethod
 from pathlib import Path
 
 import numpy as np
 import pandas as pd
+import pytorch_lightning as pl
 import torch
-from lightning import LightningModule
 from torch.utils.data import DataLoader
 from torch.utils.data.sampler import RandomSampler
 
 from htc.models.common.EvaluationMixin import EvaluationMixin
 from htc.models.common.HTCDataset import HTCDataset
 from htc.models.common.transforms import HTCTransformation
-from htc.models.common.utils import get_n_classes, parse_optimizer
+from htc.models.common.utils import get_n_classes
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 from htc.utils.type_from_string import type_from_string
 
 
-class HTCLightning(EvaluationMixin, LightningModule):
+class HTCLightning(EvaluationMixin, pl.LightningModule):
     def __init__(
-        self,
-        dataset_train: HTCDataset,
-        datasets_val: list[HTCDataset],
-        config: Config,
-        dataset_test: HTCDataset = None,
-        fold_name: str = None,
+        self, dataset_train: HTCDataset, datasets_val: list[HTCDataset], config: Config, dataset_test: HTCDataset = None
     ):
         super().__init__()
 
         self.dataset_train = dataset_train
         self.datasets_val = datasets_val
         self.dataset_test = dataset_test
-        self.fold_name = fold_name
 
         self.config = config
-        if not self.config["input/no_labels"]:
-            self.n_classes = get_n_classes(self.config)
+        self.n_classes = get_n_classes(self.config)
 
         self.checkpoint_metric_logged = {}
         self.df_validation_results = pd.DataFrame()
 
         # Statistics about the training
         self.training_stats = []
-        self.training_stats_epoch = {}
 
         # GPU transformations
         self.transforms = {}
 
     def train_dataloader(self) -> DataLoader:
         sampler = RandomSampler(self.dataset_train, replacement=True, num_samples=self.config["input/epoch_size"])
         return DataLoader(
@@ -74,47 +67,62 @@
     def test_dataloader(self, **kwargs) -> DataLoader:
         dataloader_kwargs = copy.deepcopy(self.config["dataloader_kwargs"])
         dataloader_kwargs |= kwargs
 
         return DataLoader(self.dataset_test, **dataloader_kwargs)
 
     def configure_optimizers(self):
-        return parse_optimizer(self.config, self.model)
+        # Dynamically initialize the optimizer based on the config
+        optimizer_param = copy.deepcopy(self.config["optimization/optimizer"])
+        del optimizer_param["name"]
+
+        name = self.config["optimization/optimizer/name"]
+        module = importlib.import_module("torch.optim")
+        optimizer_class = getattr(module, name)
+
+        optimizer = optimizer_class(self.model.parameters(), **optimizer_param)
+
+        if self.config["optimization/lr_scheduler"]:
+            # Same for the scheduler, if available
+            scheduler_param = copy.deepcopy(self.config["optimization/lr_scheduler"])
+            del scheduler_param["name"]
+
+            name = self.config["optimization/lr_scheduler/name"]
+            module = importlib.import_module("torch.optim.lr_scheduler")
+            scheduler_class = getattr(module, name)
+
+            scheduler = scheduler_class(optimizer, **scheduler_param)
+            return [optimizer], [scheduler]
+        else:
+            return optimizer
 
     @staticmethod
     @abstractmethod
     def dataset(paths: list[DataPath], train: bool, config: Config, fold_name: str) -> HTCDataset:
         pass
 
     def on_after_batch_transfer(self, batch: dict[str, torch.Tensor], dataloader_idx: int):
         transforms = self.parse_transforms_gpu()
-        with torch.autocast(device_type=self.device.type), torch.no_grad():
-            batch = HTCTransformation.apply_valid_transforms(batch, transforms)
-
-        if self.training and "image_index" in batch and type(batch["image_index"]) == torch.Tensor:
-            # The image index may not be a tensor in prediction scenarios if a single image is supplied manually
-            # In these cases, however, we don't need training statistics
-            if "img_indices" not in self.training_stats_epoch:
-                self.training_stats_epoch["img_indices"] = []
-
-            indices = batch["image_index"].cpu()
-            self.training_stats_epoch["img_indices"].append(indices)
+        # TODO: add autocasting after this is fixed in kornia
+        # with torch.autocast(device_type=self.device.type):
+        #     batch = HTCTransformation.apply_valid_transforms(batch, transforms)
+        batch = HTCTransformation.apply_valid_transforms(batch, transforms)
 
         return batch
 
     def _predict_images(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
         """
         Create image-level predictions for the given batch. Usually required in the validation phase to ensure that a whole image can be validated irrespective of the underlying model (e.g. the pixel model must create the image based on the single pixel predictions). Should only be called by subclasses, the external interface is `predict_step()`.
 
         Args:
             batch: Data from the dataloader.
 
         Returns: Dictionary with predictions of the model (e.g. `result['class']` has a shape of [N, C, H, W]).
         """
-        return {"class": self(batch)}
+        return {"class": self(batch["features"])}
 
     def predict_step(self, batch: dict[str, torch.Tensor], batch_idx: int = None) -> dict[str, torch.Tensor]:
         """
         Create predictions for the batch. This function should be used for inference (and not `._predict_images()`) since it evaluates some basic checks and makes sure that the GPU transformations get applied.
 
         Args:
             batch: Batch for which the predictions should be applied.
@@ -179,31 +187,25 @@
         else:
             self.checkpoint_metric_logged[self.current_epoch] = False
 
         self.log(self.config["validation/checkpoint_metric"], metric_value, prog_bar=True)
         self.checkpoint_metric_logged[self.current_epoch] = True
 
     def on_train_epoch_start(self) -> None:
+        # Workaround to always save the last epoch until the issue is fixed in pl: https://github.com/PyTorchLightning/pytorch-lightning/issues/4539
         if self.current_epoch == self.trainer.max_epochs - 1:
-            settings.log.info("Changed check_val_every_n_epoch to 1 and switched to manual optimization")
-
-            # Workaround to always save the last epoch until the bug is fixed in lightning (https://github.com/Lightning-AI/lightning/issues/4539)
+            settings.log.info("Changed check_val_every_n_epoch to 1")
             self.trainer.check_val_every_n_epoch = 1
 
-            # Disable backward pass for SWA until the bug is fixed in lightning (https://github.com/Lightning-AI/lightning/issues/17245)
-            self.automatic_optimization = False
-
-    def on_train_epoch_end(self) -> None:
-        if len(self.training_stats_epoch) > 0:
-            for key, values in self.training_stats_epoch.items():
-                self.training_stats_epoch[key] = torch.cat(values).cpu().numpy()
+    def training_epoch_end(self, training_step_outputs: list[dict]) -> None:
+        if len(training_step_outputs) > 0 and "img_indices" in training_step_outputs[0]:
+            img_indices = torch.cat([x["img_indices"] for x in training_step_outputs]).cpu().numpy()
 
-            self.training_stats.append(self.training_stats_epoch)
+            self.training_stats.append({"img_indices": img_indices})
             np.savez_compressed(Path(self.logger.save_dir) / "trainings_stats.npz", data=self.training_stats)
-            self.training_stats_epoch = {}
 
     def parse_transforms_gpu(self, config_key: str = None) -> list[HTCTransformation]:
         if config_key is None:
             config_key = "input/transforms_gpu" if self.training else "input/test_time_transforms_gpu"
 
         if config_key not in self.transforms:
             # Only needed during training
```

## htc/models/common/HTCModel.py

```diff
@@ -1,22 +1,19 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import hashlib
 import inspect
+import re
 from pathlib import Path
-from typing import Union
 from zipfile import ZipFile
 
-import pandas as pd
 import torch
 import torch.nn as nn
-from typing_extensions import Self
 
-from htc.models.common.MetricAggregation import MetricAggregation
 from htc.settings import settings
 from htc.utils.Config import Config
 from htc.utils.general import sha256_file
 from htc.utils.helper_functions import run_info
 
 
 class PostInitCaller(type):
@@ -25,76 +22,76 @@
         obj.__post__init__()
         return obj
 
 
 class HTCModel(nn.Module, metaclass=PostInitCaller):
     known_models = {
         "pixel@2022-02-03_22-58-44_generated_default_rgb_model_comparison": {
-            "sha256": "b380b43935a9d7e5fd0c39efe04420539416791b61517b69db9f8e7ee96cd5db",
+            "sha256": "c19c600958fa36f86d8742752bdbab9d067d0ee3f9c0e37cf281ce3b84b139da",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/pixel@2022-02-03_22-58-44_generated_default_rgb_model_comparison.zip",
         },
         "pixel@2022-02-03_22-58-44_generated_default_parameters_model_comparison": {
-            "sha256": "84ff8e96348e907c1911dde774085f0dd7de08f58403f2c3109845b15a1303c6",
+            "sha256": "b38de2a28464aa422b2b8d44861ad9dd1184ceb3053abb5d3d2e811f4ada662c",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/pixel@2022-02-03_22-58-44_generated_default_parameters_model_comparison.zip",
         },
         "pixel@2022-02-03_22-58-44_generated_default_model_comparison": {
-            "sha256": "fc6cdf705b3243643ab1f45daca46436e16ccb183ac62f59bbec825579704162",
+            "sha256": "628f2c79ef3ea020bfdda3820670ffb277c0eca6f46d0d72a1692ca53d80a62a",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/pixel@2022-02-03_22-58-44_generated_default_model_comparison.zip",
         },
         "superpixel_classification@2022-02-03_22-58-44_generated_default_rgb_model_comparison": {
-            "sha256": "6d5fa1a64a85439a4cac354f36801ae38ebebab62321d72338d89dfafc35e904",
+            "sha256": "c515ff6ae939408b0b866ce9d630cc5cd6c16182b0ea90e0e7e43e15673daa35",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/superpixel_classification@2022-02-03_22-58-44_generated_default_rgb_model_comparison.zip",
         },
         "superpixel_classification@2022-02-03_22-58-44_generated_default_parameters_model_comparison": {
-            "sha256": "0d9182bc82643d1f8a7bf74b8cd7bf6684968068c56daa5aef8e76162c476108",
+            "sha256": "57c81b3abfcc785ba04c1203192aa2ebb7d4274f295daff9c50d64a62fcadd5f",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/superpixel_classification@2022-02-03_22-58-44_generated_default_parameters_model_comparison.zip",
         },
         "superpixel_classification@2022-02-03_22-58-44_generated_default_model_comparison": {
-            "sha256": "f495b331b471258af88701d18b585e3638f8bba820c43056b9dda41d470423ef",
+            "sha256": "a39b3fad1e422e6e00563d9193ae751fb54aa3193196878b62be3e3dee8241f2",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/superpixel_classification@2022-02-03_22-58-44_generated_default_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_rgb_model_comparison": {
-            "sha256": "d5e9d831f03ea0b3988d766b0fff2f63d2d0f2321b60edfb3bf7f6a6aeaa17f8",
+            "sha256": "66aa16eb5c0c3969377f30fd927722c48850dba522582565247ed02dbab8db78",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_rgb_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_parameters_model_comparison": {
-            "sha256": "ac5ce443cb5564862d978b02f27560da3b5502083aac27d5dbe6c337adac5090",
+            "sha256": "a4fc42c1f49cdb8a63c070a278c790805c72c8b91679eb39b2b7af4aae73827d",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_parameters_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_model_comparison": {
-            "sha256": "cdfe5bb3894b386e63f588a25a7f2af0fec4d25c89a1f3129895624dd7497e70",
+            "sha256": "05c44ad7122260f391d88436df3402c798b6c866c8c26714802f5cbc2dfa4335",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_64_rgb_model_comparison": {
-            "sha256": "f13d098fc65999eeb3e16d572879baa14fb9c8c558acfc62b7da326691f7fbeb",
+            "sha256": "e158f5138d478f0e186106588b843609d5995dfdf91d82b914e87095af31ba78",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_64_rgb_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_64_parameters_model_comparison": {
-            "sha256": "b1246ceb17f87f08cac3f62038ebf4e29aac6ed71f71909bc58334dac3682559",
+            "sha256": "307e1770dc433dc1ad1418b8ecdfac58efe0fee1d5f6c06b555a3465965838d0",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_64_parameters_model_comparison.zip",
         },
         "patch@2022-02-03_22-58-44_generated_default_64_model_comparison": {
-            "sha256": "31ff412697122afc780c416c53ee465d606cd3fa59feadc87d86cf77f4e90358",
+            "sha256": "09a270aa35923b5053fd558307b6c272815578142dcdbf68cbb695111df51224",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/patch@2022-02-03_22-58-44_generated_default_64_model_comparison.zip",
         },
         "image@2022-02-03_22-58-44_generated_default_rgb_model_comparison": {
-            "sha256": "5bc230782e4a4f252fa5db4c2a30dbb3f150977a6ee56b05d724b84c6d4a7e64",
+            "sha256": "98fd75c3d4729e5d8ed34676e9a6a0e3c1203f56738ffe517198c7807d152611",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/image@2022-02-03_22-58-44_generated_default_rgb_model_comparison.zip",
         },
         "image@2022-02-03_22-58-44_generated_default_parameters_model_comparison": {
-            "sha256": "d65e94e5cb859afdee540e1ed454eede8e9e91778e8e6731a97e3f41889e7e17",
+            "sha256": "82617ffbf9ebc31c93e130a1af3ac8690c46f9f84c032745e97faddfb9786fce",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/image@2022-02-03_22-58-44_generated_default_parameters_model_comparison.zip",
         },
         "image@2022-02-03_22-58-44_generated_default_model_comparison": {
-            "sha256": "7a78a54c1a43ae273b22bd0f721ca1c22c57257c61cbc5bb6e5a84e4895344d4",
+            "sha256": "9df57e16a73c700ff3de1d2b0de2bbf3efdc0b954b50a7f222c268e4079c806f",
             "url": "https://e130-hyperspectal-tissue-classification.s3.dkfz.de/models/image@2022-02-03_22-58-44_generated_default_model_comparison.zip",
         },
     }
 
-    def __init__(self, config: Config, fold_name: str = None):
+    def __init__(self, config: Config):
         """
         Base class for all model classes. It implements some logic to automatically replace the weights of a network with the weights from another pretrained network.
 
         If a model class inherits from this class it becomes very easy to use the same model class again with pretrained weights. You only need to specify the pretrained network in the config and then the weights get replaced automatically. In the config, you have two options to reference the pretrained network (see also the config.schema file):
         - With its path (absolute or relative to the training directory): `config["model/pretrained_model/path"] = "image/my_training_run"`
         - With a dictionary of the relevant properties: `config["model/pretrained_model"] = {"model": "image", "run_folder": "my_training_run"}`
 
@@ -107,176 +104,89 @@
         ...         self.skip_keys_pattern.add("decoder")
         >>> my_model = MyModel(Config({}))
         >>> sorted(my_model.skip_keys_pattern)
         ['classification_head', 'decoder', 'heads.heads', 'segmentation_head']
 
         Args:
             config: Configuration object of the training run.
-            fold_name: the name of the fold being trained. This is used to find the correct parameter for temperature scaling.
         """
         super().__init__()
         self.config = config
-        self.fold_name = fold_name
 
         # Default keys to load/skip for pretraining
-        # Subclasses can modify these sets by adding elements to it or replacing them
+        # Subclasses can modify these sets by adding elements to the list or replacing them
         self.load_keys_pattern = {"model."}  # This corresponds to the name of the attribute in the lightning class
         self.skip_keys_pattern = {"segmentation_head", "classification_head", "heads.heads"}
 
-        # Check for every model once whether the input is properly L1 normalized
-        self._normalization_handle = self.register_forward_pre_hook(self._normalization_check)
-
     def __post__init__(self):
         if self.config["model/pretrained_model"]:
             self._load_pretrained_model()
 
-        # We initialize temperature scaling only after the pretrained model may be loaded because that could change the fold name
-        if self.config.get("post_processing/calibration/temperature", None) is not None:
-            factors = self.config["post_processing/calibration/temperature/scaling"]
-            biases = self.config["post_processing/calibration/temperature/bias"]
-            if self.fold_name not in factors or self.fold_name not in biases:
-                settings.log.warning(
-                    "Found temperature scaling parameters in the config but not for the requested fold"
-                    f" {self.fold_name} (temperature scaling will not be applied)"
-                )
-            else:
-                self.register_buffer("_temp_factor", torch.tensor(factors[self.fold_name]), persistent=False)
-                self.register_buffer("_temp_bias", torch.tensor(biases[self.fold_name]), persistent=False)
-                if (
-                    nll_prior := self.config.get("post_processing/calibration/temperature/nll_prior", default=None)
-                ) is not None:
-                    self.register_buffer("_nll_prior", torch.tensor(nll_prior), persistent=False)
-                self.register_forward_hook(self._temperature_scaling)
-
-    def _temperature_scaling(
-        self, module: nn.Module, input: tuple, output: Union[torch.Tensor, dict[str, torch.Tensor]]
-    ) -> Union[torch.Tensor, dict[str, torch.Tensor]]:
-        if type(output) == dict:
-            logits = output["class"]
-        else:
-            logits = output
-
-        # Actual scaling (based on: https://github.com/luferrer/psr-calibration/blob/master/psrcal/calibration.py)
-        scores = logits * self._temp_factor + self._temp_bias
-        if self._nll_prior is not None:
-            scores = scores - self._nll_prior  # - because nll = -log(prior)
-
-        scores = scores - torch.logsumexp(scores, axis=-1, keepdim=True)
-
-        if type(output) == dict:
-            output["class"] = scores
-        else:
-            output = scores
-
-        return output
+    def _load_pretrained_model(self) -> None:
+        config_pretrained = self.config["model/pretrained_model"]
 
-    def _normalization_check(self, module: nn.Module, module_in: tuple) -> None:
-        if self.config["input/n_channels"] == 100 and (
-            self.config["input/preprocessing"] == "L1" or self.config["input/normalization"] == "L1"
-        ):
-            features = module_in[0]
-
-            # Find the channel dimension
-            channel_dim = None
-            for dim, length in enumerate(features.shape):
-                if length == self.config["input/n_channels"]:
-                    channel_dim = dim
+        # Find training run
+        if "path" in config_pretrained:
+            possible_directories = [settings.training_dir / config_pretrained["path"], config_pretrained["path"]]
+            pretrained_dir = None
+
+            for d in possible_directories:
+                if d.exists():
+                    pretrained_dir = d
                     break
 
-            # It is possible that we cannot find the channel dimensions, e.g. for the pixel model if an input != 100 is passed to the model
-            if channel_dim is not None and not torch.allclose(
-                features.abs().sum(dim=channel_dim), torch.tensor(1.0, device=features.device), atol=0.1
-            ):
-                settings.log.warning(
-                    f"The model {module.__class__.__name__} expects L1 normalized input but the features"
-                    f" ({features.shape = }) do not seem to be L1 normalized:\naverage per pixel ="
-                    f" {features.abs().sum(dim=channel_dim).mean()}\nstandard deviation per pixel ="
-                    f" {features.abs().sum(dim=channel_dim).std()}\nThis check is only performed for the first batch."
+            if pretrained_dir is None:
+                raise ValueError(
+                    f"Could not find the pretrained model. Tried the following locations: {possible_directories}"
                 )
-
-        # We only perform this check for the first batch
-        self._normalization_handle.remove()
-
-    def _load_pretrained_model(self) -> None:
-        model_path = None
-        pretrained_dir = None
-        map_location = None if torch.cuda.is_available() else "cpu"
-
-        if self.config["model/pretrained_model/path"]:
-            possible_locations = HTCModel._get_possible_locations(Path(self.config["model/pretrained_model/path"]))
-            for location in possible_locations:
-                if location.is_dir():
-                    pretrained_dir = location
-                elif location.is_file():
-                    model_path = location
-                    break
         else:
-            pretrained_dir = HTCModel.find_pretrained_run(
-                self.config["model/pretrained_model/model"],
-                self.config["model/pretrained_model/run_folder"],
+            assert "model" in config_pretrained and "run_folder" in config_pretrained, (
+                "Please specify the model, run_folder and fold_name in your config (as subkeys from"
+                f" model/pretrained_model). Given options: {config_pretrained}"
             )
-        assert (
-            pretrained_dir is not None or model_path is not None
-        ), f"Could not find the pretrained model as specified in the config: {self.config['model/pretrained_model']}"
 
-        if model_path is None:
-            if self.config["model/pretrained_model/fold_name"]:
-                if pretrained_dir.name.startswith("fold"):
-                    assert pretrained_dir.name == self.config["model/pretrained_model/fold_name"], (
-                        f"The found pretrained directory {pretrained_dir} does not match the fold name"
-                        f" {self.config['model/pretrained_model/fold_name']}"
-                    )
-                else:
-                    pretrained_dir = pretrained_dir / self.config["model/pretrained_model/fold_name"]
+            pretrained_dir = HTCModel.find_pretrained_run(config_pretrained["model"], config_pretrained["run_folder"])
+            if "fold_name" in config_pretrained and config_pretrained["fold_name"] is not None:
+                pretrained_dir /= config_pretrained["fold_name"]
+
+        # Choose the model with the highest dice checkpoint
+        highest_metric = 0
+        pretrained_model = None
+        model_path = None
+        checkpoint_paths = sorted(pretrained_dir.rglob("*.ckpt"))
+        map_location = None if torch.cuda.is_available() else "cpu"
 
-            # Choose the model with the highest dice checkpoint
-            checkpoint_paths = sorted(pretrained_dir.rglob("*.ckpt"))
-            if len(checkpoint_paths) == 1:
-                model_path = checkpoint_paths[0]
+        for checkpoint_path in checkpoint_paths:
+            match = re.search(r"dice_metric=(\d+\.\d+)", checkpoint_path.name)
+            if match is not None:
+                current_metric = float(match.group(1))
+                if highest_metric < current_metric:
+                    highest_metric = current_metric
+                    model_path = checkpoint_path
             else:
-                table_path = pretrained_dir / "validation_table.pkl.xz"
-                if table_path.exists():
-                    # Best model based on the validation table
-                    df_val = pd.read_pickle(table_path)
-                    df_val = df_val.query("epoch_index == best_epoch_index and dataset_index == 0")
-
-                    # Best model per fold
-                    config = Config(pretrained_dir / "config.json")
-                    agg = MetricAggregation(df_val, config=config)
-                    df_best = agg.grouped_metrics(domains=["fold_name", "epoch_index"])
-                    df_best = df_best.groupby(["fold_name", "epoch_index"], as_index=False)["dice_metric"].mean()
-                    df_best = df_best.sort_values(by=agg.metrics, ascending=False, ignore_index=True)
-
-                    fold_dir = pretrained_dir / df_best.iloc[0].fold_name
-                    checkpoint_paths = sorted(fold_dir.rglob("*.ckpt"))
-                    if len(checkpoint_paths) == 1:
-                        model_path = checkpoint_paths[0]
-                    else:
-                        checkpoint_paths = sorted(fold_dir.rglob(f"epoch={df_best.iloc[0].epoch_index}*.ckpt"))
-                        assert len(checkpoint_paths) == 1, (
-                            f"More than one checkpoint found for the epoch {df_best.iloc[0].epoch_index}:"
-                            f" {checkpoint_paths}"
-                        )
-                        model_path = checkpoint_paths[0]
-                else:
-                    model_path = checkpoint_paths[0]
-                    settings.log.warning(
-                        f"Could not find the validation table at {table_path} but this is required to automatically"
-                        f" determine the best model. The first found checkpoint will be used instead: {model_path}"
-                    )
-
-        assert model_path is not None, "Could not find the best model"
-        pretrained_model = torch.load(model_path, map_location=map_location)
+                current_model = torch.load(checkpoint_path, map_location=map_location)
+                current_metric = [
+                    v["best_model_score"].item()
+                    for k, v in current_model["callbacks"].items()
+                    if "ModelCheckpoint" in k
+                ][0]
+                if highest_metric < current_metric:
+                    highest_metric = current_metric
+                    pretrained_model = current_model
+
+        if pretrained_model is None:
+            assert model_path is not None, "Could not find the best model"
+            pretrained_model = torch.load(model_path, map_location=map_location)
 
         # Change state dict keys
         model_dict = self.state_dict()
         num_keys_loaded = 0
         skipped_keys = []
         for k in pretrained_model["state_dict"].keys():
-            if any(skip_key_pattern in k for skip_key_pattern in self.skip_keys_pattern):
+            if any([skip_key_pattern in k for skip_key_pattern in self.skip_keys_pattern]):
                 skipped_keys.append(k)
                 continue
 
             for load_key_pattern in self.load_keys_pattern:
                 if load_key_pattern in k:
                     # If the input channels are different then use the same trick as used in segmentation_models library
                     # e.g. in case of 3 channels new_weight[:, i] = pretrained_weight[:, i % 3]
@@ -294,17 +204,14 @@
                         model_dict[k.replace(load_key_pattern, "")] = (
                             model_dict[k.replace(load_key_pattern, "")] * pretrained_in_channel
                         ) / new_in_channel
                     else:
                         model_dict[k.replace(load_key_pattern, "")] = pretrained_model["state_dict"][k]
                     num_keys_loaded += 1
 
-        if self.fold_name is None:
-            self.fold_name = model_path.parent.name
-
         # Load the new weights
         self.load_state_dict(model_dict)
         if num_keys_loaded == 0:
             settings.log.warning(f"No key has been loaded from the pretrained dir: {pretrained_dir}")
         elif num_keys_loaded + len(skipped_keys) != len(model_dict):
             settings.log.warning(
                 f"{num_keys_loaded} keys were changed in the model ({len(skipped_keys)} keys were skipped:"
@@ -315,35 +222,32 @@
             settings.log.info(
                 f"Successfully loaded the pretrained model ({len(skipped_keys)} keys were skipped: {skipped_keys})."
             )
 
     @classmethod
     def pretrained_model(
         cls,
-        model: str = None,
-        run_folder: str = None,
-        path: Union[str, Path] = None,
+        model: str,
+        run_folder: str,
         fold_name: str = None,
         n_classes: int = None,
         n_channels: int = None,
         pretrained_weights: bool = True,
-        **model_kwargs,
-    ) -> Self:
+    ) -> "HTCModel":
         """
         Load a pretrained segmentation model.
 
         You can directly use this model to train a network on your data. The weights will be initialized with the weights from the pretrained network, except for the segmentation head which is initialized randomly (and may also be different in terms of number of classes, depending on your data). The returned instance corresponds to the calling class (e.g. `ModelImage`) and you can also find it in the third column of the pretrained models table (cf. readme).
 
         For example, load the pretrained model for the image-based segmentation network:
-        >>> from htc import ModelImage, Normalization
+        >>> from htc import ModelImage
         >>> run_folder = "2022-02-03_22-58-44_generated_default_model_comparison"  # HSI model
         >>> print("some log messages"); model = ModelImage.pretrained_model(model="image", run_folder=run_folder)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(1, 100, 480, 640)  # NCHW
-        >>> input_data = Normalization(channel_dim=1)(input_data)  # Model expects L1 normalized input
         >>> model(input_data).shape
         torch.Size([1, 19, 480, 640])
 
         It is also possible to have a different number of classes as output or a different number of channels as input:
         >>> print("some log messages"); model = ModelImage.pretrained_model(model="image", run_folder=run_folder, n_classes=3, n_channels=10)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(1, 10, 480, 640)  # NCHW
@@ -351,187 +255,136 @@
         torch.Size([1, 3, 480, 640])
 
         The patch-based models also use the `ModelImage` class but with a different input (here using the patch_64 model):
         >>> run_folder = "2022-02-03_22-58-44_generated_default_64_model_comparison"  # HSI model
         >>> print("some log messages"); model = ModelImage.pretrained_model(model="patch", run_folder=run_folder)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(1, 100, 64, 64)  # NCHW
-        >>> input_data = Normalization(channel_dim=1)(input_data)  # Model expects L1 normalized input
         >>> model(input_data).shape
         torch.Size([1, 19, 64, 64])
 
         The procedure is the same for the superpixel-based segmentation network but this time also using a different calling class (`ModelSuperpixelClassification`):
         >>> from htc import ModelSuperpixelClassification
         >>> run_folder = "2022-02-03_22-58-44_generated_default_model_comparison"  # HSI model
         >>> print("some log messages"); model = ModelSuperpixelClassification.pretrained_model(model="superpixel_classification", run_folder=run_folder)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(2, 100, 32, 32)  # NCHW
-        >>> input_data = Normalization(channel_dim=1)(input_data)  # Model expects L1 normalized input
         >>> model(input_data).shape
         torch.Size([2, 19])
 
         And also the pixel network:
         >>> from htc import ModelPixel
         >>> run_folder = "2022-02-03_22-58-44_generated_default_model_comparison"  # HSI model
         >>> print("some log messages"); model = ModelPixel.pretrained_model(model="pixel", run_folder=run_folder)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(2, 100)  # NC
-        >>> input_data = Normalization(channel_dim=1)(input_data)  # Model expects L1 normalized input
         >>> model(input_data)['class'].shape
         torch.Size([2, 19])
 
         For the pixel model, you can specify a different number of classes but you do not need to set the number of input channels because the underlying convolutional operations directly operate along the channel dimension. Hence, you can just supply input data with a different number of channels and it will work as well.
         >>> print("some log messages"); model = ModelPixel.pretrained_model(model="pixel", run_folder=run_folder, n_classes=3)  # doctest: +ELLIPSIS
         some log messages...
         >>> input_data = torch.randn(2, 90)  # NC
         >>> model(input_data)['class'].shape
         torch.Size([2, 3])
 
         Args:
             model: Basic model type like image or pixel (first column in the pretrained models table). This corresponds to the folder name in the first hierarchy level of the training directory.
             run_folder: Name of the training run from which the weights should be loaded, e.g. to select HSI or RGB models (fourth column in the pretrained models table). This corresponds to the folder name in the second hierarchy level of the training directory.
-            path: Alternatively of specifying the model and run folder, you can also specify the path to the run directory, the fold directory or the path to the checkpoint file (*.ckpt) directly.
             fold_name: Name of the validation fold which defines the trained network of the run. If None, the model with the highest metric score will be used.
             n_classes: Number of classes for the network output. If None, uses the same setting as in the trained network (e.g. 18 organ classes + background for the organ segmentation networks).
             n_channels: Number of channels of the input. If None, uses the same settings as in the trained network (e.g. 100 channels). This is inspired by the timm library (https://timm.fast.ai/models#How-is-timm-able-to-use-pretrained-weights-and-handle-images-that-are-not-3-channel-RGB-images?), i.e. it repeats the weights according to the desired number of channels. Please not that this does not take any semantic of the input into account, e.g. the wavelength range or the filter functions of the camera.
             pretrained_weights: If True, overwrite the weights of the model with the weights from the pretrained model, i.e. make use of the pretrained model. If False, will still load (and download) the model but keep the weights randomly initialized. This mainly ensures that the same config is used for the pretrained model.
-            model_kwargs: Additional keyword arguments passed to the model instance.
 
         Returns: Instance of the calling model class initialized with the pretrained weights. The model object will be an instance of `torch.nn.Module`.
         """
-        run_dir = HTCModel.find_pretrained_run(model, run_folder, path)
+        run_dir = HTCModel.find_pretrained_run(model, run_folder)
         config = Config(run_dir / "config.json")
-
         if pretrained_weights:
-            if path is not None:
-                config["model/pretrained_model/path"] = path
-            else:
-                config["model/pretrained_model/model"] = model
-                config["model/pretrained_model/run_folder"] = run_folder
-
+            config["model/pretrained_model/model"] = model
+            config["model/pretrained_model/run_folder"] = run_folder
         if fold_name is not None:
             config["model/pretrained_model/fold_name"] = fold_name
         if n_classes is not None:
             config["input/n_classes"] = n_classes
         if n_channels is not None:
             assert model != "pixel", (
                 "The parameter n_channels cannot be used with the pixel model. The number of channels are solely"
                 " determined by the input (see examples)"
             )
             config["input/n_channels"] = n_channels
 
-        return cls(config, **model_kwargs)
+        return cls(config)
 
     @staticmethod
-    def find_pretrained_run(model_name: str = None, run_folder: str = None, path: Union[str, Path] = None) -> Path:
+    def find_pretrained_run(model_name: str, run_folder: str) -> Path:
         """
-        Searches for a pretrained run either in the local results directory, in the local PyTorch model cache directory or it will attempt to download the model. For the local results directory, the following folders are searched:
-        - `results/training/<model_name>/<run_folder>`
-        - `results/pretrained_models/<model_name>/<run_folder>`
-        - `results/<model_name>/<run_folder>`
-        - `<model_name>/<run_folder>` (relative/absolute path)
+        Searches for a pretrained run either in the local training directory, in the local PyTorch model cache directory or it will attempt to download the model.
 
         Args:
             model_name: Basic model type like image or pixel.
-            run_folder: Name of the training run directory (e.g. 2022-02-03_22-58-44_generated_default_model_comparison).
-            path: Alternatively to model_name and run_folder, you can also specify the path to the run directory (may also be relative to the results directory in one of the folders from above). If the path points to the fold directory or the checkpoint file (*.ckpt), the corresponding run directory will be returned.
+            run_folder: Name of the training run directory.
 
-        Returns: Path to the requested training run (run directory usually starting with a timestamp).
+        Returns: Path to the requested training run.
         """
-        if path is not None:
-            if type(path) is str:
-                path = Path(path)
-
-            possible_locations = HTCModel._get_possible_locations(path)
-            for location in possible_locations:
-                if location.is_dir():
-                    if location.name.startswith("fold"):
-                        # At this point, we are only interested in the run directory and not the fold directory
-                        location = location.parent
-
-                    if model_name is not None:
-                        assert (
-                            location.parent.name == model_name
-                        ), f"The found location {location} does not match the given model_name {model_name}"
-                    if run_folder is not None:
-                        assert (
-                            location.name == run_folder
-                        ), f"The found location {location} does not match the given run_folder {run_folder}"
-
-                    return location
-                elif location.is_file():
-                    # From the checkpoint file to the run directory
-                    return location.parent.parent
-
-            raise ValueError(
-                f"Could not find the pretrained model. Tried the following locations: {possible_locations}"
-            )
-        else:
-            assert path is None, "The path parameter is not used if model_name and run_folder are specified"
-            assert model_name is not None and run_folder is not None, (
-                "Please specify model_name and run_folder (e.g. in your config via the keys"
-                " `model/pretrained_model/model` and `model/pretrained_model/run_folder`) if no path is given"
-            )
-
-            # Option 1: local results directory
-            if settings.results_dir is not None:
-                possible_locations = HTCModel._get_possible_locations(Path(model_name) / run_folder)
-                for run_dir in possible_locations:
-                    if run_dir.is_dir():
-                        settings.log_once.info(f"Found pretrained run in the local results dir at {run_dir}")
-                        return run_dir
-
-            # Option 2: local hub dir (cache folder)
-            hub_dir = Path(torch.hub.get_dir()) / "htc_checkpoints"
-            run_dir = hub_dir / model_name / run_folder
+        # Option 1: local training directory
+        if settings.training_dir is not None:
+            run_dir = settings.training_dir / model_name / run_folder
             if run_dir.is_dir():
-                settings.log_once.info(f"Found pretrained run in the local hub dir at {run_dir}")
+                settings.log_once.info(f"Found pretrained run in the local training dir at {run_dir}")
                 return run_dir
 
-            # Option 3: download the model to the local hub dir
-            name = f"{model_name}@{run_folder}"
-            assert (
-                name in HTCModel.known_models
-            ), f"Could not find the training run for {model_name}/{run_folder} (neither locally nor as download option)"
-            model_info = HTCModel.known_models[name]
-
-            hub_dir.mkdir(parents=True, exist_ok=True)
-
-            # Download the archive containing all trained models for the run (i.e. a model per fold)
-            zip_path = hub_dir / f"{name}.zip"
-            settings.log.info(f"Downloading pretrained model {name} since it is not locally available")
-            torch.hub.download_url_to_file(model_info["url"], zip_path)
-
-            # Extract the archive in the models dir with the usual structure (e.g. image/run_folder/fold_name)
-            with ZipFile(zip_path) as f:
-                f.extractall(hub_dir)
-            zip_path.unlink()
-
-            assert run_dir.is_dir(), "run folder not available even after download"
-
-            # Check file contents to catch download errors
-            hash_cat = ""
-            for f in sorted(run_dir.rglob("*"), key=lambda x: str(x).lower()):
-                if f.is_file():
-                    hash_cat += sha256_file(f)
-
-            hash_folder = hashlib.sha256(hash_cat.encode()).hexdigest()
-            if model_info["sha256"] != hash_folder:
-                settings.log.error(
-                    f"The hash of the folder (hash of the file hashes, {hash_folder}) does not match the expected hash"
-                    f" ({model_info['sha256']}). The download of the model was likely not successful. The downloaded"
-                    f" files are not deleted and are still available at {hub_dir}. Please check the files manually"
-                    " (e.g. for invalid file sizes). If you want to re-trigger the download process, just delete the"
-                    f" corresponding run directory {run_dir}"
-                )
-            else:
-                settings.log.info(f"Successfully downloaded the pretrained run to the local hub dir at {run_dir}")
-
+        # Option 2: local hub dir (cache folder)
+        hub_dir = Path(torch.hub.get_dir()) / "htc_checkpoints"
+        run_dir = hub_dir / model_name / run_folder
+        if run_dir.is_dir():
+            settings.log_once.info(f"Found pretrained run in the local hub dir at {run_dir}")
             return run_dir
 
+        # Option 3: download the model to the local hub dir
+        name = f"{model_name}@{run_folder}"
+        assert (
+            name in HTCModel.known_models
+        ), f"Could not find the training run for {model_name}/{run_folder} (neither locally nor as download option)"
+        model_info = HTCModel.known_models[name]
+
+        hub_dir.mkdir(parents=True, exist_ok=True)
+
+        # Download the archive containing all trained models for the run (i.e. a model per fold)
+        zip_path = hub_dir / f"{name}.zip"
+        settings.log.info(f"Downloading pretrained model {name} since it is not locally available")
+        torch.hub.download_url_to_file(model_info["url"], zip_path)
+
+        # Extract the archive in the models dir with the usual structure (e.g. image/run_folder/fold_name)
+        with ZipFile(zip_path) as f:
+            f.extractall(hub_dir)
+        zip_path.unlink()
+
+        assert run_dir.is_dir(), "run folder not available even after download"
+
+        # Check file contents to catch download errors
+        hash_cat = ""
+        for f in sorted(run_dir.rglob("*"), key=lambda x: str(x).lower()):
+            if f.is_file():
+                hash_cat += sha256_file(f)
+
+        hash_folder = hashlib.sha256(hash_cat.encode()).hexdigest()
+        if model_info["sha256"] != hash_folder:
+            settings.log.error(
+                f"The hash of the folder (hash of the file hashes, {hash_folder}) does not match the expected hash"
+                f" ({model_info['sha256']}). The download of the model was likely not successful. The downloaded files"
+                f" are not deleted and are still available at {hub_dir}. Please check the files manually (e.g. for"
+                " invalid file sizes). If you want to re-trigger the download process, just delete the corresponding"
+                f" run directory {run_dir}"
+            )
+        else:
+            settings.log.info(f"Successfully downloaded the pretrained run to the local hub dir at {run_dir}")
+
+        return run_dir
+
     @staticmethod
     def markdown_table() -> str:
         """
         Generate a markdown table with all known pretrained models (used in the README).
 
         If you want to update the table in the README, simple call this function on the command line:
         ```bash
@@ -574,16 +427,7 @@
             model_lines.append(
                 f"| {model_type} | {model_info['model_type']} | [`{class_name}`](./{class_path}) |"
                 f" [{run_folder}]({download_info['url']}) |"
             )
 
         table_lines += reversed(model_lines)
         return "\n".join(table_lines)
-
-    @staticmethod
-    def _get_possible_locations(path: Path) -> list[Path]:
-        return [
-            settings.training_dir / path,
-            settings.results_dir / path,
-            settings.results_dir / "pretrained_models" / path,
-            path,
-        ]
```

## htc/models/common/HierarchicalSampler.py

```diff
@@ -3,111 +3,72 @@
 
 import math
 from collections.abc import Iterator
 
 import torch
 from torch.utils.data import Sampler
 
-from htc.cpp import hierarchical_bootstrapping, hierarchical_bootstrapping_labels
+from htc.cpp import hierarchical_bootstrapping
 from htc.models.common.utils import adjust_epoch_size
 from htc.models.data.DataSpecification import DataSpecification
-from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 from htc.utils.DomainMapper import DomainMapper
-from htc.utils.LabelMapping import LabelMapping
 
 
 class HierarchicalSampler(Sampler[int]):
-    def __init__(self, paths: list[DataPath], config: Config, batch_size: int = None):
+    def __init__(self, paths: list[DataPath], config: Config):
         """
-        Sampler which generates random batches and takes the hierarchical structure of the data into account, especially the domain. It is designed for the camera problem and ensures that in each batch every camera (domain) is present. The remaining batch size is filled by sampling multiple subjects with replacement (and one image per subject). The sampler can be passed to the Dataloader (sampler argument).
-
-        You must specify `input/target_domain` in your config (e.g. `config["input/target_domain"] = ["camera_index"]`) to define the domain used in the hierarchical sampling (if more than one domain is specified, only the first will be used).
+        Sampler which generates random batches and takes the hierarchical structure of the data into account. It is designed for the camera problem and ensures that in each batch every camera is present. The remaining batch size is filled by sampling multiple subjects with replacement (and one image per subject). The sampler can be passed to the Dataloader (sampler argument).
 
         Args:
             paths: List of paths from which samples are generated.
             config: The configuration of the current run.
-            batch_size: The batch size, i.e. the number of images per iteration. If `None`, `dataloader_kwargs/batch_size` from the config will be used which is recommended for image-based datasets. For other stream-based datasets (e.g. patch dataset), the situation is more difficult since the batch size does not correspond simply to the number of images. In this case, you can specify the value manually, e.g. to `dataloader_kwargs/num_workers` to make sure that the workers operate on the images given by this sampler.
         """
         self.config = config
-        self.batch_size = batch_size or config["dataloader_kwargs/batch_size"]
-
-        target_domain = self.config["input/target_domain"]
-        assert (
-            target_domain is not None and len(target_domain) >= 1
-        ), "At least one target domain must be specified in the config (input/target_domain)"
-        if len(target_domain) > 1:
-            settings.log.info(
-                f"More than one target domain specified. Only the first one ({target_domain[0]}) will be used for"
-                " hierarchical sampling"
-            )
 
-        domain_mapper = DomainMapper(DataSpecification.from_config(self.config), target_domain=target_domain[0])
+        cam_mapper = DomainMapper(DataSpecification.from_config(self.config), target_domain="camera_index")
         subject_mapper = DomainMapper(DataSpecification.from_config(self.config), target_domain="subject_index")
 
-        if self.config["input/hierarchical_sampling"] == "labels":
-            self.label_mapping = {}
-            mapping = LabelMapping.from_config(self.config)
-        else:
-            assert self.config.get(
-                "input/hierarchical_sampling", True
-            ), "At the moment, only True or labels can be used for `input/hierarchical_sampling`"
-            self.label_mapping = None
-
-        self.domain_mapping = {}
+        self.mapping = {}
         for image_index, path in enumerate(paths):
-            domain_index = domain_mapper.domain_index(path.image_name())
+            camera_index = cam_mapper.domain_index(path.image_name())
             subject_index = subject_mapper.domain_index(path.image_name())
 
-            if domain_index not in self.domain_mapping:
-                self.domain_mapping[domain_index] = {}
-
-            if subject_index not in self.domain_mapping[domain_index]:
-                self.domain_mapping[domain_index][subject_index] = []
+            if camera_index not in self.mapping:
+                self.mapping[camera_index] = {}
 
-            self.domain_mapping[domain_index][subject_index].append(image_index)
+            if subject_index not in self.mapping[camera_index]:
+                self.mapping[camera_index][subject_index] = []
 
-            if self.config["input/hierarchical_sampling"] == "labels":
-                for label in path.annotated_labels():
-                    label_index = mapping.name_to_index(label)
-                    if not mapping.is_index_valid(label_index):
-                        continue
+            self.mapping[camera_index][subject_index].append(image_index)
 
-                    if label_index not in self.label_mapping:
-                        self.label_mapping[label_index] = {}
-
-                    if subject_index not in self.label_mapping[label_index]:
-                        self.label_mapping[label_index][subject_index] = []
-
-                    self.label_mapping[label_index][subject_index].append(image_index)
-
-        assert self.config["dataloader_kwargs/batch_size"] >= len(self.domain_mapping), (
-            f'The batch size ({self.config["dataloader_kwargs/batch_size"]}) must be >= the number of domains'
-            f" ({len(self.domain_mapping)}) in the training set"
+        assert self.config["dataloader_kwargs/batch_size"] >= len(self.mapping), (
+            f'The batch size ({self.config["dataloader_kwargs/batch_size"]}) must be >= the number of cameras'
+            f" ({len(self.mapping)}) in the training set"
         )
 
     def __iter__(self) -> Iterator[int]:
-        n_subjects = math.ceil(self.batch_size / len(self.domain_mapping))
-        n_batches = len(self) // self.batch_size
+        n_subjects = math.ceil(self.config["dataloader_kwargs/batch_size"] / len(self.mapping))
+        n_batches = len(self) // self.config["dataloader_kwargs/batch_size"]
+        sample_indices = hierarchical_bootstrapping(
+            self.mapping, n_subjects=n_subjects, n_images=1, n_bootstraps=n_batches
+        )
 
-        if self.label_mapping is None:
-            sample_indices = hierarchical_bootstrapping(
-                self.domain_mapping, n_subjects=n_subjects, n_images=1, n_bootstraps=n_batches
-            )
+        if self.config["dataloader_kwargs/batch_size"] % len(self.mapping) != 0:
+            for row in range(sample_indices.size(0)):
+                selection = torch.randperm(sample_indices.size(1))[: self.config["dataloader_kwargs/batch_size"]]
+                assert (
+                    selection.size(0) == self.config["dataloader_kwargs/batch_size"]
+                ), f"Number of batch indices ({selection.size(0)}) does not much the batch size"
+                yield from sample_indices[row, selection].tolist()
         else:
-            sample_indices = hierarchical_bootstrapping_labels(
-                self.domain_mapping, self.label_mapping, n_labels=n_subjects, n_bootstraps=n_batches
-            )
-
-        assert (
-            sample_indices.size(1) >= self.batch_size
-        ), f"Number of batch indices ({sample_indices.size(1)}) does not much the batch size"
-        for row in range(sample_indices.size(0)):
-            # The order of images in a batch should never matter, so we make a random selection per batch
-            selection = torch.randperm(sample_indices.size(1))[: self.batch_size]
-            yield from sample_indices[row, selection].tolist()
+            assert (
+                sample_indices.size(1) == self.config["dataloader_kwargs/batch_size"]
+            ), f"Number of batch indices ({sample_indices.size(1)}) does not much the batch size"
+            for row in range(sample_indices.size(0)):
+                yield from sample_indices[row, :].tolist()
 
     def __len__(self) -> int:
         # We adjust the epoch_size here and not in the init since the epoch_size might not be converted from str to int yet (e.g. for patch dataset)
         adjust_epoch_size(self.config)
         return self.config["input/epoch_size"]
```

## htc/models/common/MetricAggregation.py

```diff
@@ -21,38 +21,38 @@
         Class for calculating metrics for validation while respecting the hierarchical structure of the data. The metrics include dice metric for checkpointing and domain accuracy for domain adaptation problems.
 
         Args:
             path_or_df: The path or dataframe of the validation/test table.
             config: The Config class object for configs of current run.
             metrics: List containing the column names of metric columns.
         """
-        self.config = config if config is not None else Config({})
         if metrics is None:
-            metrics = [self.config.get("validation/checkpoint_metric", "dice_metric")]
+            metrics = ["dice_metric"]
         self.metrics = metrics
+        self.config = config if config is not None else Config({})
 
         if isinstance(path_or_df, pd.DataFrame):
             self.df = path_or_df
         elif isinstance(path_or_df, Path):
             self.df = pd.read_pickle(path_or_df)
         else:
             raise ValueError("Neither a dataframe nor path given")
 
-        assert all(m in self.df for m in self.metrics), f"Not all metrics are in the dataframe ({self.df.columns})"
+        assert all(m in self.df for m in self.metrics), "Not all metrics are in the dataframe"
 
         if ("subject_name" not in self.df or "timestamp" not in self.df) and "image_name" in self.df:
             # Reconstruct missing information
             df_meta = pd.DataFrame(
                 [DataPath.from_image_name(name).image_name_typed() for name in self.df["image_name"]]
             )
             self.df = pd.concat([self.df.reset_index(drop=True), df_meta], axis=1)
             assert len(df_meta) == len(self.df), "The length of the dataframe should not change"
 
         assert all(
-            c in self.df.columns for c in (["subject_name", "timestamp"])
+            [c in self.df.columns for c in (["subject_name", "timestamp"])]
         ), "The dataframe misses some of the required columns"
 
     def checkpoint_metric(self, domains: Union[str, list[str], bool] = None, mode: str = None) -> float:
         """
         Calculates a metric value for checkpointing optionally utilizing one or more domains. Depending on the 'validation/checkpoint_metric_mode' in the config, image or class scores are obtained. Aggregation is always performed along the hierarchy (image, subject, domains).
 
         Args:
@@ -61,15 +61,15 @@
                 - If None, it uses the 'input/target_domain' value from the config.
                 - If False, domain columns are ignored.
             mode: Aggregation mode. Either `class_level` (one metric value per label) or `image_level` (multiple classes in an image are aggregated first to get a metric value for one image). If None, the value from the config (`validation/checkpoint_metric_mode`) is used. Defaults to `class_level`.
 
         Returns: Metric values which can be used for checkpointing.
         """
         assert all(
-            c in self.df.columns for c in ["used_labels"] + self.metrics
+            [c in self.df.columns for c in ["used_labels"] + self.metrics]
         ), "The dataframe misses some of the required columns"
 
         domains = self._domain_defaults(domains)
         if mode is None:
             mode = self.config.get("validation/checkpoint_metric_mode", "class_level")
 
         if mode == "class_level":
@@ -91,89 +91,78 @@
 
         return df_g[self.metrics].mean().mean()
 
     def grouped_metrics(
         self,
         domains: Union[str, list[str], bool] = None,
         keep_subjects: bool = False,
-        no_aggregation: bool = False,
         mode: str = None,
         dataset_index: Union[int, None] = 0,
         best_epoch_only: bool = True,
     ) -> pd.DataFrame:
         """
         Calculates a table with metric values for each label and (optionally) the corresponding domains. The scores are first aggregated per subject and then across subjects.
 
         Args:
             domains: The domains to consider additionally, i.e. the columns which should be kept in the output.
                 - If string or list, the corresponding domains will occur as columns in the result table (e.g. camera_index).
                 - If None, it uses the 'input/target_domain' value from the config (if the value does not exist in the config, domain columns will be ignored).
                 - If False, domain columns are ignored.
             keep_subjects: If True, metrics are only aggregated across images but not across subjects and a subject column will remain in the output table.
-            no_aggregation: If True, no aggregation is performed and the output table will contain one row per image. This is useful if results should be visualized per organ but you still want to show the distribution across subjects.
             mode: Aggregation mode. Either `class_level` (one metric value per label) or `image_level` (multiple classes in an image are aggregated first to get a metric value for one image). If None, the value from the config (`validation/checkpoint_metric_mode`) is used. Defaults to `class_level`.
             dataset_index: The index of the dataset which is selected in the table (if available). If None, no selection is carried out.
-            best_epoch_only: If True, only results from the best epoch are considered (if available). If False, no selection is carried out and you will get aggregated results per epoch_index (which will also be a column in the resulting table).
+            best_epoch_only: If True, only results from the best epoch are considered (if available). If False, no selection is carried out.
 
         Returns: Table with metric values.
         """
-        assert all(c in self.df.columns for c in self.metrics), "The dataframe misses some of the required columns"
+        assert all(
+            [c in self.df.columns for c in ["used_labels"] + self.metrics]
+        ), "The dataframe misses some of the required columns"
 
         df = self.df
-        domains = self._domain_defaults(domains)
-
         if dataset_index is not None and "dataset_index" in df:
             df = df[df["dataset_index"] == dataset_index]
         if best_epoch_only and "epoch_index" in df and "best_epoch_index" in df:
             df = df[df["epoch_index"] == df["best_epoch_index"]]
 
-        if not best_epoch_only and "epoch_index" not in domains:
-            domains.append("epoch_index")
-
+        domains = self._domain_defaults(domains)
         if mode is None:
             mode = self.config.get("validation/checkpoint_metric_mode", "class_level")
 
         if "confusion_matrix" in self.metrics:
             assert mode == "image_level", "The confusion matrix can only be used with the image level"
             df_g = self.grouped_cm(
                 domains,
                 dataset_index,
                 best_epoch_only,
                 additional_metrics=[m for m in self.metrics if m != "confusion_matrix"],
             )
         else:
             if mode == "class_level":
-                assert (
-                    "used_labels" in self.df.columns
-                ), "used_labels columns is required for class-level aggregation mode"
                 df_g = df.explode(self.metrics + ["used_labels"])
+                df_g = df_g.groupby(domains + ["subject_name", "used_labels"], as_index=False)[self.metrics].agg(
+                    self._default_aggregator
+                )
 
-                if not no_aggregation:
-                    df_g = df_g.groupby(domains + ["subject_name", "used_labels"], as_index=False)[self.metrics].agg(
+                if not keep_subjects:
+                    df_g = df_g.groupby(domains + ["used_labels"], as_index=False)[self.metrics].agg(
                         self._default_aggregator
                     )
-
-                    if not keep_subjects:
-                        df_g = df_g.groupby(domains + ["used_labels"], as_index=False)[self.metrics].agg(
-                            self._default_aggregator
-                        )
             elif mode == "image_level":
                 if all("image" in m for m in self.metrics):
                     df_g = df
                 else:
-                    additional = ["used_labels"] if "used_labels" in self.df.columns else []
-                    df_g = df.explode(self.metrics + additional)
+                    df_g = df.explode(self.metrics + ["used_labels"])
 
-                if not no_aggregation:
-                    df_g = df_g.groupby(domains + ["subject_name", "timestamp"], as_index=False)[self.metrics].agg(
-                        self._default_aggregator
-                    )
-                    df_g = df_g.groupby(domains + ["subject_name"], as_index=False)[self.metrics].agg(
-                        self._default_aggregator
-                    )
+                df_g = df_g.groupby(domains + ["subject_name", "timestamp"], as_index=False)[self.metrics].agg(
+                    self._default_aggregator
+                )
+                df_g = df_g.groupby(domains + ["subject_name"], as_index=False)[self.metrics].agg(
+                    self._default_aggregator
+                )
             else:
                 raise ValueError(f"Invalid mode {mode}")
 
         return self._resolve_id_columns(df_g, domains)
 
     def grouped_metrics_epochs(self, domains: Union[str, list[str], bool] = None, mode: str = None) -> pd.DataFrame:
         """
@@ -243,15 +232,15 @@
         Calculates domain prediction accuracy first aggregated per subject and then across subjects.
 
         Args:
             domain: Name of the domain (e.g. camera_index).
 
         Returns: Table with prediction accuracy for each domain value (e.g. every camera).
         """
-        assert all(c in self.df.columns for c in [f"{domain}_predicted"]), "The domain predictions are not available"
+        assert all([c in self.df.columns for c in [f"{domain}_predicted"]]), "The domain predictions are not available"
 
         df_domain = self.df.explode(["used_labels"])
 
         # There are extra rows since every label is stored as a row
         def reduce_unique(df: pd.DataFrame) -> pd.Series:
             assert (df.values == df.values[0]).all(), f"Cannot reduce the values {df} to a single value"
```

## htc/models/common/SharedMemoryDatasetMixin.py

```diff
@@ -1,16 +1,17 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
+import inspect
 from abc import abstractmethod
 from collections.abc import Iterable
 from typing import Union
 
 import torch
-from torch.utils.data import get_worker_info
+from torch.utils.data import DataLoader, get_worker_info
 from torch.utils.data.sampler import Sampler
 
 from htc.models.common.StreamDataLoader import StreamDataLoader
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 
 
@@ -24,26 +25,28 @@
         if self.sampler is None:
             size = max(len(self.paths), self.config["dataloader_kwargs/num_workers"])
         else:
             size = len(self.sampler)
         self.path_indices_worker = torch.empty(size, dtype=torch.int64).share_memory_()
 
         # Shared memory settings
-        prefetch_factor_default = 2  # Default for prefetch factor is 2 in PyTorch
+        prefetch_factor_default = (
+            inspect.signature(DataLoader).parameters["prefetch_factor"].default
+        )  # Default for prefetch factor is (usually) 2 in PyTorch
         self.prefetch_factor = self.config.get("dataloader_kwargs/prefetch_factor", prefetch_factor_default)
 
         # We always shuffle the paths in the beginning in case this class is used without the StreamDataLoader
         # For training, this means that paths are shuffled twice in the beginning (since the StreamDataLoader always shuffles paths in the beginning of each iteration)
         self.shuffle_paths()
 
     def __del__(self) -> None:
         # We should unpin the tensor memory when objects of this class get destructed
         # Event though this is not RAII in Python (https://en.wikibooks.org/wiki/Python_Programming/Context_Managers#Not_RAII)
         try:
-            if torch is not None and torch.cuda is not None:
+            if torch.cuda is not None:
                 cudart = torch.cuda.cudart()
 
                 for key, tensor in self.shared_dict.items():
                     if type(tensor) == torch.Tensor and tensor.is_pinned():
                         code = cudart.cudaHostUnregister(tensor.data_ptr())
                         assert not tensor.is_pinned(), f"Cannot unpin the tensor {key}: {code = }"
         except RuntimeError:
```

## htc/models/common/StreamDataLoader.py

```diff
@@ -150,15 +150,17 @@
         results = {}
         for i in range(self.dataloader.num_workers):
             try:
                 results[i] = next(loader_it)
             except StopIteration:
                 pass
 
-        is_batch_part = any(r["partly_filled"] for r in results.values()) or len(results) < self.dataloader.num_workers
+        is_batch_part = (
+            any([r["partly_filled"] for r in results.values()]) or len(results) < self.dataloader.num_workers
+        )
         if is_batch_part:
             used_indices = []
             for r in results.values():
                 used_indices.append(torch.arange(r["start_index"], r["end_index"], dtype=torch.int64))
             used_indices = torch.cat(used_indices)
 
         buffer_index = next(iter(results.values()))[
```

## htc/models/common/class_weights.py

```diff
@@ -4,57 +4,49 @@
 import torch
 
 from htc.models.common.utils import get_n_classes
 from htc.utils.Config import Config
 
 
 def calculate_class_weights(
-    config: Config,
-    label_indices: torch.Tensor = None,
-    label_counts: torch.Tensor = None,
-    class_weight_method: str = None,
+    config: Config, label_indices: torch.Tensor = None, label_counts: torch.Tensor = None
 ) -> torch.Tensor:
     """
     Implements different class weight calculation methods.
 
     Args:
         config: Configuration object which must contain at least a model/class_weight_method key.
         label_indices: Vector with all labels used during training.
         label_counts: Vector with corresponding counts for each label.
-        class_weight_method: Explicitly set the method for computing the class weights. If None, the method is read from the config (key `model/class_weight_method`).
 
     Returns:
         Estimated class weights normalized to a probability vector.
     """
     n_classes = get_n_classes(config)
-    weight_method = class_weight_method or config["model/class_weight_method"]
 
-    if weight_method == "1" or not weight_method:
+    if config["model/class_weight_method"] == "1" or not config["model/class_weight_method"]:
         return torch.ones(n_classes)
 
     assert (
         label_indices is not None and label_counts is not None
     ), "Cannot calculate class weights without label information"
     assert len(label_indices) == len(label_counts), "Label ids and counts must match"
 
     n_pixels = label_counts.sum()
     class_weights = torch.zeros(n_classes)
 
     # n = total counts
     # m = counts for the current label
-    if weight_method == "(n-m)n":  # ATTENTION: unicode char 2215 () used to avoid path issues
+    if config["model/class_weight_method"] == "(n-m)n":  # ATTENTION: unicode char 2215 () used to avoid path issues
         class_weights[label_indices] = (n_pixels - label_counts.float()) / n_pixels
         class_weights = class_weights / torch.sum(class_weights)
-    elif weight_method == "1m":
+    elif config["model/class_weight_method"] == "1m":
         class_weights[label_indices] = 1 / label_counts.float()
         class_weights = class_weights / torch.sum(class_weights)
-    elif weight_method == "softmin":
+    elif config["model/class_weight_method"] == "softmin":
         counts_normalized = label_counts.float() / n_pixels
         exp_counts = torch.exp(config["model/softmin_scaling"] * counts_normalized)
         class_weights[label_indices] = exp_counts / exp_counts.sum()
-    elif weight_method == "nll":
-        counts_normalized = label_counts.float() / n_pixels
-        class_weights[label_indices] = -torch.log(counts_normalized)
     else:
-        raise ValueError(f"{weight_method} is not a valid class weight calculation method")
+        raise ValueError(f'{config["model/class_weight_method"]} is not a valid class weight calculation method')
 
     return class_weights
```

## htc/models/common/distance_correlation.py

```diff
@@ -1,13 +1,14 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import numpy as np
 import pandas as pd
 import torch
+from functorch import vmap
 
 from htc.cpp import hierarchical_bootstrapping
 from htc.settings import settings
 
 
 def distance_correlation(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
     """
@@ -129,15 +130,15 @@
         n_images = 1
         n_bootstraps = 100
         bootstraps = hierarchical_bootstrapping(mapping, n_subjects, n_images, n_bootstraps).to(device)
 
         # Distance correlation for each bootstrap (chunked to reduce memory consumption)
         dcors = []
         for chunk in bootstraps.tensor_split(10):
-            dcors.append(torch.vmap(lambda indices: distance_correlation(x_all[indices], y_all[indices]))(chunk))
+            dcors.append(vmap(lambda indices: distance_correlation(x_all[indices], y_all[indices]))(chunk))
         dcors = torch.cat(dcors)
 
         rows.append(
             {
                 "label_name": label_name,
                 "dcor": dcors.tolist(),
                 "dcor_mean": dcors.mean().item(),
```

## htc/models/common/torch_helpers.py

```diff
@@ -27,15 +27,15 @@
     >>> torch.stack(tensor_ab).shape
     torch.Size([2, 3, 3])
 
     Args:
         tensors: List of tensors to stack.
         dim: Dimensions which should be padded.
         pad_value: Padding value appended to the tensors.
-        size_multiple: If not None, makes sure that the size of the tensors is divisible by this size, e.g. to have a size divisible by 32. Either provide a tuple with values per dimension (e.g. (height, width)) or an int which is then used for all dimensions.
+        size_multiple: If not None, makes sure that the respective size dimensions are divisible by this number(s), e.g. to have a size divisible by 32.
 
     Returns: Batch tensor (first dimension is the batch dimension).
     """
     if type(dim) == int:
         dim = (dim,)
 
     shapes = [t.shape for t in tensors]
@@ -72,84 +72,48 @@
 
         tensor = F.pad(tensor, padding, value=pad_value)
         padded_tensors.append(tensor)
 
     return padded_tensors
 
 
-def smooth_one_hot(labels: torch.Tensor, n_classes: int, smoothing: float = 0.0) -> torch.Tensor:
+def smooth_one_hot(labels: torch.Tensor, n_classes: int, smoothing: float = 0.0):
     """
     Create one-hot label vectors with optional label smoothing:
 
         - if smoothing == 0, it's one-hot method
         - if 0 < smoothing < 1, it's smooth method
 
     https://github.com/pytorch/pytorch/issues/7455#issuecomment-513735962
 
     >>> labels = torch.tensor([1, 0])
     >>> smooth_one_hot(labels, n_classes=2, smoothing=0)
     tensor([[0., 1.],
-            [1., 0.]], dtype=torch.float16)
+            [1., 0.]])
     >>> smooth_one_hot(labels, n_classes=2, smoothing=0.1)
-    tensor([[0.1000, 0.8999],
-            [0.8999, 0.1000]], dtype=torch.float16)
+    tensor([[0.1000, 0.9000],
+            [0.9000, 0.1000]])
 
     Args:
-        labels: Tensor with label indices, e.g. (batch, height, width).
+        labels: Vector with the label index values.
         n_classes: Number of classes which determines the output shape of the smoothed label vector.
         smoothing: Smoothing value which will be equally distributed across all other classes, e.g. if smoothing=0.1 then for the label index 1 the vector [0.1, 0.9] will be returned.
-
-    Returns: Smoothed one-hot label tensor of type torch.float16. The class dimension will be added to the end, e.g. (batch, height, width, class).
     """
     assert 0 <= smoothing < 1, "Invalid smoothing value"
+    assert len(labels.shape) == 1, "labels must be a vector"
     assert labels.dtype == torch.int64, "Wrong type for labels vector"
 
     confidence = 1.0 - smoothing
-    new_shape = torch.Size((*labels.shape, n_classes))
+    label_shape = torch.Size((labels.size(0), n_classes))
     with torch.no_grad():
-        labels_smooth = torch.empty(size=new_shape, dtype=torch.float16, device=labels.device)
-        labels_smooth.fill_(smoothing / (n_classes - 1))
-        labels_smooth.scatter_(dim=-1, index=labels.unsqueeze(dim=-1), value=confidence)
-
-    return labels_smooth
-
-
-def group_mean(indices: torch.Tensor, values: torch.Tensor) -> torch.Tensor:
-    """
-    Group and average values by the given indices.
-
-    >>> indices = torch.tensor([0, 0, 2, 2, 2])
-    >>> values = torch.tensor([1, 2, 3, 4, 5])
-    >>> group_mean(indices, values)
-    (tensor([0, 2]), tensor([1.5000, 4.0000]))
-
-    Args:
-        indices: Indices which define the group membership. Tensor will be flattened.
-        values: Values which should be averaged. Tensor will be flattened.
-
-    Returns: Averaged values per group. Vector of length `max(indices) + 1`.
-    """
-    assert indices.dtype == torch.int64, "Indices must be of type int64 (index values)"
-    indices = indices.flatten()
-    values = values.flatten()
-    assert len(indices) == len(values), "Indices and values must have the same length"
-
-    last_index = indices.max() + 1
-
-    aggregated = torch.zeros(last_index, dtype=values.dtype, device=indices.device)
-    aggregated.scatter_add_(0, indices, values)
-
-    counts = torch.zeros(last_index, dtype=values.dtype, device=indices.device)
-    counts.scatter_add_(0, indices, torch.ones_like(values))
-
-    valid = counts > 0
-    valid_indices = torch.arange(0, last_index, dtype=indices.dtype, device=indices.device)[valid]
-    valid_aggregated = (aggregated / counts)[valid]
+        true_dist = torch.empty(size=label_shape, device=labels.device)
+        true_dist.fill_(smoothing / (n_classes - 1))
+        true_dist.scatter_(1, labels.data.unsqueeze(1), confidence)
 
-    return valid_indices, valid_aggregated
+    return true_dist
 
 
 def move_batch_gpu(batch: dict, device: torch.device = None) -> dict:
     """
     Moves every tensor in the batch to the GPU (or any other device).
 
     Args:
```

## htc/models/common/transforms.py

```diff
@@ -4,15 +4,15 @@
 import pickle
 from collections.abc import Iterator
 from typing import Union
 
 import kornia.augmentation as K
 import numpy as np
 import torch
-from typing_extensions import Self
+from kornia.constants import DataKey, Resample
 
 from htc.models.common.torch_helpers import str_to_dtype
 from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 from htc.utils.type_from_string import type_from_string
@@ -128,17 +128,17 @@
     @staticmethod
     def apply_valid_transforms(
         batch: dict[str, torch.Tensor], transforms: list["HTCTransformation"]
     ) -> dict[str, torch.Tensor]:
         """
         Applies the transformations to the batch and checks whether after the transformation the batch is still valid. That is, at least one valid pixel must remain (per image). Otherwise, the original, non-transformed image will be returned.
 
-        Note: Works only for transformations which do not alter the batch in-place.
+        Note: Works only for transformations which do no alter the batch in-place.
 
-        For multi-layer segmentations, at least one valid pixel must remain in the first layer (the first layer is supposed to contain the main annotations of the image), otherwise the transformed image is not used.
+        For multi-layer segmentations, at least one valid pixel must remain per layer, otherwise the transformed image is not used.
         For multiple annotations, at least one valid pixel must remain for each annotated image. If one annotation_name has only invalid pixels left, the original, non-transformed image stays in the batch.
 
         Args:
             batch: The batch on which the transformations should be applied. All tensors must either be in the BHWC or BHW format.
             transforms: List of transformations.
 
         Returns: The transformed batch or the original batch. The key 'transforms_applied' with a value of True is added to the batch to mark it as transformed (the key is always added irrespective of whether transformations were applied or not).
@@ -155,91 +155,66 @@
         valid_keys = [k for k in batch_tmp.keys() if k.startswith("valid_pixels")]
 
         if len(label_keys) == 0 and len(valid_keys) == 0:
             # During prediction, we do not have labels
             batch |= batch_tmp
         else:
             # It is possible that an augmentation creates an invalid image, e.g. if only a small part was annotated and then this small part of the image gets rotated outside the field of view. In this case, we just use the original non-augmented image
-            # We need to make this decision for each image in the batch
             if len(valid_keys) > 0:
                 valid_pixels = [batch_tmp[k] for k in valid_keys]
             else:
                 valid_pixels = [batch_tmp[k] < settings.label_index_thresh for k in label_keys]
 
+            # We need to define per image in the batch which one to keep
             if all(t.ndim == 4 for t in valid_pixels):
-                # For multi-layer segmentations only the first layer is relevant
-                # The first layer contains the main annotations (e.g. semantic segmentations) whereas the other layers usually contain more fine-grained information (e.g. tags). The annotated region in the second layer may also be much smaller than the first layer, so we do not want to discard the image if the second layer is nearly empty
-                valid_pixels = [t[..., 0] for t in valid_pixels]
-
-            # Do we have for each image at least one valid pixel?
-            # BHW -> BX -> B
-            valid_samples = [t.reshape(t.size(0), -1).any(dim=-1) for t in valid_pixels]
+                # Multi-layer segmentations (we only keep a transformed image if in every layer at least one valid pixel remains)
+                # BHWC -> BCHW -> BCX -> BC -> B
+                valid_samples = [
+                    t.permute(0, 3, 1, 2).reshape(t.size(0), t.size(-1), -1).any(dim=-1).all(dim=-1)
+                    for t in valid_pixels
+                ]
+            elif all(t.ndim == 3 for t in valid_pixels):
+                # Do we have for each image at least one valid pixel?
+                # BHW -> BX -> B
+                valid_samples = [t.reshape(t.size(0), -1).any(dim=-1) for t in valid_pixels]
+            else:
+                raise ValueError("Invalid batch shape (either BHWC or BHW)")
 
             assert all(t.ndim == 1 for t in valid_samples), "Only the batch dimension should remain"
 
             # In case of multiple annotations, we only keep the transformed image if the augmentations from all annotations still yield a valid image
             valid_samples = torch.stack(valid_samples).all(dim=0)
 
             if valid_samples.all():
                 # Complete batch remains valid, so just use it
                 batch |= batch_tmp
             else:
                 # Decision for each image in the batch separately
                 for key in batch_tmp.keys():
-                    # index keys (e.g. image_index) never get transformed so we can just keep the original values
-                    if type(batch[key]) == torch.Tensor and not key.endswith("index"):
+                    if type(batch[key]) == torch.Tensor:
                         batch[key][valid_samples] = batch_tmp[key][valid_samples]
 
         batch["transforms_applied"] = True
         return batch
 
 
 class Normalization(HTCTransformation):
-    def __init__(self, order: int = 1, channel_dim: int = -1, **kwargs):
-        """
-        Simple normalization operation across the spectral channel independent for each pixel. Usually, all our networks expect L1 normalized data to account for multiplicative illumination changes.
-
-        This transformation accepts either a tensor object as input
-        >>> normalization = Normalization()
-        >>> features = torch.ones(1, 2, 2, 100)
-        >>> normalization(features)[0, 0, 0, :2]
-        tensor([0.0100, 0.0100])
-
-        or a dictionary with the features of the batch. In this case, only the `features` key of the batch is altered in-place:
-        >>> batch = normalization({"features": features})
-        >>> batch["features"][0, 0, 0, :2]
-        tensor([0.0100, 0.0100])
-
-        Args:
-            order: Order of the norm. 1 = L1 norm, 2 = L2 norm, etc.
-            channel_dim: Dimension of the channel axis.
-        """
+    def __init__(self, order: int = 1, **kwargs):
         self.order = order
-        self.channel_dim = channel_dim
-
-    def __call__(
-        self, batch: Union[torch.Tensor, dict[str, torch.Tensor]]
-    ) -> Union[torch.Tensor, dict[str, torch.Tensor]]:
-        if type(batch) == dict:
-            features = batch["features"]
-        else:
-            features = batch
-
-        features = features / torch.linalg.norm(features, ord=self.order, dim=self.channel_dim, keepdim=True)
-        features.nan_to_num_()
 
-        if type(batch) == dict:
-            batch["features"] = features
-        else:
-            batch = features
+    def __call__(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
+        batch["features"] = batch["features"] / torch.linalg.norm(
+            batch["features"], ord=self.order, dim=-1, keepdim=True
+        )
+        batch["features"].nan_to_num_()
 
         return batch
 
     def __repr__(self) -> str:
-        return f"Normalization(order={self.order}, channel_dim={self.channel_dim})"
+        return f"Normalization(order={self.order})"
 
 
 class StandardizeHSI(HTCTransformation):
     def __init__(self, stype: str, config: Config, fold_name: str, **kwargs):
         """
         This transformation performs standardization on the HSI pixels. This can either be done globally across all values (stype="pixel") or individually per channel (stype="channel") in which case the 480*640 values per channel are standardized separately.
 
@@ -248,15 +223,15 @@
         Args:
             stype: Type of standardization (pixel or channel).
             config: The configuration object used for training.
             fold_name: The name of the current fold (required to load the precomputed mean and std values).
         """
         # Standardization parameters are precomputed
         specs_name = DataSpecification.from_config(config).name()
-        params_path = settings.intermediates_dir_all / "data_stats" / f"{specs_name}#standardization.pkl"
+        params_path = settings.intermediates_dir / "data_stats" / f"{specs_name}#standardization.pkl"
         assert params_path.exists(), f"could not find the precomputed standardization parameter at {params_path}"
 
         params = pickle.load(params_path.open("rb"))
         assert (
             fold_name in params
         ), f"Could not find {fold_name} in standardization file (available keys: {params.keys()})"
         params = params[fold_name]
@@ -309,17 +284,17 @@
 
         return batch
 
     def __repr__(self) -> str:
         return f"ToType(dtype={self.dtype})"
 
     @staticmethod
-    def from_config(config: Config = None) -> Self:
+    def from_config(config: Config = None) -> "ToType":
         if config is not None and "trainer_kwargs/precision" in config:
-            if config["trainer_kwargs/precision"] == "16-mixed":
+            if config["trainer_kwargs/precision"] == 16:
                 return ToType(dtype=torch.float16)
             elif config["trainer_kwargs/precision"] == 32:
                 return ToType(dtype=torch.float32)
             else:
                 raise ValueError("Invalid precision value in config file")
         else:
             return ToType(dtype=torch.float32)
@@ -330,33 +305,30 @@
         assert len(transformation_names) == len(
             transformation_kwargs
         ), "There must be arguments for each transformation"
 
         transforms = []
         for name, t_kwargs in zip(transformation_names, transformation_kwargs):
             TransformationClass = getattr(K, name)
-
-            # Kornia expects tuples instead of lists for augmentation parameters
-            for k, v in t_kwargs.items():
-                if type(v) == list:
-                    t_kwargs[k] = tuple(v)
-
             transforms.append(TransformationClass(**t_kwargs))
 
-        self.compose = K.AugmentationSequential(*transforms)
+        # We need to extend the default arguments to make elastic transform work (https://github.com/kornia/kornia/issues/2002)
+        self.compose = K.AugmentationSequential(
+            *transforms, extra_args={DataKey.MASK: dict(resample=Resample.NEAREST, align_corners=True, mode="nearest")}
+        )
 
         self.keys_to_type = {
             "features": "input",
             "features_rgb": "input",
             "data_L1": "input",
             "data_parameter_images": "input",
             "labels": "mask",
             "valid_pixels": "mask",
+            "specs": "mask",
             "spxs": "mask",
-            "regions": "mask",
         }
 
     def __call__(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
         """
         Applies the specified transformations to the batch.
 
         Args:
@@ -404,26 +376,21 @@
 
                 trans_types[key] = trans_type
                 dtypes[key] = batch[key].dtype
 
         augmented = self.compose(*arguments.values(), data_keys=list(trans_types.values()))
         if type(augmented) == torch.Tensor:
             augmented = [augmented]
-        augmented = dict(zip(arguments.keys(), augmented))
 
         batch_transformed = {}
-        for key in batch.keys():
-            if key in augmented:
-                if self.keys_to_type[key] == "input":
-                    # From BCHW to BHWC
-                    batch_transformed[key] = augmented[key].permute(0, 2, 3, 1).type(dtypes[key])
-                else:
-                    # Back to the original type and remove channel dim if 1
-                    batch_transformed[key] = augmented[key].type(dtypes[key]).squeeze(dim=1)
+        for key, transformed_tensor in zip(arguments.keys(), augmented):
+            if self.keys_to_type[key] == "input":
+                # From BCHW to BHWC
+                batch_transformed[key] = transformed_tensor.permute(0, 2, 3, 1).type(dtypes[key])
             else:
-                # Make sure that the stuff we don't change is still available (e.g. image name)
-                batch_transformed[key] = batch[key]
+                # Back to the original type and remove channel dim if 1
+                batch_transformed[key] = transformed_tensor.type(dtypes[key]).squeeze(dim=1)
 
         return batch_transformed
 
     def __repr__(self) -> str:
         return "KorniaTransform"
```

## htc/models/common/utils.py

```diff
@@ -1,22 +1,18 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import copy
 import functools
-import importlib
 import math
 import platform
 import warnings
 from collections.abc import Callable
-from typing import Any, Union
 
 import psutil
 import torch
-import torch.nn as nn
 
 from htc.settings import settings
 from htc.utils.Config import Config
 from htc.utils.LabelMapping import LabelMapping
 
 
 def get_n_classes(config: Config) -> int:
@@ -40,69 +36,37 @@
     elif config["label_mapping"]:
         return len(LabelMapping.from_config(config))
     else:
         # User can explicitly disable the label mapping in which case no labels are used (e.g. self-supervised learning)
         return 0
 
 
-def parse_optimizer(config: Config, model: nn.Module) -> Union[tuple[list, list], Any]:
-    """
-    Creates an optimizer plus optionally an scheduler which can be used in your lightning module (via `configure_optimizers()`).
-
-    Args:
-        config: The training configuration (with an `optimization` key).
-        model: The network model containing the parameters to optimize.
-
-    Returns: Optimizer or optimizer and scheduler as tuple (same format as lightning).
-    """
-    # Dynamically initialize the optimizer based on the config
-    optimizer_param = copy.deepcopy(config["optimization/optimizer"])
-    del optimizer_param["name"]
-
-    name = config["optimization/optimizer/name"]
-    module = importlib.import_module("torch.optim")
-    optimizer_class = getattr(module, name)
-
-    optimizer = optimizer_class(model.parameters(), **optimizer_param)
-
-    if config["optimization/lr_scheduler"]:
-        # Same for the scheduler, if available
-        scheduler_param = copy.deepcopy(config["optimization/lr_scheduler"])
-        del scheduler_param["name"]
-
-        name = config["optimization/lr_scheduler/name"]
-        module = importlib.import_module("torch.optim.lr_scheduler")
-        scheduler_class = getattr(module, name)
-
-        scheduler = scheduler_class(optimizer, **scheduler_param)
-        return [optimizer], [scheduler]
-    else:
-        return optimizer
-
-
 def infer_swa_lr(config: Config) -> float:
     """
     Calculate the learning rate at the time when SWA kicks in. This might not be obvious if a custom learning rate scheduler is used.
 
     This is necessary because pytorch lightning now requires to explicitly set the learning rate for SWA (https://github.com/Lightning-AI/lightning/issues/11822).
 
     Args:
         config: The configuration of the training.
 
     Returns: The learning rate which can be used for SWA (`swa_lrs` argument).
     """
+    from htc.models.common.HTCLightning import HTCLightning
+
     if config["swa_kwargs/swa_lrs"]:
         return config["swa_kwargs/swa_lrs"]
     else:
         lr = config["optimization/optimizer/lr"]
 
-        # Dummy model to get a valid PyTorch optimizer
-        dummy_model = torch.nn.Linear(2, 1)
-        res = parse_optimizer(config, dummy_model)
+        # Dummy lightning class to get the optimizer and scheduler
+        LightningClass = HTCLightning.class_from_config(config)
+        module = LightningClass(dataset_train=None, datasets_val=None, config=config)
 
+        res = module.configure_optimizers()
         if type(res) == tuple and len(res) == 2:
             scheduler = res[1][0]
 
             # Run the scheduler up to the epoch where SWA kicks in
             swa_start = config.get("swa_kwargs/swa_epoch_start", 0.8)
             before_swa_epochs = int(swa_start * config["trainer_kwargs/max_epochs"]) - 1
             with warnings.catch_warnings():
@@ -117,34 +81,28 @@
                 swa_lr = swa_lr[0]
         else:
             swa_lr = lr
 
         return swa_lr
 
 
-def cluster_command(args: str, memory: str = "10.7G", n_gpus: int = 1, excluded_hosts: list[str] = None) -> str:
+def cluster_command(args: str, memory: str = "10.7G", n_gpus: int = 1) -> str:
     """
     Generates a cluster command with some default settings.
 
     Args:
         args: Argument string for the run_training.py script (model, config, etc.).
         memory: The minimal memory requirements for the GPU.
         n_gpus: The number of GPUs to use.
-        excluded_hosts: List of hosts to exclude. If None, no hosts are excluded.
 
     Returns: The command to run the job on the cluster.
     """
-    if excluded_hosts is not None:
-        excluded_hosts = " && ".join([f"hname!='{h}'" for h in excluded_hosts])
-        excluded_hosts = f'-R "select[{excluded_hosts}]" '
-    else:
-        excluded_hosts = ""
-
+    # The dgx2 nodes have only 6 cores per job which is not enough for our runs
     bsubs_command = (
-        f'bsub -R "tensorcore" {excluded_hosts}-q gpu-lowprio'
+        'bsub -R "tensorcore" -R "select[hname!=\'e230-dgx2-1\']" -R "select[hname!=\'e230-dgx2-2\']" -q gpu-lowprio'
         f" -gpu num={n_gpus}:j_exclusive=yes:mode=exclusive_process:gmem={memory} ./runner_htc.sh htc training {args}"
     )
 
     return bsubs_command
 
 
 def cpu_count() -> int:
@@ -221,16 +179,14 @@
     for key in sample1.keys():
         if type(sample1[key]) == torch.Tensor:
             if sample1[key].is_floating_point():
                 checks.append(sample2[key].is_floating_point())
                 checks.append(torch.allclose(sample1[key], sample2[key], **allclose_kwargs))
             else:
                 checks.append(torch.all(sample1[key] == sample2[key]))
-        elif type(sample1[key]) == dict and type(sample2[key]) == dict:
-            checks.append(samples_equal(sample1[key], sample2[key], **allclose_kwargs))
         else:
             checks.append(sample1[key] == sample2[key])
 
     return all(checks)
 
 
 # Make sure we always have a batch dimension
@@ -261,48 +217,7 @@
                     batch[key] = value.squeeze(dim=0)
                 else:
                     batch[key] = value[0]
 
         return batch
 
     return _sample_to_batch
-
-
-def multi_label_condensation(logits: torch.Tensor, config: Config) -> dict[str, torch.Tensor]:
-    """
-    Convert the output of a multi-label network to a single prediction per pixel.
-
-    In case this decision is ambiguous (multiple classes with a confidence > 0.5 or no class at all), the pixel will be marked as "network_unsure".
-
-    Args:
-        logits: The output of the network (batch, class, *).
-        config: The configuration of the training run.
-
-    Returns: Dictionary with the entries:
-        - `predictions`: Predicted labels (batch, *).
-        - `confidences`: Corresponding confidences of the prediction (batch, *).
-    """
-    if (logits >= 0).all() and (logits <= 1).all():
-        settings.log_once.warning(
-            "The logits seem to be already in the range [0, 1]. Please provide the raw logits of the network and not"
-            " the sigmoid activations"
-        )
-
-    confidences = logits.sigmoid()
-    preds = confidences > 0.5  # [BCHW]
-    valid = preds.count_nonzero(dim=1) == 1  # [BHW]
-
-    # We only use predictions if there is exactly one class predicted, the rest will be unsure
-    # This is because we are not evaluating a multi-label scenario yet as we want to be comparable to other runs
-    label_mapping = LabelMapping.from_config(config)
-    predicted_labels = torch.full(
-        valid.shape, label_mapping.name_to_index("network_unsure"), dtype=torch.int64, device=valid.device
-    )
-
-    # We can use argmax here because we explicitly use only pixels with exactly one predicted class
-    predicted_labels[valid] = preds.transpose(0, 1)[:, valid].float().argmax(dim=0)
-
-    confidences = confidences.gather(dim=1, index=predicted_labels.unsqueeze(dim=1)).squeeze(dim=1)
-    return {
-        "predictions": predicted_labels,
-        "confidences": confidences,
-    }
```

## htc/models/data/DataSpecification.py

```diff
@@ -6,15 +6,14 @@
 import re
 from collections.abc import Iterator
 from contextlib import contextmanager
 from pathlib import Path
 from typing import IO, Union
 
 import pandas as pd
-from typing_extensions import Self
 
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 
 
 class DataSpecification:
@@ -24,40 +23,24 @@
 
         >>> specs = DataSpecification('pigs_semantic-only_5foldsV2.json')
         >>> path = specs.folds['fold_P041,P060,P069']['train_semantic'][0]  # Select the first path of the training set
         >>> path.image_name()
         'P044#2020_02_01_09_51_15'
 
         You can also easily construct datasets from the folds:
+
         >>> from htc.models.image.DatasetImage import DatasetImage
         >>> first_fold = specs.folds['fold_P041,P060,P069']
         >>> dataset_train = DatasetImage(first_fold['train_semantic'], train=True)
         >>> dataset_train[0]['image_name']
         'P044#2020_02_01_09_51_15'
         >>> dataset_val_unknown = DatasetImage(first_fold['val_semantic_unknown'], train=False)
         >>> dataset_val_unknown[0]['image_name']
         'P041#2019_12_14_12_00_16'
 
-        Or get the paths from the different splits
-        >>> validation_paths = specs.paths("^val")
-
-        If you want to read the test paths, you have to explicitly enable it:
-        >>> with specs.activated_test_set():
-        ...     test_paths = specs.paths("^test")
-        >>> len(validation_paths)
-        340
-        >>> len(test_paths)
-        166
-        >>> len({p.subject_name for p in validation_paths})
-        15
-        >>> len({p.subject_name for p in test_paths})
-        5
-
-        The main purpose of the data specification is to define your training setup, i.e. which paths should be used for training, which for validation etc. For this, add the name or (relative) path to your data specification in your config (`input/data_spec`) and it will iterate over your folds, use all paths for training which are part of a split starting with the name `train` for training, all with `val` for validation and  `test` for testing. For testing, however, you need to add the `--test` argument to the training script if it should be used and the images validated.
-
         Args:
             path_or_file: Path (or string) to the data specification json file (path can also be relative to the models or data directories) or a file object which implements a read() method.
         """
         if isinstance(path_or_file, str):
             path_or_file = Path(path_or_file)
 
         if hasattr(path_or_file, "read"):
@@ -124,15 +107,15 @@
             self.__folds_test[fold["fold_name"]] = fold_data_test
 
         # Make sure the folds are consistent
         split_names = self.split_names()
         for fold_data in self.folds.values():
             assert split_names == list(fold_data.keys()), "Every fold must use the same splits"
 
-    def __eq__(self, other: Self) -> bool:
+    def __eq__(self, other: "DataSpecification") -> bool:
         return self.folds == other.folds and self.__folds_test == other.__folds_test
 
     def __len__(self) -> int:
         """
         Returns: The number of folds in the data specification file.
         """
         return len(self.folds)
@@ -275,15 +258,15 @@
         """
         if str(self.path).startswith(str(settings.htc_package_dir)):
             return self.path.name
         else:
             return str(self.path)
 
     @classmethod
-    def from_config(cls, config: Config) -> Self:
+    def from_config(cls, config: Config) -> "DataSpecification":
         assert "input/data_spec" in config, "There is no data specification defined in the config"
 
         if isinstance(config["input/data_spec"], DataSpecification):
             return config["input/data_spec"]
         else:
             spec = cls(config["input/data_spec"])
             config["input/data_spec"] = spec  # Cache for future use
```

## htc/models/data/run_size_dataset.py

```diff
@@ -18,21 +18,23 @@
 from htc.utils.sqldf import sqldf
 
 
 def label_mapping_dataset_size() -> LabelMapping:
     df_stats = basic_statistics("2021_02_05_Tivita_multiorgan_semantic", "pigs_semantic-only_5foldsV2.json")
 
     df_organs = df_stats.query("label_name in @settings_seg.labels")
-    df_organ_counts = sqldf("""
+    df_organ_counts = sqldf(
+        """
         SELECT label_name, COUNT(DISTINCT subject_name) AS n_pigs
         FROM df_organs
         WHERE set_type = 'train'
         GROUP BY label_name
         ORDER BY n_pigs
-    """)
+    """
+    )
 
     # We select all the labels which occur in every pig in the train dataset
     n_pigs_total = df_stats.query('set_type == "train"')["subject_name"].nunique()
     labels = df_organ_counts.query("n_pigs == @n_pigs_total")["label_name"].tolist()
 
     # Use the same background mapping
     mapping_settings = settings_seg.label_mapping
@@ -62,15 +64,15 @@
     used_classes = [
         label_name
         for label_name, label_index in mapping.mapping_name_index.items()
         if label_index > 0 and label_index < settings.label_index_thresh
     ]  # These classes are available in every pig
     available_labels = path.annotated_labels()
 
-    return any(l in used_classes for l in available_labels)
+    return any([l in used_classes for l in available_labels])
 
 
 def filter_min_pixels(path: DataPath, mapping: LabelMapping) -> bool:
     sample = DatasetImage(
         [path], train=False, config=Config({"input/no_features": True, "label_mapping": mapping.mapping_name_index})
     )[0]
```

## htc/models/image/DatasetImage.py

```diff
@@ -1,16 +1,16 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import torch
+from skimage.segmentation import slic
 
 from htc.models.common.HTCDataset import HTCDataset
 from htc.models.data.DataSpecification import DataSpecification
 from htc.utils.DomainMapper import DomainMapper
-from htc.utils.SLICWrapper import SLICWrapper
 
 
 class DatasetImage(HTCDataset):
     """
     This is the basic dataset for reading images via an index-based way. The index corresponds to the path in the paths list.
 
     If no config is provided, the default is used which reads HSI images, its labels and a mask with valid pixels without applying augmentations.
@@ -69,22 +69,20 @@
                 rgb = self.paths[index].read_rgb_reconstructed() / 255
                 sample["features_rgb"] = torch.from_numpy(rgb).float()
                 spx_features_name = "features_rgb"
             else:
                 # We already have the RGB data so we can directly use it for the superpixels
                 spx_features_name = "features"
 
-        # We need to apply the transformations before we compute the superpixels because the superpixel mask cannot be transformed
-        # The main problem is that the border values get mirrored leading to duplicate superpixel indices or missing indices
         sample = self.apply_transforms(sample)  # e.g. features.shape = [480, 640, 100]
 
         if self.config["input/superpixels"]:
-            fast_slic = SLICWrapper(**self.config["input/superpixels"])
-            sample["spxs"] = fast_slic.apply_slic(sample[spx_features_name])
-
+            sample["spxs"] = torch.from_numpy(
+                slic(sample[spx_features_name].float().numpy(), start_label=0, **self.config["input/superpixels"])
+            )
             if spx_features_name == "features_rgb":
                 # We only needed the rgb features to calculate the superpixels
                 del sample["features_rgb"]
 
         for domain in self.target_domains:
             sample[domain] = self.domain_mapper[domain].domain_index(sample["image_name"])
```

## htc/models/image/DatasetImageBatch.py

```diff
@@ -81,12 +81,14 @@
                 for name in self._possible_annotation_names():
                     self._add_tensor_shared(f"labels_{name}", torch.int64, *spatial_shape)
                     self._add_tensor_shared(f"valid_pixels_{name}", torch.bool, *spatial_shape)
             else:
                 self._add_tensor_shared("labels", torch.int64, *spatial_shape)
                 self._add_tensor_shared("valid_pixels", torch.bool, *spatial_shape)
 
+        if self.config["input/specs_threshold"]:
+            self._add_tensor_shared("specs", torch.int64, *spatial_shape)
         if self.config["input/superpixels"]:
             self._add_tensor_shared("spxs", torch.int64, *spatial_shape)
 
         for domain in self.target_domains:
             self._add_tensor_shared(domain, torch.int64)
```

## htc/models/image/DatasetImageStream.py

```diff
@@ -30,26 +30,27 @@
     ...     print(sample['features'].shape)
     ...     print(sample['labels'].shape)
     torch.Size([1, 480, 640, 100])
     torch.Size([1, 480, 640])
     """
 
     def iter_samples(self) -> Iterator[dict[str, torch.Tensor]]:
-        for worker_index, path_index in self._iter_paths():
-            sample = self[path_index]
+        for worker_index, path in self._iter_paths():
+            image_name = path.image_name()
+            sample = self.from_image_name(image_name)
             del sample["image_name"]
 
             if "features" in sample:
                 sample["features"] = sample["features"].refine_names("H", "W", "C")
             if "labels" in sample:
                 sample["labels"] = sample["labels"].refine_names("H", "W")
             if "valid_pixels" in sample:
                 sample["valid_pixels"] = sample["valid_pixels"].refine_names("H", "W")
             sample["worker_index"] = worker_index
-            sample["image_index"] = path_index
+            sample["image_index"]: self.image_names.index(path.image_name())
 
             yield sample
 
     def _add_shared_resources(self) -> None:
         self._add_image_index_shared()
         spatial_shape = self.paths[0].dataset_settings["spatial_shape"]
 
@@ -60,14 +61,16 @@
                 for name in self._possible_annotation_names():
                     self._add_tensor_shared(f"labels_{name}", torch.int64, *spatial_shape)
                     self._add_tensor_shared(f"valid_pixels_{name}", torch.bool, *spatial_shape)
             else:
                 self._add_tensor_shared("labels", torch.int64, *spatial_shape)
                 self._add_tensor_shared("valid_pixels", torch.bool, *spatial_shape)
 
+        if self.config["input/specs_threshold"]:
+            self._add_tensor_shared("specs", torch.int64, *spatial_shape)
         if self.config["input/superpixels"]:
             self._add_tensor_shared("spxs", torch.int64, *spatial_shape)
 
         for domain in self.target_domains:
             self._add_tensor_shared(domain, torch.int64)
 
     def n_image_elements(self) -> int:
```

## htc/models/image/LightningImage.py

```diff
@@ -12,64 +12,63 @@
 from torch.utils.data import DataLoader
 from torch.utils.data.sampler import RandomSampler
 
 from htc.models.common.class_weights import calculate_class_weights
 from htc.models.common.HierarchicalSampler import HierarchicalSampler
 from htc.models.common.HTCDataset import HTCDataset
 from htc.models.common.HTCLightning import HTCLightning
-from htc.models.common.loss import SuperpixelLoss
+from htc.models.common.loss import KLDivLossWeighted, SuperpixelLoss
 from htc.models.common.StreamDataLoader import StreamDataLoader
 from htc.models.common.torch_helpers import smooth_one_hot
 from htc.models.common.utils import get_n_classes
 from htc.models.image.DatasetImageStream import DatasetImageStream
 from htc.settings import settings
-from htc.utils.type_from_string import type_from_string
 
 
 class LightningImage(HTCLightning):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
         name = self.config["model/model_name"]
         if name is not None:
-            if ">" in name:
-                ModelClass = type_from_string(name)
-            else:
-                module = importlib.import_module(f"htc.models.image.{name}")
-                ModelClass = getattr(module, name)
+            module = importlib.import_module(f"htc.models.image.{name}")
+            ModelClass = getattr(module, name)
 
             self.model = ModelClass(self.config)
 
         if self.config["model/class_weight_method"] and self.dataset_train is not None:
-            class_weights = calculate_class_weights(self.config, *self.dataset_train.label_counts())
+            weights = calculate_class_weights(
+                self.config, *self.dataset_train.label_counts()
+            )  # Calculate class weights to overcome class imbalances
         else:
-            class_weights = torch.ones(get_n_classes(self.config))
-        self.register_buffer("class_weights", class_weights, persistent=False)
+            weights = torch.ones(get_n_classes(self.config))
 
-        self.ce_loss_weighted = nn.CrossEntropyLoss(
-            weight=self.class_weights, label_smoothing=self.config.get("optimization/label_smoothing_ce", 0)
-        )
+        if self.config["optimization/label_smoothing"]:
+            self.ce_loss_weighted = KLDivLossWeighted(weight=weights)
+        else:
+            self.ce_loss_weighted = nn.CrossEntropyLoss(weight=weights)
 
-        self.dice_loss = DiceLoss(reduction="none", softmax=True, batch=True)
+        self.dice_loss = DiceLoss(reduction="none", to_onehot_y=True, softmax=True, batch=True)
         if "optimization/spx_loss_weight" in self.config:
             assert (
                 "input/superpixels" in self.config
             ), "Superpixels are missing in the input specification but they are required for the superpixel loss"
             self.spx_loss = SuperpixelLoss()
 
             if type(self.config["optimization/spx_loss_weight"]) == dict:
                 # spx_loss_weight is specified as a function of the current epoch
                 epochs = self.config["optimization/spx_loss_weight/epochs"]
                 weights = self.config["optimization/spx_loss_weight/weights"]
                 self.spx_loss_weight_f = interpolate.interp1d(
                     epochs, weights, fill_value=(weights[0], weights[-1]), bounds_error=False
                 )
             else:
-                # Constant value returned as array to be consistent with the interpolation output
-                self.spx_loss_weight_f = lambda _: np.array(self.config["optimization/spx_loss_weight"])
+                self.spx_loss_weight_f = lambda _: np.array(
+                    self.config["optimization/spx_loss_weight"]
+                )  # Also an array to be consistent with the interpolation output
 
     @staticmethod
     def dataset(**kwargs) -> HTCDataset:
         if kwargs["train"]:
             if kwargs["config"]["input/hierarchical_sampling"]:
                 sampler = HierarchicalSampler(kwargs["paths"], kwargs["config"])
             else:
@@ -88,70 +87,49 @@
 
     def val_dataloader(self) -> DataLoader:
         return [StreamDataLoader(dataset_val) for dataset_val in self.datasets_val]
 
     def test_dataloader(self) -> DataLoader:
         return StreamDataLoader(self.dataset_test)
 
-    def forward(self, batch: dict[str, torch.Tensor]) -> torch.Tensor:
-        x = batch["features"]
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = x.permute(0, 3, 1, 2)  # Input dimension for UNet needs to be [N, C, H, W]
 
-        # x.stride() = (30720000, 1, 64000, 100), i.e. channel last format
         return self.model(x)
 
-    def training_step(
-        self, batch: dict[str, torch.Tensor], batch_idx: int, return_valid_tensors: bool = False
-    ) -> dict[str, torch.Tensor]:
+    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int, return_valid_tensors: bool = False) -> dict:
         ce_loss_weight = self.config.get("optimization/ce_loss_weight", 1.0)
         dice_loss_weight = self.config.get("optimization/dice_loss_weight", 1.0)
 
         if "optimization/spx_loss_weight" in self.config:
-            # .item is necessary here as otherwise lightning does not handle the logging properly
-            spx_loss_weight = self.spx_loss_weight_f(self.current_epoch).item()
+            spx_loss_weight = self.spx_loss_weight_f(
+                self.current_epoch
+            ).item()  # .item is necessary here as otherwise lightning does not handle the logging properly
             self.log("train/spx_loss_weight", spx_loss_weight)
         else:
             spx_loss_weight = 0
 
-        predictions = self(batch)
-        if type(predictions) == dict:
-            predictions = predictions["class"]  # [BCHW]
-        n_classes = predictions.size(1)
-
         labels = batch["labels"]
-        valid_pixels = batch["valid_pixels"]
-        used_labels = labels[valid_pixels].unique()
-
-        # We need to replace the invalid labels with a "valid" one for the one-hot encoding (but the values won't be used)
-        labels = labels.masked_fill(~valid_pixels, 0)
-        if self.config["optimization/label_smoothing"]:
-            labels = smooth_one_hot(
-                labels, n_classes=n_classes, smoothing=self.config["optimization/label_smoothing"]
-            )  # [BHWC]
-        else:
-            labels = F.one_hot(labels, num_classes=n_classes).to(torch.float16)  # [BHWC]
+        predictions = self(batch["features"])
+        if type(predictions) == dict:
+            predictions = predictions["class"]
 
-        if n_mix := self.config["optimization/image_mixing"]:
-            # Mix images in the batch together (with reduced number of images for a smaller memory footprint)
-            labels_mix = (labels.roll(1, dims=0)[:n_mix] + labels[:n_mix]) / 2
-            valid_pixels_mix = valid_pixels.roll(1, dims=0)[:n_mix] & valid_pixels[:n_mix]
-            features_mix = (batch["features"].roll(1, dims=0)[:n_mix] + batch["features"][:n_mix]) / 2
-            predictions_mix = self({"features": features_mix})
-            if type(predictions_mix) == dict:
-                predictions_mix = predictions_mix["class"]
-
-            labels = torch.cat([labels, labels_mix])
-            predictions = torch.cat([predictions, predictions_mix])
-            valid_pixels = torch.cat([valid_pixels, valid_pixels_mix])
-
-        # Calculate the losses only for the valid pixels
-        # Keep the class dimension
-        valid_predictions = predictions.permute(0, 2, 3, 1)[valid_pixels]  # (samples, class)
-        valid_labels = labels[valid_pixels]  # (samples, class)
-        assert valid_predictions.shape == valid_labels.shape, "Invalid shape"
+        # Calculate the losses only for the valid pixels. This is a bit complicated since we need to preserve the logits dimension (we discard the spatial dimension in this process)
+        valid_pixels_mask = (
+            batch["valid_pixels"].unsqueeze(dim=1).expand(-1, predictions.shape[1], -1, -1)
+        )  # Bring the mask to the shape [3, 19, 480, 640] (same as prediction)
+        valid_predictions = predictions.transpose(1, 0)[
+            valid_pixels_mask.transpose(1, 0)
+        ]  # Apply the mask but put the logits dimension in front [19*N] (N = number of valid pixels which remain). This ensures that we can easily reshape the logits dimension back
+        valid_predictions = valid_predictions.reshape(predictions.shape[1], -1).transpose(
+            1, 0
+        )  # Reshape the logits dimension back and put the batch dimension back to the front [N, 19]
+        valid_labels = labels[batch["valid_pixels"]]
+        assert valid_predictions.shape[0] == valid_labels.shape[0], "Invalid shape in the batch dimension"
+        assert valid_predictions.shape[1] == predictions.shape[1], "Invalid shape in the logits dimension"
 
         n_invalid = (~valid_predictions.isfinite()).sum()
         if n_invalid > 0:
             valid_predictions.nan_to_num_()
             settings.log.warning(
                 f"Found {n_invalid} invalid values in prediction of the annotated area"
                 f" ({self.current_epoch = }, {self.global_step = })"
@@ -159,48 +137,62 @@
             settings.log_once.warning(
                 "nan_to_num will be applied to the predictions but please note that this is only a workaround and no"
                 " real solution. It is very likely that the model does not learn properly (this message is not shown"
                 " again)"
             )
 
         # Cross Entropy loss
-        ce_loss = self.ce_loss_weighted(valid_predictions, valid_labels)
+        if self.config["optimization/label_smoothing"]:
+            valid_prediction_log = F.log_softmax(valid_predictions, dim=1)
+            valid_labels_smooth = smooth_one_hot(
+                valid_labels, n_classes=valid_predictions.size(1), smoothing=self.config["optimization/label_smoothing"]
+            )
+            ce_loss = self.ce_loss_weighted(valid_prediction_log, valid_labels_smooth)
+        else:
+            ce_loss = self.ce_loss_weighted(valid_predictions, valid_labels)
 
-        self.log("train/ce_loss", ce_loss, on_epoch=True)
-        loss_sum = ce_loss_weight * ce_loss
+        self.log(
+            "train/ce_loss", ce_loss, on_epoch=True
+        )  # Automatically aggregated and averaged by Lightning for all epochs
+        loss = ce_loss_weight * ce_loss
 
         # Superpixel loss
         if spx_loss_weight > 0:
             spxs_vec = []
             for b in range(batch["spxs"].shape[0]):
                 # The superpixel id must be unique per batch for easier calculation of the gini coefficient
                 # Here, we avoid this problem by shifting the indices to the next free range in the index space, e.g. 1, 2 --> 1001, 1002
                 spxs_vec.append(batch["spxs"][b].flatten() + b * self.config["input/superpixels/n_segments"])
             spxs_vec = torch.cat(spxs_vec)  # [M]
             prediction_vec = predictions.transpose(1, 0).flatten(start_dim=1).transpose(1, 0)  # [N, 19]
 
             spx_loss = self.spx_loss(prediction_vec, spxs_vec)
-            loss_sum += spx_loss_weight * spx_loss
+            loss += spx_loss_weight * spx_loss
             self.log("train/spx_loss", spx_loss, on_epoch=True)
 
         # Dice loss
+        valid_predictions = valid_predictions.unsqueeze(dim=-1)  # All pixels are put into the batch dimension [N, C, 1]
+        valid_labels = valid_labels.unsqueeze(dim=-1).unsqueeze(dim=-1)  # Same for the labels [N, 1, 1]
         dice_loss = self.dice_loss(input=valid_predictions, target=valid_labels)  # Dice per class [C]
 
         # We use only the classes available in the batch to calculate the dice
-        used_weights = self.class_weights[used_labels]
-        # Only use dice values from classes which really occurred in the images, e.g. [4] and weight the class dices
-        dice_loss = dice_loss[used_labels] * used_weights
+        used_labels = valid_labels.unique()
+        used_weights = self.ce_loss_weighted.weight[used_labels]
+        dice_loss = (
+            dice_loss[used_labels] * used_weights
+        )  # Only use dice values from classes which really occurred in the images, e.g. [4] and weight the class dices
         dice_loss = dice_loss.sum() / used_weights.sum()  # Weighted average
         self.log("train/dice_loss", dice_loss, on_epoch=True)
-        loss_sum += dice_loss_weight * dice_loss
+        loss += dice_loss_weight * dice_loss
+
+        # Normalize the loss (weighted average)
+        loss /= sum([ce_loss_weight, dice_loss_weight, spx_loss_weight])
+
+        res = {"loss": loss}
+        if "image_index" in batch:
+            res["img_indices"] = batch["image_index"]
 
-        res = {}
-        loss_weights = sum([ce_loss_weight, dice_loss_weight, spx_loss_weight])
         if return_valid_tensors:
             res["valid_predictions"] = valid_predictions
             res["valid_labels"] = valid_labels
-            res["loss_sum"] = loss_sum
-            res["loss_weights"] = loss_weights
 
-        # Normalize the loss (weighted average)
-        res["loss"] = loss_sum / loss_weights
         return res
```

## htc/models/image/ModelImage.py

```diff
@@ -11,27 +11,24 @@
 from htc.utils.Config import Config
 
 
 class ModelImage(HTCModel):
     def __init__(self, config: Config, channels: int = None):
         super().__init__(config)
 
-        if self.config["model/input_channels"]:
-            channels = self.config["model/input_channels"]
-
         if self.config["model/channel_preprocessing"]:
             self.channel_preprocessing = HSI3dChannel(self.config)
             channels = self.channel_preprocessing.output_channels()
         else:
             self.channel_preprocessing = nn.Identity()
             channels = self.config["input/n_channels"] if channels is None else channels
 
         ArchitectureClass = getattr(smp, self.config["model/architecture_name"])
         self.architecture = ArchitectureClass(
-            classes=get_n_classes(self.config), in_channels=channels, **self.config.get("model/architecture_kwargs", {})
+            classes=get_n_classes(self.config), in_channels=channels, **self.config["model/architecture_kwargs"]
         )
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = self.channel_preprocessing(x)
         x = self.architecture(x)
 
         return x
```

## htc/models/image/configs/default.json

### Pretty-printed

 * *Similarity: 0.9930555555555556%*

 * *Differences: {"'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -60,14 +60,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 100,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "dice_metric",
         "dataset_index": 0
     }
 }
```

## htc/models/image/configs/spxs.json

### Pretty-printed

 * *Similarity: 0.9583333333333334%*

 * *Differences: {"'input'": "{'superpixels': {'slic_zero': True, 'sigma': 3}}"}*

```diff
@@ -1,13 +1,15 @@
 {
     "inherits": "default",
     "input": {
         "superpixels": {
             "compactness": 10,
-            "n_segments": 1000
+            "n_segments": 1000,
+            "sigma": 3,
+            "slic_zero": true
         }
     },
     "optimization": {
         "spx_loss_weight": {
             "epochs": [
                 20,
                 90,
```

## htc/models/patch/DatasetPatchImage.py

```diff
@@ -9,79 +9,79 @@
 
 
 class DatasetPatchImage(DatasetImage):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
         assert len(self.config["input/patch_size"]) == 2, "Only 2D patches are supported"
-        self.patch_height = self.config["input/patch_size"][0]
-        self.patch_width = self.config["input/patch_size"][1]
+        assert (
+            self.config["input/patch_size"][0] == self.config["input/patch_size"][1]
+        ), "Only square patches are supported"
+        self.patch_size = self.config["input/patch_size"][0]
 
     def __getitem__(self, index: int) -> dict[str, torch.Tensor]:
         sample = super().__getitem__(index)
 
         n_channels = sample["features"].shape[-1]
 
         # Expand image so that the patch blocks fit easily
-        features = pad_tensors([sample["features"]], size_multiple=self.config["input/patch_size"])[0]
-        sample["image_size"] = torch.tensor(sample["features"].shape[:2])
-        sample["image_size_expanded"] = torch.tensor(features.shape[:2])
+        features = pad_tensors([sample["features"]], size_multiple=self.patch_size)[0]
+        sample["img_size_expanded"] = torch.tensor(features.shape[:2])
 
         # Split the image into patches
-        patch_features = features.unfold(dimension=0, size=self.patch_height, step=self.patch_height).unfold(
-            dimension=1, size=self.patch_width, step=self.patch_width
+        patch_features = features.unfold(0, self.patch_size, self.patch_size).unfold(
+            1, self.patch_size, self.patch_size
         )
-        patch_features = patch_features.reshape(
-            -1, n_channels, self.patch_height, self.patch_width
-        )  # [300, 100, 32, 32]
+        patch_features = patch_features.reshape(-1, n_channels, self.patch_size, self.patch_size)  # [300, 100, 32, 32]
         patch_features = patch_features.permute(0, 2, 3, 1)  # [300, 32, 32, 100]
         sample["features"] = patch_features
 
         return sample
 
     def reshape_img(self, tensor: torch.Tensor, sample: dict[str, torch.Tensor]) -> torch.Tensor:
         """
         Reshapes patches of an image (e.g. predictions) back to the original image. In case the image was expanded in the first place, the original image size is restored.
 
         Args:
-            tensor: Patches of an image (n_patches, n_channels, patch_height, patch_width) or (n_patches, patch_height, patch_width).
+            tensor: Patches of an image (n_patches, n_channels, patch_size, patch_size) or (n_patches, patch_size, patch_size).
             sample: The original sample from this dataset which contains information about the image dimension.
 
         Returns: Full image based on the patches (n_height, n_width, n_channels) or (n_height, n_width).
         """
-        if len(sample["image_size_expanded"].shape) == 1:
-            image_size = sample["image_size"]
-            image_size_expanded = sample["image_size_expanded"]
-        else:
-            image_size = sample["image_size"][0]
-            image_size_expanded = sample["image_size_expanded"][0]
-
-        image_size = tuple(image_size)
-        image_size_expanded = tuple(image_size_expanded)
+        img_size = (
+            sample["valid_pixels"].shape if len(sample["valid_pixels"].shape) == 2 else sample["valid_pixels"].shape[1:]
+        )
+        img_size_expanded = (
+            sample["img_size_expanded"]
+            if len(sample["img_size_expanded"].shape) == 1
+            else sample["img_size_expanded"][0]
+        )
+        img_size = tuple(img_size)
+        img_size_expanded = tuple(img_size_expanded)
 
         original_type = tensor.dtype
 
         # Reshape block of patches back to the original image size
         # tensor.shape = [300, 32, 32, 100]
 
         if len(tensor.shape) == 4:
             # Channel must be the second dimension for correct folding
             tensor = tensor.permute(0, 3, 1, 2)  # [300, 100, 32, 32]
 
         tensor = tensor.reshape(tensor.shape[0], -1).unsqueeze(dim=0).permute(0, 2, 1)  # [1, 102400, 300]
         tensor = nn.Fold(
-            output_size=image_size_expanded,
-            kernel_size=(self.patch_height, self.patch_width),
-            stride=(self.patch_height, self.patch_width),
+            output_size=img_size_expanded,
+            kernel_size=(self.patch_size, self.patch_size),
+            stride=(self.patch_size, self.patch_size),
         )(
             tensor.type(torch.float32)
         )  # [1, 100, 480, 640]
         tensor = tensor.type(original_type).squeeze()
 
         if len(tensor.shape) == 3:
             tensor = tensor.permute(1, 2, 0)  # [480, 640, 100]
 
-        if image_size != image_size_expanded:
+        if img_size != img_size_expanded:
             # Remove the expanded parts
-            tensor = tensor[: image_size[0], : image_size[1]]
+            tensor = tensor[: img_size[0], : img_size[1]]
 
         return tensor
```

## htc/models/patch/DatasetPatchStream.py

```diff
@@ -62,16 +62,16 @@
         assert self.config["input/patch_size"][0] >= 32, "At least a patch size of 32 is required"
 
         self.patch_size = self.config["input/patch_size"][0]
         self.patch_size_half = self.patch_size // 2
         self.label_mapping = LabelMapping.from_config(self.config)
 
     def iter_samples(self, include_position: bool = False) -> Iterator[dict[str, torch.Tensor]]:
-        for worker_index, path_index in self._iter_paths():
-            path = self.paths[path_index]
+        for worker_index, path in self._iter_paths():
+            # We read the HSI image like DatasetImage
             sample_img = self.read_experiment(path)
             sample_img = self.apply_transforms(sample_img)
 
             relevant_pixels = sample_img["valid_pixels"].clone()
             if self.config.get("input/background_undersampling", True) and "background" in self.label_mapping:
                 # We don't want patches with too many background pixels
                 relevant_pixels[sample_img["labels"] == self.label_mapping.name_to_index("background")] = False
@@ -111,22 +111,28 @@
                     center_col - self.patch_size_half : center_col + self.patch_size_half,
                 ]
 
                 sample = {
                     "features": features.refine_names("H", "W", "C"),
                     "labels": labels.refine_names("H", "W"),
                     "valid_pixels": valid_pixels.refine_names("H", "W"),
-                    "image_index": path_index,
+                    "image_index": self.image_names.index(path.image_name()),
                     "worker_index": worker_index,
                 }
 
                 if include_position:
                     sample["center_row"] = center_row
                     sample["center_col"] = center_col
 
+                if "input/specs_threshold" in self.config:
+                    sample["specs"] = sample_img["specs"][
+                        center_row - self.patch_size_half : center_row + self.patch_size_half,
+                        center_col - self.patch_size_half : center_col + self.patch_size_half,
+                    ].refine_names("H", "W")
+
                 yield sample
 
     def n_image_elements(self) -> int:
         return math.ceil(
             np.prod(self.paths[0].dataset_settings["spatial_shape"]) / np.prod(self.config["input/patch_size"])
         )
 
@@ -216,7 +222,10 @@
             if self.config["input/annotation_name"] and not self.config["input/merge_annotations"]:
                 for name in self._possible_annotation_names():
                     self._add_tensor_shared(f"labels_{name}", torch.int64, *self.config["input/patch_size"])
                     self._add_tensor_shared(f"valid_pixels_{name}", torch.bool, *self.config["input/patch_size"])
             else:
                 self._add_tensor_shared("labels", torch.int64, *self.config["input/patch_size"])
                 self._add_tensor_shared("valid_pixels", torch.bool, *self.config["input/patch_size"])
+
+        if self.config["input/specs_threshold"]:
+            self._add_tensor_shared("specs", torch.int64, *self.config["input/patch_size"])
```

## htc/models/patch/LightningPatch.py

```diff
@@ -13,32 +13,28 @@
 
 
 class LightningPatch(LightningImage):
     @staticmethod
     def dataset(**kwargs) -> HTCDataset:
         if kwargs["train"]:
             if kwargs["config"]["input/hierarchical_sampling"]:
-                kwargs["sampler"] = HierarchicalSampler(
-                    kwargs["paths"], kwargs["config"], batch_size=kwargs["config"]["dataloader_kwargs/num_workers"]
-                )
+                kwargs["sampler"] = HierarchicalSampler(kwargs["paths"], kwargs["config"])
 
             return DatasetPatchStream(**kwargs)
         else:
             return DatasetPatchImage(**kwargs)
 
     def val_dataloader(self, **kwargs) -> list[DataLoader]:
         # We only evaluate one image at a time (safer)
         return HTCLightning.val_dataloader(self, batch_size=1, **kwargs)
 
     def test_dataloader(self, **kwargs) -> DataLoader:
         # We only evaluate one image at a time (safer)
         return HTCLightning.test_dataloader(self, batch_size=1, **kwargs)
 
     def _predict_images(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
-        # DatasetPatchImage stores the number of patches in the image in the batch dimension so we don't need the real batch dimension
-        batch["features"] = batch["features"].squeeze(dim=0)
-
-        predictions = self(batch).permute(0, 2, 3, 1)  # [N, C, H, W] --> [N, H, W, C]
+        patches = batch["features"].squeeze(dim=0)
+        predictions = self(patches).permute(0, 2, 3, 1)  # [N, C, H, W] --> [N, H, W, C]
         image_predictions = self.datasets_val[0].reshape_img(predictions, batch)
         image_predictions = image_predictions.permute(2, 0, 1).unsqueeze(dim=0)  # [N, C, H, W]
 
         return {"class": image_predictions}
```

## htc/models/patch/configs/default.json

### Pretty-printed

 * *Similarity: 0.9930555555555556%*

 * *Differences: {"'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -67,14 +67,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 100,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "dice_metric",
         "dataset_index": 0
     }
 }
```

## htc/models/patch/configs/default_64.json

### Pretty-printed

 * *Similarity: 0.9930555555555556%*

 * *Differences: {"'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -67,14 +67,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 100,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "dice_metric",
         "dataset_index": 0
     }
 }
```

## htc/models/pixel/DatasetPixelStream.py

```diff
@@ -30,67 +30,81 @@
     ...     print(sample['labels'].shape)
     ...     break
     torch.Size([5, 100])
     torch.Size([5])
     """
 
     def iter_samples(self) -> Iterator[dict[str, torch.Tensor]]:
-        for worker_index, path_index in self._iter_paths():
-            path = self.paths[path_index]
+        for worker_index, path in self._iter_paths():
             sample = self.read_experiment(path)
+
+            # # in case there are no valid pixels in this image.
+            # # this can happen if the label mapping has been modified for a data specification file, in such a way that there are some images left without any valid pixels
+            # if sample['valid_pixels'].sum() == 0:
+            #     settings.log.warning(f'Received a batch with no valid pixels (image info: {path.image_name()})')
+            #     continue
+
             sample = self.apply_transforms(sample)
 
             features = sample["features"].reshape(-1, sample["features"].shape[2])
+            labels = sample["labels"].reshape(-1)
 
             pixel_sampling = self.config.get("input/pixel_sampling", "proportional")
             if pixel_sampling == "proportional":
                 # Sample according to the number of valid pixels
                 n_image_pixels = None
             elif pixel_sampling == "uniform":
                 # Sample as many pixels as the image contains to avoid performance issues
                 n_image_pixels = features.size(0)
             else:
                 raise ValueError(f"Invalid pixel sampling value {pixel_sampling}")
 
-            if not self.config["input/no_labels"]:
-                labels = sample["labels"].reshape(-1)
+            # Relevant pixels for the classification
+            valid_pixels = sample["valid_pixels"].reshape(-1)
 
-                # Relevant pixels for the classification
-                valid_pixels = sample["valid_pixels"].reshape(-1)
+            if not self.config["input/all_pixels"]:
+                # Only return valid pixels
+                features = features[valid_pixels, :]
+                labels = labels[valid_pixels]
 
+            if self.config["input/specs_threshold"]:
+                specs = sample["specs"].reshape(-1)
                 if not self.config["input/all_pixels"]:
-                    # Only return valid pixels
-                    features = features[valid_pixels, :]
-                    labels = labels[valid_pixels]
-
-                # if there are no valid_pixels then skip this path
-                if labels.size(0) == 0:
-                    continue
+                    specs = specs[valid_pixels]
 
-            indices = torch.tensor(list(self._sample_pixel_indices(features, n_samples=n_image_pixels)))
+            # if there are no valid_pixels then skip this path
+            if labels.size(0) == 0:
+                continue
+
+            indices = torch.tensor(list(self._sample_pixel_indices(labels, n_samples=n_image_pixels)))
             index_split = torch.split(indices, self.batch_part_size)[
                 :-1
             ]  # The last split is ignored since it only contains the residual and its smaller
             worker_index_tensor = torch.ones(self.batch_part_size, dtype=torch.int64) * worker_index
-            image_name_tensor = torch.ones(self.batch_part_size, dtype=torch.int64) * path_index
+            image_name_tensor = torch.ones(self.batch_part_size, dtype=torch.int64) * self.image_names.index(
+                path.image_name()
+            )
             batches_per_image = 0
 
             for split in index_split:
                 sample = {
                     "features": features[split, :].refine_names("B", "C"),
+                    "labels": labels[split].refine_names("B"),
                     "worker_index": worker_index_tensor.refine_names("B"),
                     "image_index": image_name_tensor.refine_names("B"),
                 }
 
-                if not self.config["input/no_labels"]:
-                    sample["labels"] = labels[split].refine_names("B")
-                    if self.config["input/all_pixels"]:
-                        sample["valid_pixels"] = valid_pixels[split].refine_names("B")
+                if self.config["input/all_pixels"]:
+                    sample["valid_pixels"] = valid_pixels[split].refine_names("B")
+
+                if self.config["input/specs_threshold"]:
+                    sample["specs"] = specs[split].refine_names("B")
 
                 batches_per_image += 1
+
                 yield sample
 
                 if self.config.get("input/batches_per_image", np.inf) < batches_per_image:
                     break
 
     def n_image_elements(self) -> int:
         return self.paths[0].dataset_settings.pixels_image()
@@ -106,7 +120,10 @@
                     self._add_tensor_shared(f"labels_{name}", torch.int64)
                     if self.config["input/all_pixels"]:
                         self._add_tensor_shared(f"valid_pixels_{name}", torch.bool)
             else:
                 self._add_tensor_shared("labels", torch.int64)
                 if self.config["input/all_pixels"]:
                     self._add_tensor_shared("valid_pixels", torch.bool)
+
+        if self.config["input/specs_threshold"]:
+            self._add_tensor_shared("specs", torch.int64)
```

## htc/models/pixel/LightningPixel.py

```diff
@@ -73,20 +73,28 @@
         logits = self(features)  # Model prediction per pixel: x.shape = [307200, 15]
         logits = logits.reshape(batches, *spatial, -1)  # Go back to an image: x.shape = [1, 480, 640, 15]
         logits = logits.permute(0, 3, 1, 2)  # Channel first format (NCHW): x.shape = [1, 15, 480, 640]
 
         return {"class": logits}
 
     def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict:
-        prediction = self(batch["features"])
-        loss = self.ce_loss_weighted(prediction, batch["labels"])
+        if self.config["input/specs_threshold"]:
+            # Exclude specular highlight pixels
+            valid_pixels = batch["features"][~batch["specs"]]
+            valid_labels = batch["labels"][~batch["specs"]]
+        else:
+            valid_pixels = batch["features"]
+            valid_labels = batch["labels"]
+
+        prediction = self(valid_pixels)
+        loss = self.ce_loss_weighted(prediction, valid_labels)
 
         self.log("train/ce_loss", loss, on_epoch=True)
 
-        return {"loss": loss}
+        return {"loss": loss, "img_indices": torch.unique(batch["image_index"])}
 
     def encode_images(self, batch: dict[str, torch.Tensor]) -> list[torch.Tensor]:
         x = batch["features"]
         channels = x.shape[3]
 
         x = x.reshape(-1, channels)
```

## htc/models/pixel/configs/default.json

### Pretty-printed

 * *Similarity: 0.9930555555555556%*

 * *Differences: {"'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -60,14 +60,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 100,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "dice_metric",
         "dataset_index": 0
     }
 }
```

## htc/models/superpixel_classification/DatasetSuperpixelImage.py

```diff
@@ -40,22 +40,19 @@
             ).squeeze(dim=0)
             x_image = self.apply_transforms(x_image)  # [100, 32, 32]
 
             features.append(x_image)
 
         # Only include what is absolutely necessary in the sample
         sample = {
-            "image_size": torch.tensor(sample_img["features"].shape[:2]),
+            "labels": sample_img["labels"],
+            "valid_pixels": sample_img["valid_pixels"],
             "image_name": sample_img["image_name"],
             "features": torch.stack(features),
             "spxs_sizes": torch.tensor(spxs_sizes),
             "spxs_indices_rows": torch.cat(
                 spxs_indices_rows
             ),  # We already concatentate the ids since we make only full image assignments later
             "spxs_indices_cols": torch.cat(spxs_indices_cols),
         }
 
-        if "labels" in sample_img:
-            sample["labels"] = sample_img["labels"]
-            sample["valid_pixels"] = sample_img["valid_pixels"]
-
         return sample
```

## htc/models/superpixel_classification/DatasetSuperpixelStream.py

```diff
@@ -10,17 +10,16 @@
 from htc.models.common.HTCDatasetStream import HTCDatasetStream
 from htc.models.common.utils import get_n_classes
 from htc.models.image.DatasetImage import DatasetImage
 
 
 class DatasetSuperpixelStream(HTCDatasetStream):
     def iter_samples(self) -> Iterator[dict[str, torch.Tensor]]:
-        for worker_index, path_index in self._iter_paths():
-            # Explicitly load via DatasetImage since we need superpixels
-            path = self.paths[path_index]
+        for worker_index, path in self._iter_paths():
+            # We read the image like in DatasetImage
             sample_img = DatasetImage([path], train=self.train, config=self.config)[0]
 
             # Calculate the mode value per superpixel, i.e. find the label which occurs most often in the superpixel
             _, spxs_label_counts = spxs_predictions(
                 sample_img["spxs"],
                 sample_img["labels"],
                 sample_img["valid_pixels"],
@@ -62,15 +61,15 @@
                     features.float(), size=self.config["input/resize_shape"], mode="bilinear", align_corners=False
                 )
                 features = features.squeeze(dim=0).half()
 
                 yield {
                     "features": features.refine_names("C", "H", "W"),
                     "weak_labels": spx_weak_label.refine_names("N"),
-                    "image_index": path_index,
+                    "image_index": self.image_names.index(path.image_name()),
                     "worker_index": worker_index,
                 }
 
     def n_image_elements(self) -> int:
         return self.config["input/superpixels/n_segments"]
 
     def _add_shared_resources(self) -> None:
```

## htc/models/superpixel_classification/LightningSuperpixelClassification.py

```diff
@@ -47,16 +47,17 @@
     def val_dataloader(self, **kwargs) -> list[DataLoader]:
         return super().val_dataloader(batch_size=1, prefetch_factor=2, **kwargs)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         return self.model(x)
 
     def _predict_images(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
-        assert batch["features"].shape[0] == 1, "Can only handle one image at a time"
-        img_height, img_width = batch["image_size"][0]
+        assert batch["valid_pixels"].shape[0] == 1, "Can only handle one image at a time"
+        img_height = batch["valid_pixels"].shape[1]
+        img_width = batch["valid_pixels"].shape[2]
 
         predictions = self(batch["features"].squeeze(dim=0))
         predictions = predictions.repeat_interleave(
             batch["spxs_sizes"].squeeze(dim=0), dim=0
         )  # Repeat each classification value according to the superpixel size; [307200, 19]
         predictions = predictions.permute(1, 0)  # [19, 307200]
 
@@ -70,8 +71,8 @@
     def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict:
         predictions = self(batch["features"])
 
         predictions = F.log_softmax(predictions, dim=1)
         loss = self.kl_loss_weighted(predictions, batch["weak_labels"])
         self.log("train/kl_loss", loss, on_epoch=True)
 
-        return {"loss": loss}
+        return {"loss": loss, "img_indices": batch["image_index"]}
```

## htc/models/superpixel_classification/configs/default.json

### Pretty-printed

 * *Similarity: 0.9910714285714286%*

 * *Differences: {"'input'": "{'superpixels': {'slic_zero': True, 'sigma': 3}}",*

 * * "'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -11,15 +11,17 @@
         "preprocessing": "L1",
         "resize_shape": [
             32,
             32
         ],
         "superpixels": {
             "compactness": 10,
-            "n_segments": 1000
+            "n_segments": 1000,
+            "sigma": 3,
+            "slic_zero": true
         },
         "transforms_cpu": [
             {
                 "class": "KorniaTransform",
                 "degrees": 45,
                 "p": 0.5,
                 "padding_mode": "reflection",
@@ -67,14 +69,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 100,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "dice_metric",
         "dataset_index": 0
     }
 }
```

## htc/tissue_atlas/run_test_table_generation.py

```diff
@@ -42,15 +42,15 @@
         logits = data["logits"]
         if all_logits is None:
             all_logits = np.empty((len(folds), *logits.shape), dtype=np.float32)
 
         all_logits[i] = logits
 
     # all_logits.shape = [n_folds, n_samples, n_classes]
-    ensemble_mode = stats.mode(np.argmax(all_logits, axis=2), axis=0, keepdims=False).mode
+    ensemble_mode = stats.mode(np.argmax(all_logits, axis=2), axis=0).mode[0]
     ensemble_logits = np.mean(all_logits, axis=0)
     ensemble_softmax = np.mean(softmax(all_logits, axis=2), axis=0)
 
     df = {
         "label": labels,
         "ensemble_mode": ensemble_mode,
         "ensemble_logits": [
```

## htc/tissue_atlas/median_pixel/LightningMedianPixel.py

```diff
@@ -28,28 +28,26 @@
 
         if self.config["model/class_weight_method"] and self.dataset_train is not None:
             weights = calculate_class_weights(self.config, *self.dataset_train.label_counts())
         else:
             weights = torch.ones(get_n_classes(self.config))
 
         self.ce_loss_weighted = nn.CrossEntropyLoss(weight=weights)
-        self.validation_results_epoch = {"labels": [], "predictions": [], "image_names": []}
-        self.test_results_epoch = {"labels": [], "logits": [], "image_names": []}
 
     @staticmethod
     def dataset(**kwargs) -> HTCDataset:
         return DatasetMedianPixel(**kwargs)
 
     def train_dataloader(self) -> DataLoader:
         if self.config["input/oversampling"]:
             config = copy.copy(self.config)
             config["model/class_weight_method"] = "1m"  # This gives the "true" values needed for oversampling
-            config["model/background_weight"] = (
-                None  # Disable manual background weight as this could lead to a higher background sampling (since the 1/m values are usually smaller)
-            )
+            config[
+                "model/background_weight"
+            ] = None  # Disable manual background weight as this could lead to a higher background sampling (since the 1/m values are usually smaller)
 
             weights = calculate_class_weights(config, *self.dataset_train.label_counts())
             sample_weights = weights[self.dataset_train.labels]
             sampler = WeightedRandomSampler(sample_weights, num_samples=self.config["input/epoch_size"])
         else:
             sampler = RandomSampler(self.dataset_train, replacement=True, num_samples=self.config["input/epoch_size"])
 
@@ -66,30 +64,23 @@
 
         predictions = self(features)
         ce_loss = self.ce_loss_weighted(predictions, labels)
         self.log("train/ce_loss", ce_loss, on_epoch=True)
 
         return {"loss": ce_loss}
 
-    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:
-        if batch_idx == 0:
-            assert all(
-                len(values) == 0 for values in self.validation_results_epoch.values()
-            ), "Validation results are not properly cleared"
-
+    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict:
         predictions = self(batch["features"]).argmax(dim=1)
 
-        self.validation_results_epoch["labels"].append(batch["labels"])
-        self.validation_results_epoch["predictions"].append(predictions)
-        self.validation_results_epoch["image_names"].append(batch["image_name"])
-
-    def on_validation_epoch_end(self) -> None:
-        labels = torch.cat(self.validation_results_epoch["labels"])
-        predictions = torch.cat(self.validation_results_epoch["predictions"])
-        image_names = np.concatenate(self.validation_results_epoch["image_names"]).tolist()
+        return {"labels": batch["labels"], "predictions": predictions, "image_names": batch["image_name"]}
+
+    def validation_epoch_end(self, outputs: list[dict]) -> None:
+        labels = torch.cat([x["labels"] for x in outputs if x is not None])
+        predictions = torch.cat([x["predictions"] for x in outputs if x is not None])
+        image_names = np.concatenate([x["image_names"] for x in outputs if x is not None]).tolist()
 
         cm_pigs = confusion_matrix_groups(predictions, labels, image_names, get_n_classes(self.config))
 
         rows = []
         for subject_name, cm in cm_pigs.items():
             accuracy = accuracy_from_cm(cm)
             rows.append(
@@ -102,36 +93,24 @@
                     "confusion_matrix": cm.cpu().numpy(),
                 }
             )
 
         df_epoch = pd.DataFrame(rows)
         self.df_validation_results = pd.concat([self.df_validation_results, df_epoch])
         self.log_checkpoint_metric(self.df_validation_results["accuracy"].mean())
-        self.df_validation_results.to_pickle(Path(self.logger.save_dir) / "validation_results.pkl.xz")
-
-        # Start clean for the next validation round
-        self.validation_results_epoch = {"labels": [], "predictions": [], "image_names": []}
-
-    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:
-        if batch_idx == 0:
-            assert all(
-                len(values) == 0 for values in self.test_results_epoch.values()
-            ), "Test results are not properly cleared"
 
+    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict:
         labels = batch["labels"]
         features = batch["features"]
         image_names = batch["image_name"]
 
         logits = self(features)
 
-        self.test_results_epoch["labels"].append(labels)
-        self.test_results_epoch["logits"].append(logits)
-        self.test_results_epoch["image_names"].append(image_names)
+        return {"image_names": image_names, "labels": labels, "logits": logits}
 
-    def on_test_epoch_end(self) -> None:
+    def test_epoch_end(self, outputs: list[dict]) -> None:
         results = {}
-        results["logits"] = torch.cat(self.test_results_epoch["logits"]).cpu().numpy()
-        results["labels"] = torch.cat(self.test_results_epoch["labels"]).cpu().numpy()
-        results["image_names"] = np.concatenate(self.test_results_epoch["image_names"])
+        results["logits"] = torch.cat([x["logits"] for x in outputs if x is not None]).cpu().numpy()
+        results["labels"] = torch.cat([x["labels"] for x in outputs if x is not None]).cpu().numpy()
+        results["image_names"] = np.concatenate([x["image_names"] for x in outputs if x is not None])
 
         np.savez_compressed(Path(self.logger.save_dir) / "test_results.npz", **results)
-        self.test_results_epoch = {"labels": [], "logits": [], "image_names": []}
```

## htc/tissue_atlas/median_pixel/configs/default.json

### Pretty-printed

 * *Similarity: 0.9930555555555556%*

 * *Differences: {"'trainer_kwargs'": "{'precision': 16}"}*

```diff
@@ -33,14 +33,14 @@
     "swa_kwargs": {
         "annealing_epochs": 0
     },
     "trainer_kwargs": {
         "accelerator": "gpu",
         "devices": 1,
         "max_epochs": 10,
-        "precision": "16-mixed"
+        "precision": 16
     },
     "validation": {
         "checkpoint_metric": "accuracy",
         "dataset_index": 0
     }
 }
```

## htc/tissue_atlas_open/run_readme_gif.py

```diff
@@ -1,25 +1,42 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
+import argparse
 import io
 
 import matplotlib
 import matplotlib.pyplot as plt
 import numpy as np
 from PIL import Image
 from rich.progress import track
 
 from htc.cpp import map_label_image
 from htc.settings import settings
 from htc.tissue_atlas.settings_atlas import settings_atlas
 from htc.tivita.DataPath import DataPath
+from htc.utils.file_transfer import upload_file_s3
 from htc.utils.LabelMapping import LabelMapping
 
 if __name__ == "__main__":
+    parser = argparse.ArgumentParser(
+        description="Create the gif for the dataset README.",
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+    )
+    parser.add_argument(
+        "--upload",
+        default=False,
+        action="store_true",
+        help=(
+            "Upload the generated files to our S3 storage (requires that you have access and that the storage is"
+            " configured correctly)."
+        ),
+    )
+    args = parser.parse_args()
+
     # Select the example image
     path = DataPath.from_image_name("P088#2021_04_19_11_48_27")
     label_name = "peritoneum"
 
     # RGB with mask
     mapping_path = LabelMapping.from_path(path)
     label_index = mapping_path.name_to_index(label_name)
@@ -45,15 +62,15 @@
     # HSI gif
     imgs_channels = []
     cube = path.read_cube()
 
     # We are using the jet colormap to visualize visible range
     # The start index is a rough match to the color for 500 nm
     jet_start_index = 0.45
-    cmap_jet = matplotlib.colormaps.get_cmap("jet")
+    cmap_jet = matplotlib.cm.get_cmap("jet")
 
     for c in track(range(cube.shape[2])):
         # Map wavelengths to colors based on the jet colormap
         wavelength = c * 5 + 500
         if wavelength > 780:
             # We don't have colors anymore in the NIR, so we just use gray instead
             color = (0.4, 0.4, 0.4, 1.0)
@@ -83,7 +100,15 @@
         format="GIF",
         append_images=imgs_channels[1:],
         save_all=True,
         duration=200,
         loop=0,
         optimize=True,
     )
+
+    if args.upload:
+        link_rgb = upload_file_s3(local_path=rgb_file, remote_path="figures/HeiPorSPECTRAL_readme_example.png")
+        link_gif = upload_file_s3(local_path=gif_file, remote_path="figures/HeiPorSPECTRAL_readme_example.gif")
+
+        print("Links which can be used in the README:")
+        print(link_rgb)
+        print(link_gif)
```

## htc/tivita/DataPath.py

```diff
@@ -6,15 +6,14 @@
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Callable, Union
 
 import numpy as np
 import pandas as pd
 from PIL import Image
-from typing_extensions import Self
 
 from htc.settings import settings
 from htc.tivita.DatasetSettings import DatasetSettings
 from htc.utils.blosc_compression import decompress_file
 from htc.utils.Config import Config
 from htc.utils.LabelMapping import LabelMapping
 
@@ -100,15 +99,15 @@
         self.intermediates_dir = intermediates_dir
         if self.image_dir is not None:
             self.timestamp = self.image_dir.name
         self.annotation_name_default = annotation_name_default
 
         if (data_dir is None or intermediates_dir is None) and self.image_dir is not None:
             # Check whether the image directory is from a known location so that we can infer data/intermediates
-            entry = settings.datasets.find_entry(self.image_dir)
+            entry = settings.data_dirs.find_entry(self.image_dir)
             if entry is not None:
                 if data_dir is None:
                     self.data_dir = entry["path_data"]
                 if intermediates_dir is None:
                     self.intermediates_dir = entry["path_intermediates"]
 
         if dataset_settings is None:
@@ -134,21 +133,21 @@
     def __str__(self) -> str:
         return str(self.image_dir)
 
     def __repr__(self) -> str:
         return str(self)
 
     def __hash__(self) -> int:
-        return hash(self.image_name_annotations())
+        return hash(self.image_name())
 
-    def __eq__(self, other: Self) -> bool:
-        return self.image_name_annotations() == other.image_name_annotations()
+    def __eq__(self, other: "DataPath") -> bool:
+        return self.image_name() == other.image_name()
 
-    def __lt__(self, other: Self) -> bool:
-        return self.image_name_annotations() < other.image_name_annotations()
+    def __lt__(self, other: "DataPath") -> bool:
+        return self.image_name() < other.image_name()
 
     def cube_path(self) -> Path:
         """
         Path to the HSI data cube (*.dat file) for this image.
 
         >>> path = DataPath.from_image_name('P043#2019_12_20_12_38_35')
         >>> path.cube_path().name
@@ -314,16 +313,16 @@
         (480, 640)
 
         If you want to know the meaning of the label indices, you can use the dataset settings:
         >>> mapping = LabelMapping.from_path(path)
         >>> for l in np.unique(seg):
         ...     print(f"{l} = {mapping.index_to_name(l)}")
         4 = blue_cloth
-        13 = heart
-        18 = lung
+        11 = heart
+        15 = lung
 
         Args:
             annotation_name: Unique name of the annotation(s) for cases where multiple annotations exist (e.g. inter-rater variability). If None, will use the default set to this path or the default set in the dataset settings (in that order and only if available). If annotation_name is a string, it can also be in the form name1&name which will be treated identical to ['name1', 'name2']. If 'all', the original annotation file with all available annotations is returned.
 
         Returns: The segmentation mask of the image or None if no annotation is available. Is a dict if multiple annotations are requested.
         """
         path = self.segmentation_path()
@@ -388,131 +387,96 @@
 
         Returns: None if no mask could be found or a dictionary with information about the colorchecker image:
             - mask: Array of shape (height, width) with the label index for each pixel.
             - median_table: Table with median spectra (unnormalized and L1-normalized) for each color chip.
             - label_mapping: The label mapping object to interpret the values of the mask array.
         """
         mask_dir = self.image_dir / "annotations"
-        mask_paths = list(mask_dir.glob(f"{self.timestamp}#squares#automask#*.png"))  # searching for automasks
-        if len(mask_paths) == 0:
-            mask_paths = list(mask_dir.glob(f"{self.timestamp}#polygon#*.nrrd"))  # searching for MITK masks
+        mask_paths = list(mask_dir.glob(f"{self.timestamp}#squares#automask#*.png"))
         assert len(mask_paths) <= 1, f"Too many colorchecker masks available for {self.image_dir}"
 
         if len(mask_paths) == 0:
             settings.log.warning(
                 f"Colorchecker mask cannot be found for {self.image_dir}. Please refer to"
-                " ColorcheckerMaskCreation.ipynb or use MITK to generate the corresponding colorchecker mask!"
+                " ColorcheckerMaskCreation.ipynb to generate the corresponding colorchecker mask!"
             )
             return None
 
         else:
             mask_path = mask_paths[0]
+            mask = np.array(Image.open(mask_path))
 
-            from htc.utils.ColorcheckerReader import ColorcheckerReader
-
-            if mask_path.suffix == ".png":
-                mask = np.array(Image.open(mask_path))
-
-                cc_board = mask_path.name.split("#")[-1].removesuffix(".png")
-                assert cc_board in ["cc_classic", "cc_passport"], f"Unknown colorchecker board {cc_board} given!"
-                label_mapping_mask = (
-                    ColorcheckerReader.label_mapping_classic
-                    if cc_board == "cc_classic"
-                    else ColorcheckerReader.label_mapping_passport
-                )
+            cc_board = mask_path.name.split("#")[-1].removesuffix(".png")
+            assert cc_board in ["cc_classic", "cc_passport"], f"Unknown colorchecker board {cc_board} given!"
 
-            else:
-                from htc.utils.mitk.mitk_masks import nrrd_mask
+            from htc.utils.ColorcheckerReader import ColorcheckerReader
 
-                mask_dict = nrrd_mask(mask_path)
-                mask = mask_dict["mask"]
-                label_mapping_mitk = mask_dict["label_mapping"]
-
-                cc_board = mask_path.name.split("#")[-1].removesuffix(".nrrd")
-                assert cc_board in ["cc_classic", "cc_passport"], f"Unknown colorchecker board {cc_board} given!"
-                label_mapping_mask = (
-                    ColorcheckerReader.label_mapping_classic
-                    if cc_board == "cc_classic"
-                    else ColorcheckerReader.label_mapping_passport
-                )
-                # remap the MITK mask to the default format
-                label_dict = label_mapping_mask.mapping_name_index | {"unlabeled": 0}
-                label_mapping_tmp = LabelMapping(
-                    label_dict,
-                    zero_is_invalid=True,
-                    label_colors=label_mapping_mask.label_colors,
-                )
-                label_mapping_tmp.map_tensor(mask, label_mapping_mitk)
+            label_mapping = (
+                ColorcheckerReader.label_mapping_classic
+                if cc_board == "cc_classic"
+                else ColorcheckerReader.label_mapping_passport
+            )
 
             cube_norm = self.read_cube(normalization=1)
             cube = self.read_cube()
 
             table_rows = []
-            for i in np.unique(mask[mask != 0]):
+            for i in np.arange(1, np.max(mask) + 1):
                 spectra_norm = cube_norm[mask == i, :]
                 spectra = cube[mask == i, :]
                 table_rows.append(
                     {
                         "label_index": i,
-                        "label_name": label_mapping_mask.index_to_name(i),
-                        "label_color": label_mapping_mask.index_to_color(i),
+                        "label_name": label_mapping.index_to_name(i),
+                        "label_color": label_mapping.index_to_color(i),
                         "row": (i - 1) // 6,
                         "col": (i - 1) % 6,
                         "median_spectrum": np.median(spectra, axis=0),
                         "std_spectrum": np.std(spectra, axis=0),
                         "median_normalized_spectrum": np.median(spectra_norm, axis=0),
                         "std_normalized_spectrum": np.std(spectra_norm, axis=0),
                     }
                 )
             table = pd.DataFrame(table_rows)
 
-            res = {"mask": mask, "median_table": table, "label_mapping": label_mapping_mask}
+            res = {"mask": mask, "median_table": table, "label_mapping": label_mapping}
 
             if return_spectra:
                 # Add additional arrays with the raw spectral data for each color chip
-                _, counts = np.unique(mask[label_mapping_mask.is_index_valid(mask)], return_counts=True)
+                _, counts = np.unique(mask[label_mapping.is_index_valid(mask)], return_counts=True)
                 assert len(np.unique(counts)) == 1
                 n_rows = table["row"].max() + 1
                 n_cols = table["col"].max() + 1
                 n_samples = counts[0]
                 n_channels = cube.shape[-1]
 
                 spectra = np.empty((n_rows, n_cols, n_samples, n_channels), dtype=cube.dtype)
                 for i, row in table.iterrows():
                     spectra[row["row"], row["col"]] = cube[mask == row["label_index"], :]
 
                 if normalization is not None:
-                    with np.errstate(invalid="ignore"):
-                        spectra = spectra / np.linalg.norm(spectra, ord=normalization, axis=-1, keepdims=True)
-                        spectra = np.nan_to_num(spectra, copy=False)
+                    spectra = spectra / np.linalg.norm(spectra, ord=normalization, axis=-1, keepdims=True)
+                    spectra = np.nan_to_num(spectra, copy=False)
 
                 res["spectra"] = spectra
 
             return res
 
     def annotated_labels(self, annotation_name: Union[str, list[str]] = None) -> list[str]:
         """
         Gives a list of all label names which are part of the segmentation mask (based on the corresponding *.blosc file).
 
         >>> path = DataPath.from_image_name('P070#2020_07_25_00_29_02')
         >>> path.annotated_labels()
-        ['anorganic_artifact', 'fat_subcutaneous', 'foil', 'heart', 'lung', 'metal', 'muscle', 'organic_artifact', 'skin', 'unsure']
-
-        The annotated labels may be a subset of the image labels, if they exist:
-        >>> path.meta("image_labels")
-        ['anorganic_artifact', 'fat_subcutaneous', 'foil', 'heart', 'lung', 'metal', 'muscle', 'organic_artifact', 'skin', 'unsure']
-        >>> set(path.meta("image_labels")).issuperset(path.annotated_labels())
-        True
-
-        The image labels are weak labels and only indicate that a certain label is present in the image but they convey no information about whether the label is also annotated.
+        ['anorganic_artifact', 'fat', 'foil', 'heart', 'lung', 'metal', 'muscle', 'organic_artifact', 'skin', 'unsure']
 
         Args:
             annotation_name: Name of the annotation(s) passed on to read_segmentation(). If it refers to more than one annotation, the (sorted) unique set of all labels will be returned.
 
-        Returns: Sorted list of valid label names. If no valid labels are available, an empty list is returned.
+        Returns: Sorted list of valid label names. If no labels are available, an empty list is returned.
         """
         names = []
 
         label_mask = self.read_segmentation(annotation_name)
 
         if label_mask is not None:
             if type(label_mask) == dict:
@@ -541,15 +505,15 @@
         >>> sto2.shape
         (480, 640)
 
         The result is a masked array since automatic background detection (provided by Tivita) is applied to the cube. However, you can always access the raw data
         >>> sto2.data.shape
         (480, 640)
 
-        or the background mask separately if necessary. In the background mask, True indicates that the corresponding pixel is part of the background.
+        or the background mask separately if necessary
         >>> sto2.mask.shape
         (480, 640)
         >>> np.unique(sto2.mask)
         array([False,  True])
 
         Args:
             cube: If not None, will use this cube instead of loading it.
@@ -941,86 +905,29 @@
 
         >>> path = DataPath.from_image_name('P043#2019_12_20_12_38_35')
         >>> path.image_name_typed()
         {'subject_name': 'P043', 'timestamp': '2019_12_20_12_38_35'}
 
         Returns: Dictionary with the image_name parts (values) and its common names (keys).
         """
-        return dict(zip(self.image_name_parts(), self.image_name().split("#")))
-
-    def image_name_annotations(self) -> str:
-        """
-        Unique name of the image including the associated or default annotation name(s).
-
-        Note: A data path object may only be unique with its annotation name if more than one annotation type is used. Two data paths can have the same image name but different annotation names. This implication is important when using the image name as a unique identifier (e.g. in dictionaries or tables).
-
-        >>> path = next(DataPath.iterate(settings.data_dirs.semantic))
-        >>> path.image_name_annotations()
-        'P041#2019_12_14_12_00_16@semantic#primary'
-
-        Returns: Image name with annotation information appended.
-        """
-        name = self.image_name()
-
-        annotations = self.annotation_names()
-        if len(annotations) > 0:
-            name += "@" + "&".join(annotations)
-
-        return name
+        return {t: n for t, n in zip(self.image_name_parts(), self.image_name().split("#"))}
 
     def datetime(self) -> datetime:
         return datetime.strptime(self.timestamp, "%Y_%m_%d_%H_%M_%S")
 
-    def annotation_names(self) -> list[str]:
-        """
-        Returns the names of all associated annotations for this image.
-
-        The annotation name may be explicitly set when constructing the data path from the image name:
-        >>> path = DataPath.from_image_name("P091#2021_04_24_12_02_50@polygon#annotator1&polygon#annotator2")
-        >>> path.annotation_names()
-        ['polygon#annotator1', 'polygon#annotator2']
-
-        If no annotation name is explicitly set, the default annotation name from the dataset settings is used:
-        >>> path = DataPath.from_image_name("P091#2021_04_24_12_02_50")
-        >>> path.annotation_names()
-        ['polygon#annotator1']
-
-        The list of all possible annotation names can be retrieved via the `meta()` method:
-        >>> path.meta("annotation_name")
-        ['polygon#annotator1', 'polygon#annotator2', 'polygon#annotator3']
-
-        Returns: List of annotation names. If no annotation names are associated with this image (neither directly nor via the dataset settings), an empty list is returned.
-        """
-        if self.annotation_name_default is not None:
-            # Explicit default set to this class
-            if type(self.annotation_name_default) == str:
-                assert "&" not in self.annotation_name_default, "Multiple annotations should already be resolved."
-                return [self.annotation_name_default]
-            else:
-                return self.annotation_name_default
-        else:
-            name = self.dataset_settings.get("annotation_name_default")
-            if name is not None:
-                # Default via dataset settings
-                if type(name) == str:
-                    name = name.split("&")
-                return name
-            else:
-                return []
-
     @staticmethod
     def _build_cache(local: bool) -> dict[str, Any]:
         # We use a dict for the cache because it is much faster than a dataframe
         cache = {}
 
-        for env_key in settings.datasets.env_keys():
+        for env_key in settings.data_dirs.env_keys():
             if not env_key.upper().startswith("PATH_TIVITA"):
                 continue
 
-            entry = settings.datasets.get(env_key, local_only=local)
+            entry = settings.data_dirs.get(env_key, local_only=local, return_entry=True)
             if entry is None:
                 continue
 
             if (local and entry["location"] == "local") or (not local and entry["location"] == "network"):
                 table_path = list((entry["path_intermediates"] / "tables").glob("*@meta.feather"))
                 if len(table_path) > 0:
                     assert len(table_path) == 1, f"More than one meta table found for {entry}"
@@ -1061,43 +968,38 @@
             cache = DataPath._network_cache()
             if image_name not in cache:
                 return None
 
         return cache[image_name]
 
     @staticmethod
-    def from_image_name(image_name: str) -> Self:
+    def from_image_name(image_name: str) -> "DataPath":
         """
         Constructs a data path based on its unique identifier.
 
         This function can only be used if the corresponding dataset has a *meta.feather table with an overview of all paths of the dataset. This table can be created via the run_meta_table script.
 
-        Note: Data path objects created via this method are cached and exists globally during program execution. If the same `image_name` is used again, then the reference to the same object as before is returned.
-
         Args:
             image_name: Unique identifier of the path. Usually in the form subject#timestamp but you can also extend it to define the default annotations to read, for example subject#timestamp@name1&name2.
 
         Returns: The data path object.
         """
-        # The same path but with different annotation names should also be separate path objects since each object has its own annotation_name_default
-        cache_name = image_name
-
         if image_name not in DataPath._data_paths_cache:
             if "@" in image_name:
                 image_name, annotation_name = image_name.split("@")
                 annotation_name = annotation_name.split("&")
                 if len(annotation_name) == 1:
                     annotation_name = annotation_name[0]
             else:
                 annotation_name = None
 
             if image_name.startswith("ref"):
                 from htc.tivita.DataPathReference import DataPathReference
 
-                DataPath._data_paths_cache[cache_name] = DataPathReference.from_image_name(image_name, annotation_name)
+                DataPath._data_paths_cache[image_name] = DataPathReference.from_image_name(image_name, annotation_name)
             else:
                 match = DataPath._find_image(image_name)
                 assert match is not None, (
                     f"Could not find the path for the image {image_name} ({len(DataPath._local_cache()) = },"
                     f" {len(DataPath._network_cache()) = })"
                 )
 
@@ -1105,30 +1007,30 @@
                 if DataPathClass is None:
                     raise ValueError(
                         f"No known DataPath class for the dataset {match['dataset_env_name']}. Please make sure that"
                         " you have a dataset_settings.json file in your dataset which has a key data_path_class which"
                         " refers to a valid data path class (e.g. htc.tivita.DataPathMultiorgan>DataPathMultiorgan)"
                     )
 
-                DataPath._data_paths_cache[cache_name] = DataPathClass(
+                DataPath._data_paths_cache[image_name] = DataPathClass(
                     match["data_dir"] / match["path"],
                     match["data_dir"],
                     match["intermediates_dir"],
                     match["dsettings"],
                     annotation_name,
                 )
 
-        return DataPath._data_paths_cache[cache_name]
+        return DataPath._data_paths_cache[image_name]
 
     @staticmethod
     def iterate(
         data_dir: Path,
-        filters: Union[list[Callable[[Self], bool]], None] = None,
+        filters: Union[list[Callable[["DataPath"], bool]], None] = None,
         annotation_name: Union[str, list[str]] = None,
-    ) -> Iterator[Self]:
+    ) -> Iterator["DataPath"]:
         """
         Helper function to iterate over the folder structure of a dataset (e.g. subjects folder), yielding one image at a time.
 
         >>> paths = list(DataPath.iterate(settings.data_dirs.semantic))
         >>> len(paths)
         506
```

## htc/tivita/DataPathMultiorgan.py

```diff
@@ -54,15 +54,15 @@
         ```
         """
         super().__init__(*args, **kwargs)
         self.subject_name = self.image_dir.parent.name
 
         # For some files we have additional masks (e.g. overlap)
         self.parent_folder = self.image_dir.parents[2].name
-        self.is_overlap = any(self.parent_folder.startswith(x) for x in ["overlap"])
+        self.is_overlap = any([self.parent_folder.startswith(x) for x in ["overlap"]])
 
     def build_path(self, base_folder: Path) -> Path:
         return base_folder / self.subject_name / self.timestamp
 
     def image_name(self) -> str:
         name = f"{self.subject_name}#{self.timestamp}"
         if self.is_overlap:
@@ -92,15 +92,15 @@
     @staticmethod
     def iterate(
         data_dir: Path,
         filters: list[Callable[["DataPath"], bool]],
         annotation_name: Union[str, list[str]],
     ) -> Iterator["DataPathMultiorgan"]:
         dataset_settings = DatasetSettings(data_dir / "dataset_settings.json")
-        intermediates_dir = settings.datasets.find_intermediates_dir(data_dir)
+        intermediates_dir = settings.data_dirs.find_intermediates_dir(data_dir)
 
         # Multi-organ data
         for subject_name_path in sorted(data_dir.glob("subjects/*")):
             for image_dir in sorted(subject_name_path.iterdir()):
                 path = DataPathMultiorgan(image_dir, data_dir, intermediates_dir, dataset_settings, annotation_name)
-                if all(f(path) for f in filters):
+                if all([f(path) for f in filters]):
                     yield path
```

## htc/tivita/DataPathReference.py

```diff
@@ -1,22 +1,19 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 from collections.abc import Iterator
 from pathlib import Path
 from typing import Any, Callable, Union
 
-import numpy as np
 import pandas as pd
-from typing_extensions import Self
 
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.tivita.DatasetSettings import DatasetSettings
-from htc.utils.blosc_compression import decompress_file
 
 
 class DataPathReference(DataPath):
     _references_cache = None
 
     def __init__(self, network_path: Path, dataset_name: str, *args, **kwargs) -> None:
         """
@@ -26,19 +23,19 @@
         Note: This implementation relies on unique timestamps.
 
         Args:
             network_path: Relative path to the image on the network drive.
             dataset_name: Name of the dataset where the image comes from.
         """
         self.network_path = network_path
-        if settings.datasets.network_data is None:
+        if settings.data_dirs.network_data is None:
             super().__init__(None, *args, **kwargs)
             self.timestamp = self.network_path.name
         else:
-            super().__init__(settings.datasets.network_data / self.network_path, *args, **kwargs)
+            super().__init__(settings.data_dirs.network_data / self.network_path, *args, **kwargs)
 
         self.dataset_name = dataset_name
 
     def image_name(self) -> str:
         return f"ref#{self.dataset_name}#{self.timestamp}"
 
     def image_name_parts(self):
@@ -50,62 +47,44 @@
             "dataset_name": self.dataset_name,
             "timestamp": self.timestamp,
         }
 
     def build_path(self, base_folder: Path) -> Path:
         return base_folder / self.network_path
 
-    def rgb_path_reconstructed(self) -> Path:
-        if self.image_dir is None:
-            # image_dir may not be available if no network directory is set (e.g. on the cluster)
-            assert (
-                self.intermediates_dir is not None
-            ), "Either the network drive or the intermediates directory must be set to get the RGB image path"
-            return self.intermediates_dir / "preprocessing" / "rgb_reconstructed" / f"{self.image_name()}.blosc"
-        else:
-            return super().rgb_path_reconstructed()
-
-    def read_rgb_reconstructed(
-        self,
-    ) -> np.ndarray:
-        if self.image_dir is None:
-            return decompress_file(self.rgb_path_reconstructed())
-        else:
-            return super().read_rgb_reconstructed()
-
     @staticmethod
     def _cache() -> dict:
         if DataPathReference._references_cache is None:
             DataPathReference._references_cache = {}
 
             unsorted_dir = settings.data_dirs.unsorted
             df = pd.read_feather(unsorted_dir / "image_references.feather")
             dsettings = DatasetSettings(unsorted_dir / "dataset_settings.json")
 
             # The unsorted dataset has its own intermediates but the data dir always references the original dataset
-            intermediates_dir = settings.datasets.find_intermediates_dir(unsorted_dir)
+            intermediates_dir = settings.data_dirs.find_intermediates_dir(unsorted_dir)
 
             for image_name, network_path, dataset_name in zip(df["image_name"], df["network_path"], df["dataset_name"]):
-                if settings.datasets.network_data is None:
+                if settings.data_dirs.network_data is None:
                     data_dir = None
                 else:
-                    data_dir = settings.datasets.network_data / dataset_name / "data"
+                    data_dir = settings.data_dirs.network_data / dataset_name / "data"
 
                 DataPathReference._references_cache[image_name] = {
                     "network_path": Path(network_path),
                     "dataset_name": dataset_name,
                     "dsettings": dsettings,
                     "data_dir": data_dir,
                     "intermediates_dir": intermediates_dir,
                 }
 
         return DataPathReference._references_cache
 
     @staticmethod
-    def from_image_name(image_name: str, annotation_name: Union[str, list[str]]) -> Self:
+    def from_image_name(image_name: str, annotation_name: Union[str, list[str]]) -> "DataPathReference":
         cache = DataPathReference._cache()
         assert image_name in cache, f"Could not find the image {image_name} in the reference table"
 
         match = cache[image_name]
         return DataPathReference(
             match["network_path"],
             match["dataset_name"],
@@ -114,20 +93,20 @@
             match["dsettings"],
             annotation_name,
         )
 
     @staticmethod
     def iterate(
         data_dir: Path,
-        filters: list[Callable[[Self], bool]],
+        filters: list[Callable[["DataPath"], bool]],
         annotation_name: Union[str, list[str]],
     ) -> Iterator["DataPathReference"]:
         dataset_settings = DatasetSettings(data_dir / "dataset_settings.json")
         df_references = pd.read_feather(data_dir / "image_references.feather")
-        intermediates_dir = settings.datasets.find_intermediates_dir(data_dir)
+        intermediates_dir = settings.data_dirs.find_intermediates_dir(data_dir)
 
         for i, row in df_references.iterrows():
             path = DataPathReference(
                 row["network_path"], row["dataset_name"], data_dir, intermediates_dir, dataset_settings, annotation_name
             )
-            if all(f(path) for f in filters):
+            if all([f(path) for f in filters]):
                 yield path
```

## htc/tivita/DataPathSepsis.py

```diff
@@ -100,40 +100,40 @@
     @staticmethod
     def iterate(
         data_dir: Path,
         filters: list[Callable[["DataPath"], bool]],
         annotation_name: Union[str, list[str]],
     ) -> Iterator["DataPathSepsis"]:
         dataset_settings = DatasetSettings(data_dir / "dataset_settings.json")
-        intermediates_dir = settings.datasets.find_intermediates_dir(data_dir)
+        intermediates_dir = settings.data_dirs.find_intermediates_dir(data_dir)
 
         study_dir = data_dir / "hand_posture_study"
         for subject_name_path in sorted((study_dir / "healthy").iterdir()):
             for location_path in sorted(subject_name_path.iterdir()):
                 for image_dir in sorted(location_path.iterdir()):
                     path = DataPathSepsis(image_dir, data_dir, intermediates_dir, dataset_settings, annotation_name)
-                    if all(f(path) for f in filters):
+                    if all([f(path) for f in filters]):
                         yield path
 
         study_dir = data_dir / "sepsis_study"
         for health_status_path in sorted(study_dir.iterdir()):
             if health_status_path.name in ["annotations", "meta"]:
                 continue
 
             for subject_name_path in sorted(health_status_path.iterdir()):
                 if health_status_path.name == "healthy":
                     for location_path in sorted(subject_name_path.iterdir()):
                         for image_dir in sorted(location_path.iterdir()):
                             path = DataPathSepsis(
                                 image_dir, data_dir, intermediates_dir, dataset_settings, annotation_name
                             )
-                            if all(f(path) for f in filters):
+                            if all([f(path) for f in filters]):
                                 yield path
                 else:
                     for timepoint_path in sorted(subject_name_path.iterdir()):
                         for location_path in sorted(timepoint_path.iterdir()):
                             for image_dir in sorted(location_path.iterdir()):
                                 path = DataPathSepsis(
                                     image_dir, data_dir, intermediates_dir, dataset_settings, annotation_name
                                 )
-                                if all(f(path) for f in filters):
+                                if all([f(path) for f in filters]):
                                     yield path
```

## htc/tivita/DataPathTivita.py

```diff
@@ -1,17 +1,14 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import os
 from collections.abc import Iterator
 from pathlib import Path
 from typing import Callable, Union
 
-from typing_extensions import Self
-
 from htc.settings import settings
 from htc.tivita.DataPath import DataPath
 from htc.tivita.DatasetSettings import DatasetSettings
 
 
 class DataPathTivita(DataPath):
     def __init__(self, *args, **kwargs) -> None:
@@ -21,37 +18,36 @@
         super().__init__(*args, **kwargs)
         self.attributes = list(self.image_dir.relative_to(self.data_dir).parts[:-1])
 
     def build_path(self, base_folder: Path) -> Path:
         return base_folder / "/".join(self.attributes + [self.timestamp])
 
     @staticmethod
-    def from_image_name(image_name: str) -> Self:
+    def from_image_name(image_name: str) -> "DataPathTivita":
         raise NotImplementedError()
 
     @staticmethod
     def iterate(
         data_dir: Path,
-        filters: list[Callable[[Self], bool]],
+        filters: list[Callable[["DataPath"], bool]],
         annotation_name: Union[str, list[str]],
     ) -> Iterator["DataPathTivita"]:
         # Settings of the dataset (shapes etc.) can be referenced by the DataPaths
         path_settings = None
         possible_paths = [data_dir] + list(data_dir.parents)
         for p in possible_paths:
             if (p / "dataset_settings.json").exists():
                 path_settings = p / "dataset_settings.json"
                 break
 
         dataset_settings = DatasetSettings(path_settings)
-        intermediates_dir = settings.datasets.find_intermediates_dir(data_dir)
+        intermediates_dir = settings.data_dirs.find_intermediates_dir(data_dir)
 
-        # Keep a list of used image folders in case a folder contains both a cube file and a tiv archive
-        used_folders = set()
-        for root, dirs, files in os.walk(data_dir):
-            dirs.sort()  # Recurse in sorted order
-            for f in sorted(files):
-                if f.endswith(("SpecCube.dat", ".tiv")) and root not in used_folders:
-                    path = DataPathTivita(Path(root), data_dir, intermediates_dir, dataset_settings, annotation_name)
-                    if all(f(path) for f in filters):
-                        yield path
-                    used_folders.add(root)
+        parents = {}
+        for p in sorted(data_dir.rglob("*")):
+            if p.name.endswith("SpecCube.dat") or p.suffix == ".tiv":
+                parents[p.parent] = True
+
+        for p in parents.keys():
+            path = DataPathTivita(p, data_dir, intermediates_dir, dataset_settings, annotation_name)
+            if all([f(path) for f in filters]):
+                yield path
```

## htc/tivita/DatasetSettings.py

```diff
@@ -9,23 +9,15 @@
 
 from htc.utils.type_from_string import type_from_string
 
 
 class DatasetSettings:
     def __init__(self, path_or_data: Union[str, Path, dict]):
         """
-        Settings of the dataset (label_mapping, shape, etc.) defined in the dataset_settings.json file of the data folder. The data is not loaded when constructing this object but only when the settings data is accessed for the first time (lazy loading).
-
-        The dataset settings can be conveniently accessed via data paths:
-        >>> from htc import DataPath
-        >>> path = DataPath.from_image_name("P041#2019_12_14_12_00_16")
-        >>> path.dataset_settings["dataset_name"]
-        '2021_02_05_Tivita_multiorgan_semantic'
-        >>> path.dataset_settings["shape"]
-        (480, 640, 100)
+        Settings of the dataset (shapes etc.) which can be accessed by the DataPaths as path.dataset_settings. The data is not loaded when constructing this object but only when the settings data is actually accessed.
 
         It is also possible to load the settings explicitly based on the path to the data directory:
         >>> from htc.settings import settings
         >>> dsettings = DatasetSettings(settings.data_dirs.semantic)
         >>> dsettings["shape"]
         (480, 640, 100)
 
@@ -34,72 +26,56 @@
         """
         if isinstance(path_or_data, str):
             path_or_data = Path(path_or_data)
 
         if type(path_or_data) == dict:
             self._data = path_or_data
             self._data_conversions()
-            self._path = None
+            self.path = None
         else:
             self._data = None
-            self._path = path_or_data
+            self.path = path_or_data
 
     def __repr__(self) -> str:
-        res = (
-            "Settings for the dataset"
-            f" {self.settings_path.parent.name if self.settings_path is not None else '(no path available)'}\n"
-        )
+        res = f"Settings for the dataset {self.path.parent.name if self.path is not None else '(no path available)'}\n"
         res += "The following settings are available:\n"
         res += f"{list(self.data.keys())}"
 
         return res
 
     def __eq__(self, other: "DatasetSettings") -> bool:
         if self._data is None and other._data is None:
-            return self.settings_path == other.settings_path
+            return self.path == other.path
         else:
             return self.data == other.data
 
     def __getitem__(self, key: str) -> Any:
-        assert key in self.data, f"Cannot find {key} in the dataset settings\n{self.settings_path = }\n{self.data = }"
         return self.data[key]
 
     def get(self, key: str, default: Any = None) -> Any:
         return self.data[key] if key in self.data else default
 
     def __contains__(self, key: str) -> bool:
         return key in self.data
 
     @property
-    def settings_path(self) -> Union[None, Path]:
-        """
-        Returns: The Path to the dataset_settings.json file if it exists or None if not.
-        """
-        if self._path is None:
-            return None
-        else:
-            if self._path.exists():
-                p = self._path
-                if self._path.is_dir():
-                    p /= "dataset_settings.json"
-
-                return p if p.exists() else None
-            else:
-                return None
-
-    @property
-    def data(self) -> dict:
+    def data(self):
         if self._data is None:
-            if self.settings_path is None:
+            if self.path is None:
                 self._data = {}
-            else:
-                with self.settings_path.open(encoding="utf-8") as f:
+            elif self.path.exists():
+                if self.path.is_dir():
+                    self.path = self.path / "dataset_settings.json"
+
+                with self.path.open(encoding="utf-8") as f:
                     self._data = json.load(f)
 
                 self._data_conversions()
+            else:
+                self._data = {}
 
         return self._data
 
     def data_path_class(self) -> Union[type, None]:
         """
         Tries to infer the appropriate data path class for the current dataset. Ideally, this is defined in the dataset_settings.json file with a key data_path_class referring to a valid data path class (e.g. htc.tivita.DataPathMultiorgan>DataPathMultiorgan). If this is not the case, it tries to infer the data path class based on the dataset name or based on the files in the folder.
 
@@ -111,36 +87,32 @@
             from htc.tivita.DataPathMultiorgan import DataPathMultiorgan
 
             DataPathClass = DataPathMultiorgan
         elif "sepsis" in self.get("dataset_name", ""):
             from htc.tivita.DataPathSepsis import DataPathSepsis
 
             DataPathClass = DataPathSepsis
-        elif self._path is not None:
+        elif self.path is not None:
             # Try to infer the data path class from the files in the directory
-            if self._path.is_file() or not self._path.exists():
-                dataset_dir = self._path.parent
+            if self.path.is_file() or not self.path.exists():
+                dataset_dir = self.path.parent
             else:
-                dataset_dir = self._path
+                dataset_dir = self.path
             assert dataset_dir.exists() and dataset_dir.is_dir(), f"The dataset directory {dataset_dir} does not exist"
 
             files = sorted(dataset_dir.iterdir())
-            if any(f.name.startswith("Cat") for f in files):
-                from htc.tivita.DataPathTissueAtlas import DataPathTissueAtlas
-
-                DataPathClass = DataPathTissueAtlas
-            elif any(f.name.endswith("subjects") for f in files):
+            if any([f.name.endswith("subjects") for f in files]):
                 from htc.tivita.DataPathMultiorgan import DataPathMultiorgan
 
                 DataPathClass = DataPathMultiorgan
-            elif any(f.name == "sepsis_study" for f in files):
+            elif any([f.name == "sepsis_study" for f in files]):
                 from htc.tivita.DataPathSepsis import DataPathSepsis
 
                 DataPathClass = DataPathSepsis
-            elif any(f.stem == "image_references" for f in files):
+            elif any([f.stem == "image_references" for f in files]):
                 from htc.tivita.DataPathReference import DataPathReference
 
                 DataPathClass = DataPathReference
             else:
                 DataPathClass = None
         else:
             DataPathClass = None
```

## htc/tivita/hsi.py

```diff
@@ -1,26 +1,25 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 from pathlib import Path
-from typing import Union
 
 import numpy as np
 
 
 def tivita_wavelengths() -> np.array:
     """
     Returns: The wavelength for each channel of a Tivita image.
     """
     # "The spatial resolution can principally be in the megapixel range, but is reduced to a standard range of 640  480 pixel in practice, the     spectral resolution is approx. 5 nm in the range from 500 to 1000 nm, generating 100 spectral values"
     # https://pubmed.ncbi.nlm.nih.gov/29522415/
     return np.linspace(500, 1000, 100)
 
 
-def read_tivita_hsi(path: Path, normalization: Union[int, None] = None) -> np.ndarray:
+def read_tivita_hsi(path: Path, normalization: int = None) -> np.ndarray:
     """
     Load the Tivita data cube as Numpy array.
 
     Note: If possible, it is recommended to use the DataPath class instead of this (low-level) function. There, you can use the `read_cube()` method to read the cube.
 
     >>> from htc.settings import settings
     >>> from htc.tivita.DataPath import DataPath
@@ -36,19 +35,16 @@
         path: Path to the cube file (e.g. "[...]/2020_07_20_18_17_26/2020_07_20_18_17_26_SpecCube.dat").
         normalization: If not None, apply normalization to the data with the given order (e.g. normalization=1 equals L1 normalization, normalization=2 is L2 normalization).
 
     Returns: Cube data as array of shape (480, 640, 100).
     """
     assert path.exists() and path.is_file(), f"Data cube {path} does not exist or is not a file"
 
-    shape = np.fromfile(path, dtype=">i", count=3)  # Read shape of HSI cube
-    cube = np.fromfile(
-        path, dtype=">f", offset=12
-    )  # Read 1D array in big-endian binary format and ignore first 12 bytes which encode the shape
-    cube = cube.reshape(*shape)  # Reshape to data cube
+    cube = np.fromfile(path, dtype=">f")  # Read 1D array in big-endian binary format
+    cube = cube[3:].reshape(640, 480, 100)  # Reshape to data cube and ignore first 3 values which encode the shape
     cube = np.flip(cube, axis=1)  # Flip y-axis to match RGB image coordinates
 
     cube = np.swapaxes(cube, 0, 1)  # Consistent image shape (height, width)
     cube = cube.astype(np.float32)  # Consistently convert to little-endian
 
     if normalization is not None:
         cube = cube / np.linalg.norm(cube, ord=normalization, axis=2, keepdims=True)
```

## htc/tivita/metadata.py

```diff
@@ -1,12 +1,11 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import configparser
-import re
 import xml.etree.ElementTree as ET
 from collections.abc import Iterable
 from datetime import datetime
 from pathlib import Path
 
 import pandas as pd
 
@@ -91,17 +90,17 @@
     config.optionxform = str  # Avoid automatic lowercase transformation
     config.read(path, encoding="cp1252")  # ANSI encoding
 
     values = {}
     for section in config:
         for key in config[section]:
             value = config[section][key].strip('"')  # Remove quotes from strings: "Reflektanz" --> Reflektanz
-            if re.search(r"\d+,\d+", value) is not None:
+            if key == "Intensity Grenzwert":
                 value = value.replace(",", ".")  # 7,000000 --> 7.000000
-            if key == "Fremdlicht erkannt?":
+            elif key == "Fremdlicht erkannt?":
                 value = config[section].getboolean(key)
 
             # Convert strings to numbers if possible
             try:
                 value = pd.to_numeric(value)
             except Exception:
                 pass
```

## htc/tivita/rgb.py

```diff
@@ -1,11 +1,12 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 from pathlib import Path
+from typing import Any
 
 import numpy as np
 import torch
 from PIL import Image
 
 
 def read_tivita_rgb(path: Path, target_shape: tuple = (480, 640)) -> np.ndarray:
```

## htc/utils/ColorcheckerReader.py

```diff
@@ -1,26 +1,28 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
+import itertools
+
+import numpy as np
 import torch
 from PIL import Image
+from skimage.color import rgb2gray
+from skimage.transform import rotate
 
-from htc.cpp import colorchecker_automask, colorchecker_automask_search_area
 from htc.tivita.DataPath import DataPath
 from htc.utils.import_extra import requires_extra
 from htc.utils.LabelMapping import LabelMapping
 
 try:
     from deskew import determine_skew
-    from skimage.color import rgb2gray
-    from skimage.transform import rotate
 
     _missing_library = ""
 except ImportError:
-    _missing_library = "deskew and skimage"
+    _missing_library = "deskew"
 
 
 class ColorcheckerReader:
     label_colors_classic = {
         "dark_skin": "#735244",
         "light_skin": "#c29682",
         "blue_sky": "#627a9d",
@@ -135,190 +137,271 @@
     label_mapping_passport = LabelMapping(
         label_mapping_passport,
         zero_is_invalid=True,
         label_colors=label_colors_passport,
     )
 
     @requires_extra(_missing_library)
-    def __init__(
-        self,
-        img_dir: DataPath,
-        cc_board: str,
-        flipped: bool = False,
-        rot_angle: float = None,
-        square_size: int = None,
-        safety_margin: int = None,
-        square_dist_horizontal: int = None,
-        square_dist_vertical: int = None,
-    ):
+    def __init__(self, img_dir: DataPath, cc_board: str, rot_angle: float = None):
         """
         The purpose of this class is to generate annotation masks for hyperspectral colorchecker images.
 
         There is two annotation options:
         - `ColorcheckerReader.create_automask()`: Automatically generates an annotation mask.
         - `ColorcheckerReader.create_mask()`: Generates a custom annotation mask.
 
         >>> from htc.settings import settings
         >>> from htc.utils.ColorcheckerReader import ColorcheckerReader
         >>> from htc.tivita.DataPath import DataPath
         >>> cc_board = "cc_classic"
-        >>> img_dir = settings.data_dirs.studies / "2022_12_25_colorchecker_MIC1_TivitaHalogen/2022_12_25_20_45_29"
+        >>> img_dir = settings.data_dirs.studies / "2021_03_31_yellow_filter_colorchecker/0202-00118/2022_12_25_colorchecker_MIC1/2022_12_25_20_45_29"
         >>> img_dir.exists()
         True
         >>> img_dir = DataPath(img_dir)
         >>> cc_reader = ColorcheckerReader(img_dir, cc_board)
         >>> automask = cc_reader.create_automask()
         >>> custom_mask_params = dict(square_size=70, square_dist_horizontal=34, square_dist_vertical=30, offset_top=60, offset_left=24)
-        >>> custom_mask = cc_reader.create_mask({"mask_0": custom_mask_params})
+        >>> custom_mask = cc_reader.create_mask(custom_mask_params)
 
         Args:
             img_dir: DataPath to the image directory (timestamp folder).
             cc_board: String describing the type of colorchecker board which is either the[colorchecker classic](https://www.xrite.com/de/categories/calibration-profiling/colorchecker-classic), referred to as "cc_classic" or a combination of the [colorchecker classic mini](https://www.xrite.com/categories/calibration-profiling/colorchecker-classic-family/colorchecker-classic-mini) and the video color chips of the [colorchecker passport video](https://www.xrite.com/categories/calibration-profiling/colorchecker-passport-video), referred to as "cc_passport".
             rot_angle: The rotation of the colorchecker board is corrected. The rotation angle is automatically determined by default, but if the determined rotation angle is not satisfying, a custom rotation angle rot_angle (in degrees) can be input.
-            flipped: If the colorchecker board is upside-down, set flipped to True. The orientation of the mask will then be corrected.
-            square_size: The size of the squares of the colorchecker mask. Provide a custom value if the default is not satisfying.
-            safety_margin: The safety margin is the number of pixels added to the square size. TProvide a custom value if the default is not satisfying.
-            square_dist_horizontal: The horizontal distance between the squares of the colorchecker mask. Provide a custom value if the default is not satisfying.
-            square_dist_vertical: The vertical distance between the squares of the colorchecker mask. Provide a custom value if the default is not satisfying.
         """
         self.img_dir = img_dir
-        rgb_img = self.img_dir.read_rgb_reconstructed()
+        self.cube = torch.from_numpy(self.img_dir.read_cube())
+        self.img_height, self.img_width = self.cube.shape[0:2]
 
         if rot_angle is None:
+            rgb_img = self.img_dir.read_rgb_reconstructed()
             self.rot_angle = determine_skew(rgb2gray(rgb_img))
         else:
             self.rot_angle = rot_angle
 
         assert (
-            self.rot_angle is not None
-        ), "Rotation angle could not be determined as None was returned, doublechecking of the HSI cube is needed!"
-        assert (
             -30 <= self.rot_angle <= 30
         ), f"Rotation angle of {self.rot_angle} is not applied, doublechecking is needed!"
-
-        if flipped:
-            self.rot_angle = 180 + self.rot_angle
-
-        self.rot_rgb = torch.from_numpy(rotate(rgb_img, self.rot_angle, resize=False, mode="reflect")).float()
-        self.img_height, self.img_width = self.rot_rgb.shape[0:2]
+        self.rot_cube = torch.from_numpy(rotate(self.cube, self.rot_angle, resize=False, mode="reflect"))
+        self.rot_rgb = torch.from_numpy(
+            rotate(self.img_dir.read_rgb_reconstructed(), self.rot_angle, resize=False, mode="reflect")
+        )
 
         assert cc_board in [
             "cc_passport",
             "cc_classic",
         ], f"cc_board should be either cc_passport or cc_classic, but {cc_board} was given"
         self.cc_board = cc_board
 
         if self.cc_board == "cc_passport":
-            self.square_size = 26 if square_size is None else square_size
-            self.safety_margin = 12 if safety_margin is None else safety_margin
-            self.square_dist_horizontal = 36 if square_dist_horizontal is None else square_dist_horizontal
-            self.square_dist_vertical = 35 if square_dist_vertical is None else square_dist_vertical
+            self.square_size = 26
+            self.safety_margin = 12  # used to dilate the square size during mask optimization
+            self.square_dist_horizontal = 36 - self.safety_margin
+            self.square_dist_vertical = 35 - self.safety_margin
 
         if self.cc_board == "cc_classic":
-            self.square_size = 64 if square_size is None else square_size
-            self.safety_margin = 15 if safety_margin is None else safety_margin
-            self.square_dist_horizontal = 40 if square_dist_horizontal is None else square_dist_horizontal
-            self.square_dist_vertical = 36 if square_dist_vertical is None else square_dist_vertical
-
-        # The safety margin ensures that a solution more close to the center of the color chips is found
-        self.square_dist_horizontal -= self.safety_margin
-        self.square_dist_vertical -= self.safety_margin
+            self.square_size = 64
+            self.safety_margin = 12
+            self.square_dist_horizontal = 42 - self.safety_margin
+            self.square_dist_vertical = 38 - self.safety_margin
 
         self.mask_params = {}
 
     def create_mask(
         self,
-        mask_params: dict[str, dict[str, int]],
+        mask_params: dict[str, int],
     ) -> torch.Tensor:
         """
-        Generates an annotation mask from a given set of mask parameters. This is for example stored in `self.mas_params` after running `create_automask()`.
+        Generates a custom annotation mask from a given set of mask parameters.
 
         Args:
-            mask_params: Specifies the 5 mask parameters `offset_left` and `offset_top` (x- and y-coordinate of the origin of the colorchecker mask),  `square_size` (edge length of the square annotation for an individual color chip), `square_dist_horizontal` and`square_dist_vertical` (distance between neighboring square annotations along the x- and y-axis) (cf. htc/utils/colorchecker_mask_sketch.svg) for each mask, e.g. `{"mask_0": {"offset_left": 1}}`. There should be only "mask_0" for the colorchecker classic and "mask_0" and "mask_1" for the colorchecker passport.
+            mask_params: Specifies the 5 mask parameters `offset_left` and `offset_top` (x- and y-coordinate of the origin of the colorchecker mask),  `square_size` (edge length of the square annotation for an individual color chip), `square_dist_horizontal` and`square_dist_vertical` (distance between neighboring square annotations along the x- and y-axis) (cf. htc/utils/colorchecker_mask_sketch.svg).
 
         Returns: The custom annotation mask of the image.
         """
+        spatial_shape = self.img_dir.dataset_settings["spatial_shape"]
+        mask = torch.zeros(spatial_shape, dtype=torch.int32)
 
-        def mask_from_params(params: dict[str, int]) -> torch.Tensor:
-            mask = torch.zeros(self.img_height, self.img_width, dtype=torch.int32)
+        if self.cc_board == "cc_passport":
+            for row in range(4):
+                for col in range(6):
+                    idx = row * 6 + (5 - col) + 1
+                    left_min = mask_params["offset_left"] + row * (
+                        mask_params["square_size"] + mask_params["square_dist_horizontal"]
+                    )
+                    top_min = mask_params["offset_top"] + col * (
+                        mask_params["square_size"] + mask_params["square_dist_vertical"]
+                    )
+                    mask[
+                        top_min : top_min + mask_params["square_size"], left_min : left_min + mask_params["square_size"]
+                    ] = idx
 
-            if self.cc_board == "cc_passport":
-                for row in range(4):
-                    for col in range(6):
-                        idx = row * 6 + (5 - col) + 1
-                        left_min = params["offset_left"] + row * (
-                            params["square_size"] + params["square_dist_horizontal"]
-                        )
-                        top_min = params["offset_top"] + col * (params["square_size"] + params["square_dist_vertical"])
-                        mask[top_min : top_min + params["square_size"], left_min : left_min + params["square_size"]] = (
-                            idx
-                        )
-
-            else:
-                for row in range(4):
-                    for col in range(6):
-                        idx = row * 6 + col + 1
-                        top_min = params["offset_top"] + row * (params["square_size"] + params["square_dist_vertical"])
-                        left_min = params["offset_left"] + col * (
-                            params["square_size"] + params["square_dist_horizontal"]
-                        )
-
-                        mask[top_min : top_min + params["square_size"], left_min : left_min + params["square_size"]] = (
-                            idx
-                        )
-
-            if self.rot_angle is not None:
-                mask = rotate(mask.numpy(), -self.rot_angle, resize=False, mode="constant", cval=0, order=0)
-                mask = torch.from_numpy(mask)
+        else:
+            for row in range(4):
+                for col in range(6):
+                    idx = row * 6 + col + 1
+                    top_min = mask_params["offset_top"] + row * (
+                        mask_params["square_size"] + mask_params["square_dist_vertical"]
+                    )
+                    left_min = mask_params["offset_left"] + col * (
+                        mask_params["square_size"] + mask_params["square_dist_horizontal"]
+                    )
+
+                    mask[
+                        top_min : top_min + mask_params["square_size"], left_min : left_min + mask_params["square_size"]
+                    ] = idx
+
+        if self.rot_angle is not None:
+            mask = rotate(mask.numpy(), -self.rot_angle, resize=False, mode="constant", cval=0, order=0)
+            mask = torch.from_numpy(mask)
+
+        return mask
+
+    def compute_mask_score(
+        self, offset_left: int, offset_top: int, delta_horizontal: int, delta_vertical: int
+    ) -> float:
+        """
+        During automated annotation mask generation, this function computes a score for a given set of mask parameters. Across all mask parameter sets, the set with the smallest score will later be selected to generate an optimal automask. See htc.utils.colorchecker_mask_sketch.svg for a visualization of the mask parameters.
+
+        Args:
+            offset_left: x-coordinate of the origin of the colorchecker mask.
+            offset_top: y-coordinate of the origin of the colorchecker mask.
+            delta_horizontal: Deviation of the mask parameter square_dist_horizontal in pixels.
+            delta_vertical: Deviation of the mask parameter square_dist_vertical in pixels.
 
-            return mask
+        Returns: Score for the given set of mask parameters.
+        """
+        stds = torch.empty(24, self.rot_rgb.shape[-1], dtype=self.rot_rgb.dtype)
+        square_size = self.square_size + self.safety_margin
 
         if self.cc_board == "cc_passport":
-            assert len(mask_params) == 2, f"Two masks are supported for cc_passport, but {len(mask_params)} were given"
-            left_mask = mask_from_params(mask_params["mask_0"])
-            right_mask = mask_from_params(mask_params["mask_1"])
-            right_mask[right_mask > 0] = right_mask[right_mask > 0] + 24
+            for row in range(4):
+                for col in range(6):
+                    idx = row * 6 + col
+                    left_min = offset_left + row * (square_size + self.square_dist_horizontal + delta_horizontal)
+                    top_min = offset_top + col * (square_size + self.square_dist_vertical + delta_vertical)
+                    spectra = self.rot_rgb[top_min : top_min + square_size, left_min : left_min + square_size, :]
+                    assert spectra.shape == (square_size, square_size, 3), spectra.shape
+                    stds[idx, :] = torch.std(spectra, dim=(0, 1))
 
-            return left_mask + right_mask
         else:
-            assert (
-                len(mask_params) == 1
-            ), f"Only one mask is supported for cc_classic, but {len(mask_params)} were given"
-            return mask_from_params(mask_params["mask_0"])
+            for row in range(4):
+                for col in range(6):
+                    idx = row * 6 + col
+                    top_min = offset_top + row * (square_size + self.square_dist_vertical + delta_vertical)
+                    left_min = offset_left + col * (square_size + self.square_dist_horizontal + delta_horizontal)
+                    spectra = self.rot_rgb[top_min : top_min + square_size, left_min : left_min + square_size, :]
+                    stds[idx, :] = torch.std(spectra, dim=(0, 1))
 
-    def create_automask(self) -> torch.Tensor:
+        assert not stds.isnan().any()
+        return stds.max(dim=1).values.sum().numpy()  # emphasize consistency across worst spectral channel
+
+    def automask_helper(
+        self,
+        offset_left_range: np.ndarray,
+        offset_top_range: np.ndarray,
+        deltas_horizontal_range: np.ndarray,
+        deltas_vertical_range: np.ndarray,
+    ) -> torch.Tensor:
         """
-        Automatically create an annotation mask for the colorchecker image. Only works for "regular" images without distortions (viewed from the top) and only if all colorchecker chips are visible.
+        During the automated annotation mask generation, the mask parameters will be varied in the given ranges to generate an optimal annotation mask. See htc.utils.colorchecker_mask_sketch.svg for a visualization of the mask parameters.
+
+        Args:
+            offset_left_range: Range in which x-coordinate of the origin of the colorchecker mask will be varied.
+            offset_top_range: Range in which y-coordinate of the origin of the colorchecker mask will be varied.
+            deltas_horizontal_range: Range in which deviation from `square_dist_horizontal` will be varied.
+            deltas_vertical_range: Range in which deviation from `square_dist_vertical` will be varied.
 
-        Returns: Annotation mask of the image which is optimal according to the deviation score. The estimated paramters are stored in this class in the `self.mas_params` attribute.
+        Returns: Annotation mask of the image which is optimal according to the score.
         """
-        self.mask_params = colorchecker_automask(
-            self.rot_rgb,
-            cc_board=self.cc_board,
-            square_size=self.square_size,
-            safety_margin=self.safety_margin,
-            square_dist_horizontal=self.square_dist_horizontal,
-            square_dist_vertical=self.square_dist_vertical,
+        offsets_left = []
+        offsets_top = []
+        deltas_horizontal = []
+        deltas_vertical = []
+        for ol, ot, dh, dv in itertools.product(
+            offset_left_range, offset_top_range, deltas_horizontal_range, deltas_vertical_range
+        ):
+            offsets_left.append(ol)
+            offsets_top.append(ot)
+            deltas_horizontal.append(dh)
+            deltas_vertical.append(dv)
+
+        # Note: using p_map here leads to multiprocessing issues when running from a Jupyter notebook
+        overall_stds = []
+        for ol, ot, dh, dv in zip(offsets_left, offsets_top, deltas_horizontal, deltas_vertical):
+            overall_stds.append(self.compute_mask_score(ol, ot, dh, dv))
+
+        opt_idx = np.argmin(overall_stds)
+        mask_id = f"mask_{len(self.mask_params.keys())}"
+        self.mask_params[mask_id] = {}
+        self.mask_params[mask_id]["offset_left"] = offsets_left[opt_idx] + int(self.safety_margin / 2)
+        self.mask_params[mask_id]["offset_top"] = offsets_top[opt_idx] + int(self.safety_margin / 2)
+        self.mask_params[mask_id]["square_size"] = self.square_size
+        self.mask_params[mask_id]["square_dist_horizontal"] = (
+            self.square_dist_horizontal + self.safety_margin + deltas_horizontal[opt_idx]
         )
-        return self.create_mask(self.mask_params)
+        self.mask_params[mask_id]["square_dist_vertical"] = (
+            self.square_dist_vertical + self.safety_margin + deltas_vertical[opt_idx]
+        )
+
+        return self.create_mask(self.mask_params[mask_id])
 
-    def automask_search_area(self) -> torch.Tensor:
+    def create_automask(self) -> torch.Tensor:
         """
-        Returns the search area for the automatic mask generation. This is the area where squares for the color chips are placed during optimization.
+        Automated annotation mask generation.
 
-        Returns: A count map denoting the number of times each pixel was selected as the origin (top left corner) of a color chip area.
+        Returns: Annotation mask of the image which is optimal according to the score.
         """
-        return colorchecker_automask_search_area(
-            self.rot_rgb,
-            cc_board=self.cc_board,
-            square_size=self.square_size,
-            safety_margin=self.safety_margin,
-            square_dist_horizontal=self.square_dist_horizontal,
-            square_dist_vertical=self.square_dist_vertical,
-        )
+        if self.cc_board == "cc_passport":
+            deltas_horizontal = np.arange(-1, 1, 1)
+            deltas_vertical = np.arange(-1, 2, 1)
+            # optimize left part of mask
+            # max offset top set to make sure the mask will not exceed the image boundaries
+            ot_max = (
+                self.img_height
+                - 6 * (self.square_size + self.safety_margin)
+                - 5 * (self.square_dist_vertical + np.max(deltas_vertical))
+            )
+            offsets_left = np.arange(0, 50, 2)
+            offsets_top = np.arange(0, ot_max, 2)
+            left_mask = self.automask_helper(offsets_left, offsets_top, deltas_horizontal, deltas_vertical)
+
+            # optimize right part of mask
+            # max offset left and top set to make sure the mask will not exceed the image boundaries
+            ol_min = np.max(np.where(left_mask == 24)) + 100
+            ol_max = (
+                self.img_width
+                - 4 * (self.square_size + self.safety_margin)
+                - 3 * (self.square_dist_horizontal + np.max(deltas_horizontal))
+            )
+            assert ol_min < ol_max, "Right part of mask seems not to fit entirely into image frame"
+            offsets_left = np.arange(ol_min, ol_max, 2)
+            right_mask = self.automask_helper(offsets_left, offsets_top, deltas_horizontal, deltas_vertical)
+            right_mask[right_mask > 0] = right_mask[right_mask > 0] + 24
+
+            automask = left_mask + right_mask
+
+        else:
+            deltas_horizontal = np.arange(-2, 2, 2)
+            deltas_vertical = np.arange(-2, 2, 2)
+            # max offset left and top set to make sure the mask will not exceed the image boundaries
+            ol_max = (
+                self.img_width
+                - 6 * (self.square_size + self.safety_margin)
+                - 5 * (self.square_dist_horizontal + np.max(deltas_horizontal))
+            )
+            ot_max = (
+                self.img_height
+                - 4 * (self.square_size + self.safety_margin)
+                - 3 * (self.square_dist_vertical + np.max(deltas_vertical))
+            )
+            offsets_left = np.arange(0, ol_max, 2)
+            offsets_top = np.arange(0, ot_max, 2)
+            automask = self.automask_helper(offsets_left, offsets_top, deltas_horizontal, deltas_vertical)
+
+        return automask
 
     def save_mask(self, mask: torch.Tensor) -> None:
         """
         Saves a given annotation mask.
 
         Args:
             mask: Annotation mask which was generated either automatically or custom.
```

## htc/utils/Config.py

```diff
@@ -1,52 +1,38 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import copy
 import functools
 import json
 import pprint
+import re
 from collections.abc import Iterator
 from multiprocessing import Manager
 from pathlib import Path
 from typing import Any, Callable, Union
 
 import commentjson
-from typing_extensions import Self
 
 from htc.settings import settings
 from htc.utils.AdvancedJSONEncoder import AdvancedJSONEncoder
 from htc.utils.general import merge_dicts_deep
-from htc.utils.unify_path import unify_path
 
 
 # Decorator to count key usage (useful to determine which keys have not been used during program execution)
 def track_key_usage(method: Callable) -> Callable:
     @functools.wraps(method)
     def _track_key_usage(self, identifier: str, *args, **kwargs):
         if self._used_keys is not None:
             self._used_keys[identifier] = 1  # We re-use a dict, but we are only interested in the unique set of keys
         return method(self, identifier, *args, **kwargs)
 
     return _track_key_usage
 
 
-def get_possible_paths(path: Path) -> list[Path]:
-    return [
-        # Relative/absolute
-        path,
-        # Relative to the model's dir (name of the model must be provided, though, e.g. image/configs/default)
-        settings.models_dir / path,
-        # Relative to the htc package directory
-        settings.htc_package_dir / path,
-        # Relative to the src directory
-        settings.src_dir / path,
-    ]
-
-
 class Config:
     def __init__(self, path_or_dict: Union[str, Path, dict], use_shared_dict=False):
         """
         This class can be used to work with the configuration files. It can be used as a dict but has some conveniences like accessing nested dicts via 'key1/key2' identifier. For the semantic meaning of common attributes, please take a look at the config.schema file.
 
         This class uses the commentjson library (https://github.com/vaidik/commentjson) to load the json configuration file. This means that comments are supported in the configuration file. However, they are lost when the configuration file is saved to a file.
 
@@ -68,62 +54,47 @@
 
         >>> config['input/preprocessing'] = None  # We are making the default config identical to the rgb config just for demonstration purposes
         >>> config['input/n_channels'] = 3        # These settings are changed by the rgb config
         >>> config['config_name'] = 'default_rgb'
         >>> config == config_rgb  # The rgb config is now identical to the default config from which it inherits
         True
 
-        For list config values inheritance is difficult as usually the user wants to append to the list. To achieve this, you can add a config value with the same key as the original list but with `_extends` appended. Those values will then be merged:
-        >>> config = Config(
-        ...     {
-        ...         "input/my_list": [1, 2],
-        ...         "input/my_list_extends": [3],
-        ...     }
-        ... )
-        >>> config["input/my_list"]
-        [1, 2, 3]
-
         Config objects can also be constructed from existing dictionaries:
 
         >>> Config({'a/b': 1, 'c': 3})
         {'a': {'b': 1}, 'c': 3}
         >>> Config({'a/b/c': 'str_value1', 'd': {'x': 'str_value2'}})
         {'a': {'b': {'c': 'str_value1'}}, 'd': {'x': 'str_value2'}}
 
         Note: If you want to make a copy of the config to change values (without affecting the existing config), you can use copy.copy() which gives you a copy of all keys pointing to builtins (e.g. strings) but only copies the reference to objects like a data specification file (which is much better for performance). You should only deepcopy a config file if you want to make changes to e.g. the references data specs.
 
         Args:
-            path_or_dict: Path (or string) to the configuration file to load or a dictionary with the data. If a path, several common locations are searched:
-            * absolute/relative
-            * relative to the model's directory (e.g. image/configs/default.json)
-            * relative to the htc package directory (e.g. models/image/configs/default.json)
-            * relative to the repository root (e.g. htc/models/image/configs/default.json)
-
+            path_or_dict: Path (or string) to the configuration file to load or a dictionary with the data.
             use_shared_dict: If True, then a shared dictionary is used to track key usage (via multiprocessing.Manager). This is necessary to get correct usage statistics when multiprocessing is used. However, the manager object creates a subprocess and this may lead to problems if the config is created inside a processing pool as daemonic processes are not allowed to have children. The default is to use a standard Python dictionary. Note: if correct usage statistics are needed inside a processing pool, then the ProcessPoolExecutor might be an option (https://stackoverflow.com/a/61470465/2762258).
         """
         if isinstance(path_or_dict, str):
             path_or_dict = Path(path_or_dict)
 
         self.path_config = None
         self._used_keys = None
 
         if isinstance(path_or_dict, Path):
-            # Add .json extension if the user forgot it
-            if not path_or_dict.name.endswith(".json"):
-                path_or_dict = path_or_dict.with_name(path_or_dict.name + ".json")
+            # Load from file
+            self.path_config = path_or_dict
 
-            # Find the location to the config file
-            for p in get_possible_paths(path_or_dict):
-                if p.exists():
-                    self.path_config = unify_path(p)
-                    break
-            assert self.path_config is not None, (
-                f"Cannot find the config file at {self.path_config}. Please make sure that the file exists at this"
-                " location"
-            )
+            # Add .json extension if the user forgot it
+            if not self.path_config.exists():
+                with_extension = self.path_config.with_name(self.path_config.name + ".json")
+                if with_extension.exists():
+                    self.path_config = with_extension
+                else:
+                    raise ValueError(
+                        f"Cannot find the config file at {self.path_config}. Please make sure that the file exists at"
+                        " this location"
+                    )
 
             with self.path_config.open() as fp:
                 self.data = commentjson.load(fp)
 
             self["config_name"] = self.path_config.stem
         else:
             self.data = {}
@@ -132,19 +103,26 @@
                     # a/b style
                     self[k] = v
                 else:
                     # Standard dict
                     self.data[k] = v
 
         if self["inherits"]:
-            extension = "" if self["inherits"].endswith(".json") else ".json"
+            match = re.search(r"\.[^.]+$", self["inherits"])
+            extension = ".json" if match is None else ""
             inherits = Path(self["inherits"] + extension)
 
             # We try several locations to find the parent config file
-            possible_paths = get_possible_paths(inherits)
+            possible_paths = [
+                inherits,  # Relative/absolute
+                settings.models_dir
+                / inherits,  # Relative to the model's dir (name of the model must be provided, though, e.g. image/configs/default)
+                settings.htc_package_dir / inherits,  # Relative to the htc package directory
+                settings.src_dir / inherits,  # Relative to the src directory
+            ]
             if self.path_config is not None:
                 possible_paths.append(self.path_config.with_name(inherits.name))  # Same directory as the child config
 
             parent_path = None
             for path in possible_paths:
                 if path.exists():
                     parent_path = path
@@ -157,29 +135,14 @@
             data_parent = Config(parent_path).data
 
             # The existing data (=data from the child) has precedence over the parent data
             self.data = dict(merge_dicts_deep(data_parent, self.data))
 
             del self["inherits"]
 
-        # Users can extend additional lists by adding the same key with _extends appended
-        for k, v in self.items():
-            if k.endswith("_extends") and type(v) == list:
-                k_original = k.removesuffix("_extends")
-                assert k_original in self, (
-                    f"The extends key {k} is in the config but not the original version (without extends, i.e."
-                    f" {k_original})"
-                )
-                assert type(self[k_original]) == list, (
-                    f"The original key {k_original} is not a list but a {type(self[k_original])} which is not supported"
-                    " for the extends feature"
-                )
-                self[k_original] = self[k_original] + v
-                del self[k]
-
         # We start counting usage from here on
         if use_shared_dict:
             # The key tracking should also work in multiprocessing environments (e.g. network training)
             self._used_keys = Manager().dict()
         else:
             self._used_keys = {}
 
@@ -209,19 +172,21 @@
         config_new.data = copy_data(self.data)
         config_new.path_config = self.path_config
         config_new._used_keys = self._used_keys
 
         return config_new
 
     def used_keys(self) -> list[str]:
-        """Returns: List of all keys which have been accessed from this config (either via getter, setter, contains check or deletion). Nested keys are not in the list if only the top-level key has been used (e.g. 'a/b' and 'a/c are not in the list if only 'a' is accessed)."""
+        """Returns: List of all keys which have been accessed from this config (either via getter, setter, contains check or deletion). Nested keys are not in the list if only the top-level key has been used (e.g. 'a/b' and 'a/c are not in the list if only 'a' is accessed).
+        """
         return sorted(self._used_keys.keys())
 
     def unused_keys(self) -> list[str]:
-        """Returns: List of keys from this config which have never been used (either via getter, setter, contains or deletion)."""
+        """Returns: List of keys from this config which have never been used (either via getter, setter, contains or deletion).
+        """
         # First determine all used keys including nested keys
         all_used_keys = set(self._used_keys.keys())
         for used_key in self._used_keys:
             for k in self.keys():
                 if k.startswith(f"{used_key}/"):
                     all_used_keys.add(k)
 
@@ -239,15 +204,15 @@
             return False
 
     @track_key_usage
     def __contains__(self, identifier: str) -> bool:
         """
         Checks if the configuration has a certain key.
 
-        >>> config = Config.from_model_name(model_name='pixel')
+        >>> config = Config.load_config(model_name='pixel')
         >>> 'model/activation_function' in config
         True
         >>> 'model' in config
         True
         >>> 'lr' in config
         False
         """
@@ -278,15 +243,15 @@
 
         return last_data
 
     def get(self, identifier: str, default=None):
         """
         Same as __getitem__ but allowing a default value in case the identifier does not exist.
 
-        >>> config = Config.from_model_name(model_name='image')
+        >>> config = Config.load_config(model_name='image')
         >>> config.get('non_existing_key',1.0)
         1.0
 
         Args:
             identifier: Key to search for.
             default: Default value in case the key does not exist.
 
@@ -298,15 +263,15 @@
             return default
 
     @track_key_usage
     def __setitem__(self, identifier: str, value) -> None:
         """
         Change the value of a configuration.
 
-        >>> config = Config.from_model_name(model_name='pixel')
+        >>> config = Config.load_config(model_name='pixel')
         >>> config['model/lr'] = 0.1
         >>> config['model/lr']
         0.1
         """
         keys = identifier.split("/")
 
         last_data = self.data
@@ -320,15 +285,15 @@
                 last_data = last_data[key]
 
     @track_key_usage
     def __delitem__(self, identifier: str) -> None:
         """
         Removes an identifier from the configuration.
 
-        >>> config = Config.from_model_name(model_name='pixel')
+        >>> config = Config.load_config(model_name='pixel')
         >>> del config['dataloader_kwargs/batch_size']
         >>> config.keys()[:2]
         ['config_name', 'dataloader_kwargs/num_workers']
         >>> config = Config({'test': 1})
         >>> del config['test']
         >>> config.keys()
         []
@@ -344,15 +309,15 @@
 
             del last_data[keys[-1]]
 
     def keys(self) -> list[str]:
         """
         List of all (nested) keys in the configuration.
 
-        >>> config = Config.from_model_name(model_name='pixel')
+        >>> config = Config.load_config(model_name='pixel')
         >>> config.keys()[:2]
         ['config_name', 'dataloader_kwargs/batch_size']
 
         Returns: List of keys.
         """
 
         def get_keys(dict_data, prefix=""):
@@ -379,60 +344,65 @@
     def save_config(self, path: Path) -> None:
         with path.open("w") as fp:
             commentjson.dump(self.data, fp, indent=4, sort_keys=True, cls=AdvancedJSONEncoder)
 
         self.path_config = path
 
     @classmethod
-    def from_model_name(cls, config_name: Union[Path, str] = None, model_name: str = None, **config_kwargs) -> Self:
+    def load_config(cls, path: Union[Path, str] = None, model_name: str = None, **config_kwargs) -> "Config":
         """
-        Load the configuration file for a model. Several common locations are searched:
+        Tries to find the configuration file in several common locations:
 
             * Relative or absolute path.
             * Relative to settings.models_dir.
             * Relative to settings.htc_package_dir.
             * Relative to the model's config folder (inside settings.models_dir).
 
-        >>> config = Config.from_model_name("default", "image")
+        >>> config = Config.load_config("default", "image")
         >>> config["input/n_channels"]
         100
 
         Args:
-            config_name: Name or absolute/relative path to the configuration file. If None, loads the default configuration file.
+            path: Name or absolute/relative path to the configuration file. If None, loads the default configuration file.
             model_name: Name of the model inside settings.models_dir (e.g. pixel). If not None, will be used as additional search path.
             config_kwargs: Additional keyword arguments passed on to the Config constructor.
 
         Returns: The configuration object.
         """
-        if config_name is None:
-            config_name = "default.json"
+        if path is None:
+            path = "default.json"
 
-        path_config = Path(config_name)
-        if not path_config.name.endswith(".json"):
-            path_config = path_config.with_name(path_config.name + ".json")
+        path_config = Path(path)
+        if path_config.suffix == "":
+            path_config = path_config.with_suffix(".json")
+
+        possible_paths = [
+            path_config,
+            settings.models_dir / path_config,
+            settings.htc_package_dir / path_config,
+        ]
 
-        possible_paths = get_possible_paths(path_config)
         if model_name is not None:
             possible_paths.append(settings.models_dir / model_name / "configs" / path_config)
 
         config = None
         for p in possible_paths:
             if p.exists():
                 config = Config(p, **config_kwargs)
                 break
 
         if config is None:
             raise ValueError(
-                f"Cannot find the configuration file {config_name}. Tried the following locations: {possible_paths}"
+                f"Cannot find the configuration file {path}. Tried the following locations: {possible_paths}"
             )
 
         return config
 
     @classmethod
-    def from_fold(cls, experiment_folder: Path) -> Self:
+    def load_config_fold(cls, experiment_folder: Path) -> "Config":
         """
         Loads a config.json file from an experiment consisting of multiple config files (in corresponding subdirectories for each fold). The config files from all folds will be read and it is checked that all configs are identical and that a config file exists for each fold.
 
         Args:
             experiment_folder: Path to the experiment folder or path to a folder which contains a config.json file.
 
         Returns: The configuration object.
```

## htc/utils/DomainMapper.py

```diff
@@ -2,15 +2,14 @@
 # SPDX-License-Identifier: MIT
 
 import random
 from typing import Any, Union
 
 import numpy as np
 import pandas as pd
-from typing_extensions import Self
 
 from htc.models.data.DataSpecification import DataSpecification
 from htc.tivita.DataPath import DataPath
 from htc.utils.Config import Config
 from htc.utils.helper_functions import median_table
 
 
@@ -91,15 +90,15 @@
         if domain_colors is None:
             domain_colors = {domain: "#FFFFFF" for domain in domains}
 
         # baseline case where the domain ids are shuffled, to check if the domain task is acting as a regularization
         if self.shuffle_domains:
             mappings = list(domain_mapping.values())
             random.Random(0).shuffle(mappings)  # Does not affect global seed
-            domain_mapping = dict(zip(list(domain_mapping.keys()), mappings))
+            domain_mapping = {key: mapping for key, mapping in zip(list(domain_mapping.keys()), mappings)}
 
         return domains, domain_mapping, domain_colors
 
     @staticmethod
     def _cam_domains(dataset: list[str], paths: list[DataPath]) -> tuple[list, dict]:
         df = median_table(image_names=[x.image_name() for x in paths])
 
@@ -121,15 +120,15 @@
 
     @staticmethod
     def _pig_domains(dataset: list[str], paths: list[DataPath]) -> tuple[list, dict]:
         return dataset, {x.image_name(): x.subject_name for x in paths}
 
     @staticmethod
     def _species_domains(domains: list, paths: list[DataPath]) -> tuple[list, dict]:
-        return domains, {x.image_name(): domains[1] if "SPACE_" in x.subject_name else domains[0] for x in paths}
+        return domains, {x.image_name(): (domains[1] if "SPACE_" in x.subject_name else domains[0]) for x in paths}
 
     def domain_name(self, image_name: str) -> str:
         """
         Returns: Domain name (e.g. P001) for the given image_name.
         """
         return self.domain_mapping[image_name]
 
@@ -151,15 +150,15 @@
     def to_json(self) -> dict:
         """
         Returns: Basic properties of the mapping for future reference.
         """
         return {"target_domain": self.target_domain, "domains": self.domains}
 
     @classmethod
-    def from_config(cls, config: Config, target_domains: list[str] = None) -> dict[str, Self]:
+    def from_config(cls, config: Config, target_domains: list[str] = None) -> dict[str, "DomainMapper"]:
         if target_domains is None:
             target_domains = config.get("input/target_domain", ["camera_index"])
         else:
             target_domains = target_domains
         shuffle_domains = config.get("input/shuffle_domains", False)
 
         if "domain_mappings" in config:
```

## htc/utils/LabelMapping.py

```diff
@@ -5,15 +5,14 @@
 import itertools
 import re
 from pathlib import Path
 from typing import TYPE_CHECKING, Union
 
 import numpy as np
 import torch
-from typing_extensions import Self
 
 from htc.cpp import automatic_numpy_conversion, tensor_mapping
 from htc.settings import settings
 from htc.tivita.DatasetSettings import DatasetSettings
 from htc.utils.Config import Config
 
 if TYPE_CHECKING:
@@ -22,33 +21,30 @@
 
 class LabelMapping:
     def __init__(
         self,
         mapping_name_index: dict[str, int],
         last_valid_label_index: int = None,
         zero_is_invalid: bool = False,
-        unknown_invalid: bool = False,
         mapping_index_name: dict[int, str] = None,
         label_colors: dict[str, str] = None,
     ):
         """
         Small helper class to handle different label mappings (e.g. original labels as defined by the clinicians vs. labels used for training).
 
         Args:
             mapping_name_index: Mapping of label identifiers (name of the organs) to indices (integer values).
             last_valid_label_index: The index of the last valid label in the mapping. This is useful to distinguish between valid and invalid pixels. If None, every label index smaller than settings.label_index_thresh will be considered valid.
             zero_is_invalid: If True, 0 will be treated as invalid label (additional labels may be treated as invalid via last_valid_label_index).
-            unknown_invalid: If True, all labels which are not part of this mapping are considered invalid (except of raising an error).
             mapping_index_name: Mapping of label indices to names (used to construct a mapping based on a saved config).
             label_colors: Mapping of label names to color values (e.g. '#ffffff'). If None, settings.label_colors will be used.
         """
         self.mapping_name_index = mapping_name_index
         self.label_colors = label_colors if label_colors is not None else settings.label_colors
         self.zero_is_invalid = zero_is_invalid
-        self.unknown_invalid = unknown_invalid
 
         if last_valid_label_index is None:
             valid_indices = [
                 i for i in mapping_name_index.values() if i < settings.label_index_thresh
             ]  # We need to find the last valid index
             self.last_valid_label_index = max(valid_indices)
         else:
@@ -79,84 +75,61 @@
             sorted(self.mapping_name_index.items(), key=lambda item: item[1])
         )  # Sort by value
 
         assert set(self.mapping_name_index.values()) == set(
             self.mapping_index_name.keys()
         ), "Both mappings must have the same ids (it is only allowed to have more names for the same id)"
 
-    def __repr__(self):
-        labels = ", ".join([f"{l}={self.name_to_index(l)}" for l in self.label_names()])
-        return f"LabelMapping({labels})"
-
     def __len__(self):
-        """Returns: The number of unique valid label indices in this mapping. This is identical to the number of classes used during training. Please note that it is possible to have more names than ids since multiple names can map to the same id."""
+        """Returns: The number of unique valid label indices in this mapping. This is identical to the number of classes used during training. Please note that it is possible to have more names than ids since multiple names can map to the same id.
+        """
         return len(self.label_indices())
 
     def __contains__(self, name_or_index: Union[str, int, torch.Tensor]) -> bool:
-        """Returns: True when the given name or index is part of this mapping (valid or invalid)."""
+        """Returns: True when the given name or index is part of this mapping."""
         if type(name_or_index) == torch.Tensor:
             name_or_index = name_or_index.item()
 
         if type(name_or_index) == str:
             return name_or_index in self.mapping_name_index
         elif type(name_or_index) == int:
             return name_or_index in self.mapping_index_name
         else:
             raise ValueError(f"Invalid type {type(name_or_index)}")
 
-    def __eq__(self, other: Self) -> bool:
+    def __eq__(self, other: "LabelMapping") -> bool:
         return (
             self.mapping_name_index == other.mapping_name_index
             and self.last_valid_label_index == other.last_valid_label_index
             and self.zero_is_invalid == other.zero_is_invalid
-            and self.unknown_invalid == other.unknown_invalid
             and self.mapping_index_name == other.mapping_index_name
         )
 
     def name_to_index(self, name: str) -> int:
-        """
-        Maps a label name to the corresponding label index.
-
-        Args:
-            name: Name of the label.
-
-        Returns: The index associated with the label name.
-        """
-        if name in self.mapping_name_index:
-            return self.mapping_name_index[name]
-        else:
-            if self.unknown_invalid:
-                # One above the last known index
-                return list(self.mapping_index_name.keys())[-1] + 1
-            else:
-                raise ValueError(f"Cannot find the label {name} in the mapping {self.mapping_name_index}")
+        assert name in self.mapping_name_index, f"Cannot find the label {name} in the mapping {self.mapping_name_index}"
+        return self.mapping_name_index[name]
 
     def index_to_name(self, i: Union[int, torch.Tensor], all_names: bool = False) -> Union[str, list[str]]:
         """
         Maps a label_index to its name(s).
 
         Args:
             i: The label_index.
             all_names: If True, all names are returned if one id maps to multiple names. Otherwise, only the first name is returned.
 
         Returns: The (first) name for the label (all_names=False) or list of label names (all_names=True).
         """
         if type(i) == torch.Tensor:
             i = i.item()
 
-        if i in self.mapping_index_name:
-            if all_names:
-                return [name for name, label_index in self.mapping_name_index.items() if label_index == i]
-            else:
-                return self.mapping_index_name[i]
+        assert i in self.mapping_index_name, f"Cannot find the label index {i} in the mapping {self.mapping_index_name}"
+        if all_names:
+            return [name for name, label_index in self.mapping_name_index.items() if label_index == i]
         else:
-            if self.unknown_invalid:
-                return "unknown"
-            else:
-                raise ValueError(f"Cannot find the label index {i} in the mapping {self.mapping_index_name}")
+            return self.mapping_index_name[i]
 
     def name_to_color(self, name: str) -> str:
         """
         Map label names to colors.
 
         >>> m = LabelMapping({'a': 0, 'a_second_name': 0}, last_valid_label_index=0, label_colors={'a': '#FFFFFF'})
         >>> m.name_to_color('a')
@@ -187,18 +160,14 @@
     def is_index_valid(self, i: Union[int, torch.Tensor, np.ndarray]) -> Union[bool, torch.Tensor, np.ndarray]:
         """Returns: True when the given label index (or tensor with indices) is valid according to this mapping."""
         if self.zero_is_invalid:
             return (0 < i) & (i <= self.last_valid_label_index)
         else:
             return i <= self.last_valid_label_index
 
-    def is_name_valid(self, name: str) -> bool:
-        """Returns: True when the given label name is valid according to this mapping."""
-        return self.is_index_valid(self.name_to_index(name))
-
     def label_names(self, include_invalid: bool = False, all_names: bool = False) -> list[str]:
         """
         List of all label names as defined by this mapping.
 
         Args:
             include_invalid: If True, also include invalid labels.
             all_names: If True, include all names for a label if there are more than one.
@@ -222,15 +191,17 @@
         """
         if include_invalid:
             return list(self.mapping_index_name.keys())
         else:
             return [label_index for label_index in self.mapping_index_name.keys() if self.is_index_valid(label_index)]
 
     @automatic_numpy_conversion
-    def map_tensor(self, tensor: Union[torch.Tensor, np.ndarray], old_mapping: Self) -> Union[torch.Tensor, np.ndarray]:
+    def map_tensor(
+        self, tensor: Union[torch.Tensor, np.ndarray], old_mapping: "LabelMapping"
+    ) -> Union[torch.Tensor, np.ndarray]:
         """
         Remaps all label ids in the tensor to the new label id as defined by the label mapping of this class (self = new_mapping). Mapping happens based on the label name.
 
         Args:
             tensor: Tensor or array with label id values which should be remapped.
             old_mapping: The label mapping which defines the name of the labels used in the tensor.
 
@@ -264,45 +235,44 @@
 
     def to_json(self) -> dict:
         """Returns: All class attributes as dictionary so that the object can be reconstructed again from the dict."""
         return {
             "mapping_name_index": self.mapping_name_index,
             "last_valid_label_index": self.last_valid_label_index,
             "zero_is_invalid": self.zero_is_invalid,
-            "unknown_invalid": self.unknown_invalid,
             "mapping_index_name": self.mapping_index_name,
         }
 
     @classmethod
-    def from_path(cls, path: "DataPath") -> Self:
+    def from_path(cls, path: "DataPath") -> "LabelMapping":
         """
         Constructs a label mapping based on the default labels of the dataset accessed via the path object.
 
         These are the labels as defined by the clinicians.
         """
         label_colors = path.dataset_settings["label_colors"] if "label_colors" in path.dataset_settings else None
         return cls(
             path.dataset_settings["label_mapping"],
             path.dataset_settings["last_valid_label_index"],
             label_colors=label_colors,
         )
 
     @classmethod
-    def from_data_dir(cls, data_dir: Path) -> Self:
+    def from_data_dir(cls, data_dir: Path) -> "LabelMapping":
         """
         Similar to from_path() but using the dataset_settings.json from the data directory directly.
 
         Args:
             data_dir: Path to the data directory which must contain a dataset_settings.json file.
         """
         dsettings = DatasetSettings(data_dir)
         return cls(dsettings["label_mapping"], dsettings["last_valid_label_index"])
 
     @classmethod
-    def from_config(cls, config: Config) -> Self:
+    def from_config(cls, config: Config) -> "LabelMapping":
         """
         Constructs a label mapping as defined in the config file. config['label_mapping'] can be defined as:
 
         * a LabelMapping instance.
         * a config definition string in the format module>variable (e.g. htc.settings_seg>label_mapping). module must be importable and variable must exist in the module.
         * a dict from a JSON file (as saved via to_class_dict()).
         * a dict with label_name:label_index definitions (like settings_seg.label_mapping) in which case settings.label_index_thresh will be used to determine invalid labels.
@@ -322,27 +292,22 @@
                 # In case settings is an object
                 module = getattr(module, match.group(1).split(".")[-1])
             mapping = getattr(module, match.group(2))
             # Now load as usual
 
         if isinstance(mapping, LabelMapping):
             mapping_obj = mapping
-        elif all(var in mapping for var in ("mapping_name_index", "last_valid_label_index", "mapping_index_name")):
+        elif all([var in mapping for var in ("mapping_name_index", "last_valid_label_index", "mapping_index_name")]):
             # This is easier because we have all information we need in the config
             mapping_index_name = {
                 int(i): n for i, n in mapping["mapping_index_name"].items()
             }  # JSON only allows strings as keys
             zero_is_invalid = mapping.get("zero_is_invalid", False)
-            unknown_invalid = mapping.get("unknown_invalid", False)
             mapping_obj = cls(
-                mapping["mapping_name_index"],
-                mapping["last_valid_label_index"],
-                zero_is_invalid,
-                unknown_invalid,
-                mapping_index_name,
+                mapping["mapping_name_index"], mapping["last_valid_label_index"], zero_is_invalid, mapping_index_name
             )
         else:
             if "label_mapping/background" in config and config["label_mapping/background"] == 0:
                 # Unfortunately, we need to manually handle the background class as the config files are sorted and abdominal_linen comes before background and also has the label 0
                 label_mapping = {"background": 0}
                 for label_name, label_index in mapping.items():
                     if label_name != "background":
```

## htc/utils/MeasureTime.py

```diff
@@ -37,8 +37,8 @@
             tag = "[" + self.name + "] "
         else:
             tag = ""
 
         self.elapsed_seconds = seconds
 
         if not self.silent:
-            print(f"{tag}Elapsed time: {seconds // 60:.0f} m and {seconds % 60:.2f} s")
+            print("%sElapsed time: %d m and %.2f s" % (tag, seconds // 60, seconds % 60))
```

## htc/utils/MultiPath.py

```diff
@@ -45,61 +45,46 @@
     def __new__(cls, *args, **kwargs):
         if len(args) == 1:
             if type(args[0]) == dict:
                 # Construction from pickled object
 
                 # e.g. .../2021_02_05_Tivita_multiorgan_masks/intermediates/preprocessing/L1
                 path = super().__new__(cls, args[0]["path"])
+                # e.g. .../2021_02_05_Tivita_multiorgan_semantic/intermediates
+                path._alternatives_root = args[0]["alternatives_root"]
                 # e.g. [.../2021_02_05_Tivita_multiorgan_semantic/intermediates, .../2021_02_05_Tivita_multiorgan_masks/intermediates, ...]
                 path._alternatives = args[0]["alternatives"]
                 # e.g. 2021_02_05_Tivita_multiorgan_masks
                 path._default_needle = args[0]["default_needle"]
 
                 return path
             else:
                 # Default construction, we just make sure that the path is expanded
-
-                # Normalize the path and make it absolute without resolving symbolic links as this may break the logic of the MultiPath class which relies on path replacements
-                # For example, after resolving a symlink the path may not contain any of the alternatives anymore so it is impossible to do the replacements
-                new_args = [str(unify_path(args[0], resolve_symlinks=False))]
+                new_args = [str(unify_path(args[0]))]
         else:
             # Construction from parts
             new_args = args
 
         super_path = super().__new__(cls, *new_args, **kwargs)
 
         # Custom attributes
-        super_path._alternatives = [str(super_path)]
+        super_path._alternatives = []
+        super_path._alternatives_root = str(super_path)
         super_path._default_needle = None
-        super_path._set_attributes()
 
         return super_path
 
     def _make_child(self, args):
-        if len(args) == 1 and Path(args[0]).is_absolute():
-            # If the child path is already absolute, we can just use it as-is
-            abs_path = args[0]
-            if type(abs_path) == str:
-                abs_path = Path(abs_path)
-            return abs_path
-        else:
-            # Any child path which is created via base / new should also receive the additional class attributes
-            child = super()._make_child(args)
-            child._alternatives = self._alternatives
-            child._default_needle = self._default_needle
-            child._set_attributes()
-
-            return child
-
-    def _set_attributes(self):
-        # The attributes are always based on the current best location
-        location = self.find_best_location()
-        self._drv = location._drv
-        self._root = location._root
-        self._parts = location._parts
+        # Any child path which is created via base / new should also receive the additional class attributes
+        child = super()._make_child(args)
+        child._alternatives = self._alternatives
+        child._alternatives_root = self._alternatives_root
+        child._default_needle = self._default_needle
+
+        return child
 
     def __repr__(self):
         """
         Generates a user-friendly description of this multi-path instance.
 
         >>> path = MultiPath('/a/b')
         >>> path.add_alternative('/')
@@ -141,36 +126,40 @@
 
         return repr
 
     def __reduce__(self):
         # Called when pickling path objects (e.g. multiprocessing)
         kwargs = {
             "path": super().__str__(),
+            "alternatives_root": self._alternatives_root,
             "alternatives": self._alternatives,
             "default_needle": self._default_needle,
         }
         return (self.__class__, (kwargs,))
 
     def __str__(self):
         # Paths are always converted to strings when they are used, e.g. on open or on .exists()
         # Here, we overwrite it to return the first existing path from all alternatives
         if hasattr(self, "_alternatives") and len(self._alternatives) > 0:
+            assert hasattr(self, "_alternatives_root"), "_alternatives cannot exists without _alternatives_root"
+
             return str(self.find_best_location())
         else:
             return super().__str__()
 
     @property
     def parent(self):
-        alternatives = self._alternatives
+        alternatives = [self._alternatives_root] + self._alternatives
         if str(self) in alternatives:
             # We go back to a standard Path object when we walk outside of one of the alternative locations because then there is no common subdirectory anymore to share between the alternatives
             return Path(str(self)).parent
         else:
             p = super().parent
             p._alternatives = self._alternatives
+            p._alternatives_root = self._alternatives_root
             p._default_needle = self._default_needle
 
             return p
 
     @property
     def parents(self):
         # We directly go to a standard Path since we don't know how many parents should be accessed at this point
@@ -199,62 +188,61 @@
             location = self.find_best_location(writing=True)
         else:
             # If there is no default location, we want to create the dir in the root location (even if one of the other locations may exists)
             location = Path(super().__str__())
 
         location.mkdir(*args, **kwargs)
 
-    def relative_to(self, *other) -> Path:
-        if len(other) == 1 and isinstance(other[0], MultiPath):
-            other = other[0]
-            error = None
-
-            # The first other location which is relative to self will be used
-            for location in other.possible_locations():
-                try:
-                    return self.relative_to(location)
-                except ValueError as e:
-                    error = e
-
-            assert error is not None, "The parent class should have risen an exception"
-            raise error
-        else:
-            return super().relative_to(*other)
+    def resolve(self, *args, **kwargs) -> "MultiPath":
+        # MultiPath is resolved by default since we call unify_path in the constructor and in possible_locations()
+        return self
 
     def write_text(self, *args, **kwargs):
-        return self.find_best_location().write_text(*args, **kwargs)
+        self.find_best_location().write_text(*args, **kwargs)
 
     def write_bytes(self, *args, **kwargs):
-        return self.find_best_location().write_bytes(*args, **kwargs)
-
-    def with_name(self, *args, **kwargs) -> Path:
-        return self.find_best_location().with_name(*args, **kwargs)
-
-    def with_stem(self, *args, **kwargs) -> Path:
-        return self.find_best_location().with_stem(*args, **kwargs)
-
-    def with_suffix(self, *args, **kwargs) -> Path:
-        return self.find_best_location().with_suffix(*args, **kwargs)
-
-    def resolve(self, *args, **kwargs) -> "Path":
-        return self.find_best_location().resolve(*args, **kwargs)
+        self.find_best_location().write_bytes(*args, **kwargs)
 
     def add_alternative(self, path: Union[Path, str]) -> None:
         """Adds an alternative location to this path which will be replaced with the root location."""
-        self._alternatives.append(str(unify_path(path, resolve_symlinks=False)))
+        self._alternatives.append(str(unify_path(path)))
 
     def set_default_location(self, location_needle: str) -> None:
         """
         Sets a needle to select one of the possible locations for find_best_location(). This is mainly interesting for write operations to determine where new folders/files should be stored.
 
         Args:
             location_needle: Part of path of the default location (it is sufficient if a subset matches). The best match (according to the string similarity) will be used in case of non-unique matches.
         """
         self._default_needle = location_needle
 
+    def find_existing_location(self) -> Path:
+        """
+        Searches for the first existing path by checking all alternatives.
+
+        >>> path = MultiPath('/a/b')
+        >>> path.add_alternative('/')
+        >>> target_path = path / 'home'
+        >>> str(target_path.find_existing_location())
+        '/home'
+
+        Raises:
+            FileNotFoundError: If the target file cannot be find in any of the alternatives.
+
+        Returns: The full path to the target file.
+        """
+        path_str = str(self)
+        if Path(path_str).exists():
+            return Path(path_str)
+        else:
+            raise FileNotFoundError(
+                f'Cannot find the file {path_str.removeprefix(self._alternatives_root + "/")}. Tried the following root'
+                f" locations: {self._alternatives + [self._alternatives_root]}"
+            )
+
     def find_location(self, needle: str) -> Path:
         """
         Searches all locations for the given needle.
 
         >>> path = MultiPath('/a/c')
         >>> path.add_alternative('/b/c')
         >>> str(path.find_location('b'))
@@ -342,31 +330,21 @@
         Args:
             only_existing: Include only locations which exist.
             filter: Filter function to select locations. The function receives a paths and must return True if the path should be used.
 
         Returns: All possible locations for the current path.
         """
         path_str = super().__str__()
+        assert path_str.startswith(
+            self._alternatives_root
+        ), f"The root location ({self._alternatives_root}) must always match the root path ({path_str})"
 
-        # Find the root of the current main path (last match)
-        alternatives_root = None
-        for alternative in self._alternatives:
-            if path_str.startswith(alternative):
-                alternatives_root = alternative
-                break
-        assert (
-            alternatives_root is not None
-        ), f"Could not find the alternatives root for {path_str}\n{self._alternatives}"
-
-        # Replace the root with all alternatives to get all possible locations (including the main location)
-        locations = []
+        locations = [Path(path_str)]
         for alternative in self._alternatives:
-            new = path_str.replace(alternatives_root, alternative)
-            new = unify_path(new, resolve_symlinks=False)
-            locations.append(new)
+            locations.append(unify_path(path_str.replace(self._alternatives_root, alternative)))
 
         if filter is not None:
             locations = [l for l in locations if filter(l)]
 
         if only_existing:
             locations = [l for l in locations if l.exists()]
```

## htc/utils/SpectrometerReader.py

```diff
@@ -1,70 +1,84 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
 import re
 from pathlib import Path
-from typing import Union
 
 import numpy as np
 
 
+def custom_normalization(spec: np.array) -> tuple[np.array, np.array, np.array]:
+    """
+    Compute median and std of an array of spectrometer measurements, normalized such that it can easily be plotted together with Tivita measurements.
+
+    Args:
+        spec: Array of spectrometer measurements of shape a x b x 2 with a: number of samples, b: dimensionality of spectrum
+
+    Returns: Tuple of wavelengths, median of spectra across samples a, std of spectra across samples a.
+    """
+    assert spec.ndim == 3, (
+        "Array of spectra with shape a x b x 2 with a: number of samples, b: dimensionality of spectrum should be"
+        f" given, instead shape is {spec.shape}"
+    )
+    x = spec[0, :, 0]
+    mask = (x <= 1000) & (x >= 500)
+    x = x[mask]
+    spec_mask = spec[:, mask, 1]
+    assert not np.any(np.linalg.norm(spec_mask, axis=-1, ord=1) == 0), "Normalization failure due to zero division"
+    spec_mask = spec_mask / np.linalg.norm(spec_mask, ord=1, axis=-1, keepdims=True) * len(x) / 100
+    y_median = np.median(spec_mask, axis=0)
+    y_std = np.std(spec_mask, axis=0)
+
+    return x, y_median, y_std
+
+
 class SpectrometerReader:
     def __init__(self, data_dir: Path):
         """
         Reads spectrometer measurements from txt files with a `>>>>>Begin Spectral Data<<<<<` section as produced by the Ocean Insight HR2000+ spectrometer.
 
         Example directory with colorchecker measurements
         >>> from htc.settings import settings
         >>> spectra_dir = settings.data_dirs.studies / "colorchecker_spectrometer/2022_09_19_MIC1"
         >>> len(list(spectra_dir.iterdir()))  # 100 measurements for all 24 color chips + white and dark calibration
         2600
         >>> reader = SpectrometerReader(spectra_dir)
         >>> reader.label_names()  # doctest: +ELLIPSIS
         ['black', 'blue', 'blue_flower', ...]
-        >>> blue_sky = reader.read_spectrum("blue_sky", calibration=True, normalization=1)
-        >>> list(blue_sky.keys())
-        ['wavelengths', 'spectra', 'median_spectrum', 'std_spectrum']
-        >>> blue_sky["wavelengths"][:3]  # First three wavelengths (in nm)
+        >>> blue_sky = reader.read_spectrum("blue_sky", calibration=True, normalization=1, median=True)
+        >>> blue_sky.shape
+        (2048, 2)
+        >>> blue_sky[:3, 0]  # First three wavelengths (in nm)
         array([187.255, 187.731, 188.206])
-        >>> blue_sky["median_spectrum"][:3]  # First three reflectance values from the median spectrum (calibrated, L1-normalized)
+        >>> blue_sky[:3, 1]  # First three reflectance values from the median spectrum (calibrated, L1-normalized)
         array([0.00010494, 0.00010494, 0.00010494])
 
         Args:
             data_dir: Path to the directory which contains the spectrometer files (*.txt files).
         """
         self.data_dir = data_dir
-        self.white = self.read_spectrum("white_calibration", calibration=False, normalization=None)
-        self.dark = self.read_spectrum("dark_calibration", calibration=False, normalization=None)
+        self.white = self.read_spectrum("white_calibration", calibration=False, normalization=None, median=True)
+        self.dark = self.read_spectrum("dark_calibration", calibration=False, normalization=None, median=True)
 
     def read_spectrum(
-        self,
-        label_name: str,
-        calibration: bool = False,
-        normalization: int = None,
-        adapt_to_tivita: bool = False,
-        transform_to_tivita: bool = False,
-    ) -> Union[dict[str, np.ndarray], None]:
+        self, label_name: str, calibration: bool = False, normalization: int = None, median: bool = False
+    ) -> np.ndarray:
         """
         Reads the spectrometer measurements for a specific label.
 
         Args:
             label_name: If label_name is a prefix, function will iterate over all files with that prefix in the data directory. To read a single spectrometer file, the path name should be passed.
-            calibration: Whether white and dark correction should be performed. Only possible if dark and white measurements exist in the data directory.
+            calibration: Whether white and dark balancement should be performed. Only possible if dark and white measurements exist in the data directory.
             normalization: Which normalization to perform. None: no normalization, 1: L1, 2: L2, ...
-            adapt_to_tivita: If True, limit the spectral range to the range of the Tivita camera (from 500 to 1000 nm). If normalization is applied, the resulting spectra is rescaled so that the reflectance values are in the same range as the Tivita reflectance values.
-            transform_to_tivita: If True, transform the spectrometer measurements to Tivita measurements by averaging the spectrometer measurements in non-overlapping wavelength ranges corresponding to the Tivita measurements.
+            median: If False, an array of shape a x b x 2 containing all available spectra is returned, otherwise, the median reflectance per wavelength across all measurements for this label is computed and an array of shape b x 2 is returned (see below).
 
-        Returns: Dictionary with the following keys (a = number of files, b = dimensionality of the spectrum) or None if no files could be found:
-        - `wavelengths`: Array of wavelengths (shape b).
-        - `spectra`: Array with the measured reflectance values for each wavelength (shape a x b).
-        - `median_spectrum`: Median spectrum across all measurements (shape b).
-        - `std_spectrum`: Standard deviation of the spectra across all measurements (shape b).
-        - `median_spectrum_mapped`: Median spectrum across all measurements mapped to the Tivita measurements (shape b). Only if transform_to_tivita is True.
-        - `std_spectrum_mapped`: Standard deviation of the spectra across all measurements mapped to the Tivita measurements (shape b). Only if transform_to_tivita is True.
+        Returns:
+            if median == False: Array of spectra with shape a x b x 2 with a: number of files, b: dimensionality of spectrum. spectra[:, :, 0] denotes the measurement wavelengths, spectra[:, :, 1] the corresponding intensity measurement.
+            if median == True: Array of median spectrum with shape b x 2 with b: dimensionality of spectrum. spectra[:, 0] denotes the measurement wavelengths, spectra[:, 1] the corresponding intensity measurement.
         """
         if label_name.endswith(".txt"):
             paths = [self.data_dir / label_name]
         else:
             paths = sorted(self.data_dir.glob(f"{label_name}_HRC*.txt"))
 
         if len(paths) == 0:
@@ -100,62 +114,28 @@
 
             spectra = np.asarray(spectra)
 
             if calibration:
                 assert (
                     self.dark is not None and self.white is not None
                 ), "white and/or dark calibration measurements are missing!"
-                spectra[:, :, 1] = (spectra[:, :, 1] - self.dark["median_spectrum"]) / (
-                    self.white["median_spectrum"] - self.dark["median_spectrum"]
-                )
-
-            if adapt_to_tivita:
-                x = spectra[0, :, 0]
-                mask = (x <= 1000) & (x >= 500)
-                x = x[mask]
-                spectra = spectra[:, mask, :]
+                spectra[:, :, 1] = (spectra[:, :, 1] - self.dark[:, 1]) / (self.white[:, 1] - self.dark[:, 1])
 
             if normalization is not None:
                 assert not np.any(
                     np.linalg.norm(spectra[:, :, 1], axis=-1, ord=normalization) == 0
                 ), f"Normalization failure due to zero division for {label_name}"
                 spectra[:, :, 1] = spectra[:, :, 1] / np.linalg.norm(
                     spectra[:, :, 1], axis=-1, keepdims=True, ord=normalization
                 )
 
-                if adapt_to_tivita:
-                    # Scale the spectrometer measurements to the same range as the TIVITA measurements
-                    spectra[:, :, 1] = spectra[:, :, 1] * spectra.shape[1] / 100
-
-            # transform spectrometer measurements to Tivita measurements
-            if transform_to_tivita:
-                spectra_transformed = np.zeros((spectra.shape[0], 100))
-                spectrometer_wavelengths = spectra[0, :, 0]
-                tivita_wavelengths = np.linspace(500, 1000, 101)
-                for i in np.arange(len(tivita_wavelengths) - 1):
-                    mask = (tivita_wavelengths[i] <= spectrometer_wavelengths) & (
-                        spectrometer_wavelengths < tivita_wavelengths[i + 1]
-                    )
-                    spectra_transformed[:, i] = np.mean(spectra[:, mask, 1], axis=1)
-
-                return {
-                    "wavelengths": spectra[0, :, 0],
-                    "spectra": spectra[:, :, 1],
-                    "median_spectrum": np.median(spectra[:, :, 1], axis=0),
-                    "std_spectrum": np.std(spectra[:, :, 1], axis=0),
-                    "median_spectrum_transformed": np.median(spectra_transformed, axis=0),
-                    "std_spectrum_transformed": np.std(spectra_transformed, axis=0),
-                }
-
-            return {
-                "wavelengths": spectra[0, :, 0],
-                "spectra": spectra[:, :, 1],
-                "median_spectrum": np.median(spectra[:, :, 1], axis=0),
-                "std_spectrum": np.std(spectra[:, :, 1], axis=0),
-            }
+            if median:
+                spectra = np.median(spectra, axis=0)
+
+            return spectra
 
     def label_names(self) -> list[str]:
         """Returns a sorted list of label names in the data directory"""
         names = set()
         for f in self.data_dir.iterdir():
             match = re.search(r"^(\w+)_HRC", f.name)
             assert match is not None, f"Could not extract the label name from {f.name}"
```

## htc/utils/colors.py

```diff
@@ -76,29 +76,31 @@
     labels = list(set(labels))
 
     return labels
 
 
 def color_organs() -> dict[str, tuple]:
     labels = unique_labels()
-    return dict(zip(labels, generate_distinct_colors(len(labels))))
+    label_colors = {label: color for label, color in zip(labels, generate_distinct_colors(len(labels)))}
+
+    return label_colors
 
 
 def color_organs_extending() -> dict[str, tuple]:
     labels = unique_labels()
     labels = [label for label in labels if label not in settings.label_colors.keys()]
 
     if len(labels) == 0:
         return {}
     else:
         existing_colors = list(settings.label_colors.values())
         existing_colors = [to_rgb(c) for c in existing_colors]
         new_colors = generate_distinct_colors(len(labels), existing_colors)
 
-        return dict(zip(labels, new_colors))
+        return {label: color for label, color in zip(labels, new_colors)}
 
 
 if __name__ == "__main__":
     color_mapping = color_organs()
     color_mapping = {l: to_hex(c).upper() for l, c in color_mapping.items()}
     color_mapping = sort_labels(color_mapping)
```

## htc/utils/helper_functions.py

```diff
@@ -1,74 +1,60 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import itertools
 import json
+import logging
 import re
+import subprocess
 import warnings
 from pathlib import Path
-from typing import Any, Union
+from typing import Union
 
-import nbformat
 import numpy as np
 import pandas as pd
+import papermill as pm
 import torch
-from nbconvert import HTMLExporter
-from nbconvert.preprocessors import ExecutePreprocessor
-from traitlets.config import Config as NBConfig
 
-from htc.cpp import automatic_numpy_conversion
 from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
 from htc.settings_seg import settings_seg
-from htc.tivita.DataPath import DataPath
 from htc.tivita.DatasetSettings import DatasetSettings
 from htc.utils.Config import Config
 from htc.utils.LabelMapping import LabelMapping
 
 
-def basic_statistics(
-    dataset_name: str,
-    specs_name: str = None,
-    label_mapping: LabelMapping = None,
-    annotation_name: Union[str, list[str]] = None,
-) -> pd.DataFrame:
+def basic_statistics(dataset_name: str, specs_name: str, label_mapping: LabelMapping = None) -> pd.DataFrame:
     """
     Basic statistics about a dataset.
 
     >>> df = basic_statistics("2021_02_05_Tivita_multiorgan_semantic", "pigs_semantic-only_5foldsV2.json", label_mapping=settings_seg.label_mapping)
     >>> print(df.head().to_string())
-                     image_name  label_index        label_name  label_valid set_type subject_name            timestamp  n_pixels
-    0  P041#2019_12_14_12_00_16            0        background         True    train         P041  2019_12_14_12_00_16    158786
-    1  P041#2019_12_14_12_00_16            4             colon         True    train         P041  2019_12_14_12_00_16     67779
-    2  P041#2019_12_14_12_00_16            5       small_bowel         True    train         P041  2019_12_14_12_00_16     65634
-    3  P041#2019_12_14_12_00_16            9           bladder         True    train         P041  2019_12_14_12_00_16     10594
-    4  P041#2019_12_14_12_00_16           13  fat_subcutaneous         True    train         P041  2019_12_14_12_00_16      4251
+                     image_name  label_index   label_name  label_valid set_type subject_name            timestamp  n_pixels
+    0  P041#2019_12_14_12_00_16            0   background         True    train         P041  2019_12_14_12_00_16    158786
+    1  P041#2019_12_14_12_00_16            4        colon         True    train         P041  2019_12_14_12_00_16     67779
+    2  P041#2019_12_14_12_00_16            5  small_bowel         True    train         P041  2019_12_14_12_00_16     65634
+    3  P041#2019_12_14_12_00_16            9      bladder         True    train         P041  2019_12_14_12_00_16     10594
+    4  P041#2019_12_14_12_00_16           13          fat         True    train         P041  2019_12_14_12_00_16      4251
 
     Args:
         dataset_name: Name of the dataset (folder name on the network drive).
         specs_name: Name or path to a data specification file. A set_type column will be added indicating for each image whether it is part of the train or test set.
         label_mapping: Optional label mapping which is applied to the statistics table. It will rename all labels, remove invalid labels and give the sum of pixels for the new labels (in case multiple labels like blue_cloth or metal map to the same name like background).
-        annotation_name: Optional parameter. If not None, the table will only include the annotations corresponding zu the given annotation_name. Otherwise, all available annotations will be included.
 
     Returns: Statistics in table format.
     """
-    df = median_table(dataset_name=dataset_name, annotation_name=annotation_name)
+    df = median_table(dataset_name=dataset_name)
     df = df[["image_name", "subject_name", "timestamp", "label_name", "n_pixels"]]
 
     # Add a set_type column based on the data specification file
-    if specs_name is not None:
-        specs = DataSpecification(specs_name)
-        specs.activate_test_set()
+    specs = DataSpecification(specs_name)
+    specs.activate_test_set()
 
-        image_names_train = [p.image_name() for p in specs.paths("^train")]
-        image_names_test = [p.image_name() for p in specs.paths("^test")]
-    else:
-        image_names_train = []
-        image_names_test = []
+    image_names_train = [p.image_name() for p in specs.paths("^train")]
+    image_names_test = [p.image_name() for p in specs.paths("^test")]
 
     set_types = []
     for image_name in df["image_name"]:
         if image_name in image_names_train:
             set_types.append("train")
         elif image_name in image_names_test:
             set_types.append("test")
@@ -91,70 +77,41 @@
         df = df.sort_values(by=["image_name", "label_index"])
 
     return df.reset_index(drop=True)
 
 
 def median_table(
     dataset_name: str = None,
-    paths: list[DataPath] = None,
     image_names: list[str] = None,
     label_mapping: LabelMapping = None,
     annotation_name: Union[str, list[str]] = None,
 ) -> pd.DataFrame:
     """
     This function is the general entry point for reading the median spectra tables. You can either read the table from a specific dataset or provide image names for which you want to have the spectra (also works if the names come from different datasets).
 
-    >>> df = median_table(dataset_name="2021_02_05_Tivita_multiorgan_semantic")
-    >>> df.iloc[0]  # doctest: +ELLIPSIS
-    image_name ... P041#2019_12_14_12_01_09...
-    subject_name ... P041...
-    median_normalized_spectrum ... [0.0038273174, 0.0038260417, 0.0040428545, 0.0...
-
-    Besides basic info about the image and the median spectra (`median_normalized_spectrum`), all available metadata is included in the table as well:
-    >>> df.columns.to_list()
-    ['image_name', 'subject_name', 'timestamp', 'label_index', 'label_name', 'median_spectrum', 'std_spectrum', 'median_normalized_spectrum', 'std_normalized_spectrum', 'n_pixels', 'median_sto2', 'std_sto2', 'median_nir', 'std_nir', 'median_twi', 'std_twi', 'median_ohi', 'std_ohi', 'median_thi', 'std_thi', 'median_tli', 'std_tli', 'image_labels', 'Camera_CamID', 'Camera_Exposure', 'Camera_analoger Gain', 'Camera_digitaler Gain', 'Camera_Speed', 'SW_Name', 'SW_Version', 'Fremdlichterkennung_Fremdlicht erkannt?', 'Fremdlichterkennung_PixelmitFremdlicht', 'Fremdlichterkennung_Breite LED Rot', 'Fremdlichterkennung_Breite LED Gruen', 'Fremdlichterkennung_Grenzwert Pixelanzahl', 'Fremdlichterkennung_Intensity Grenzwert', 'Aufnahme_Aufnahmemodus', 'camera_name', 'annotation_name']
-
-    This function can also be used to select specific annotations, either globally per dataset:
-    >>> df = median_table(dataset_name="2021_02_05_Tivita_multiorgan_semantic", annotation_name="semantic#intra1")
-    >>> df["annotation_name"].unique().tolist()
-    ['semantic#intra1']
-
-    or individually per image:
-    >>> df = median_table(image_names=["P091#2021_04_24_12_02_50@polygon#annotator1&polygon#annotator2", "P041#2019_12_14_12_01_39"])
-    >>> sorted(df["annotation_name"].unique())
-    ['polygon#annotator1', 'polygon#annotator2', 'semantic#primary']
-
-    Note: In the original table, one row denotes one label of one image from one annotator which also corresponds to the default of this function since the default annotation is used (similar to DataPath.read_segmentation()). If more than one annotation name is requested, a row is unique by its image_name, label_name and annotation_name.
+    Note: In the original table, one row denotes one label of one image from one annotator but the default of this function is to return only the default annotation (similar to DataPath.read_segmentation()).
 
     Args:
         dataset_name: Name of the dataset from which you want to have the median spectra table.
-        paths: List of DataPath objects from which you want to have the median spectra. If annotation names are specified with a data path object, those names will be used. If specified, image_names must be None.
-        image_names: List of image ids to search for (similar to the paths parameter). Image names may also include annotation names (e.g. subject#timestamp@name1&name2). It is not ensured that the resulting table contains all requested images because some images may lack annotations or are filtered out by the label_mapping. If specified, paths must be None.
+        image_names: List of image ids to search for.
         label_mapping: The target label mapping. There will be a new label_index_mapped column (and a new label_name_mapped column with the new names defined by the mapping) and the old label_index column will be removed (since the label_index is not unique across datasets). If set to None, then mapping is not carried out.
-        annotation_name: Unique name of the annotation(s) for cases where multiple annotations exist (e.g. inter-rater variability). If None, will use the default from the dataset. If the dataset does not have a default (i.e. the annotation_name_default is missing in the dataset_settings.json file), all annotations are returned. It is also possible to explicitly retrieve all annotations by setting this parameter to 'all'.
+        annotation_name: Unique name of the annotation(s) for cases where multiple annotations exist (e.g. inter-rater variability). If None, will use the default from the dataset. If the dataset does not have a default (i.e. the annotation_name_default is missing in the dataset_settings.json file), all annotations are returned. If 'all', all available annotations are returned.
 
     Returns: Median spectra data frame. The table is either sorted by image names (if image_names is not None) or by the sort_labels() function (if dataset_name is used).
     """
-    # Collect all available tables
     tables = {}
-    for path in sorted((settings.intermediates_dir_all / "tables").glob("*median_spectra*.feather")):
+    for path in sorted((settings.intermediates_dir / "tables").glob("*median_spectra*.feather")):
         parts = path.stem.split("@")
-        assert 2 <= len(parts) <= 3, (
-            "Invalid file format for median spectra table (it should be"
-            f" dataset_name@median_spectra@annotation_name.feather): {path}"
-        )
+        assert 2 <= len(parts) <= 3
         if len(parts) == 2:
             _dataset_name, _table_type = path.stem.split("@")
             _annotation_name = None
         else:
             _dataset_name, _table_type, _annotation_name = path.stem.split("@")
-        assert _table_type == "median_spectra", (
-            f"Invalid table name for median spectra table ({_table_type} instead of median_spectra, the general format"
-            f" should be dataset_name@median_spectra@annotation_name.feather): {path}"
-        )
+        assert _table_type == "median_spectra"
 
         if _dataset_name not in tables:
             tables[_dataset_name] = {}
         tables[_dataset_name][_annotation_name] = path
 
     def read_table(dataset_name: str, annotation_name: Union[str, list[str], None]) -> pd.DataFrame:
         # Find the default annotation_name
@@ -180,148 +137,65 @@
             df.append(df_a)
 
         needs_sorting = len(df) > 1
         df = pd.concat(df)
 
         if len(df) > 0 and label_mapping is not None:
             # Mapping from path to config (the mapping depends on the dataset and must be done separately)
-            df = df.query("label_name in @label_mapping.label_names(all_names=True)").copy()
-            if len(df) > 0:
+            df_mapping = df.query("label_name in @label_mapping.label_names(all_names=True)").copy()
+            if len(df_mapping) > 0:
+                df = df_mapping
                 label_indices = torch.from_numpy(df["label_index"].values)
-                assert (
-                    settings.data_dirs[dataset_name] is not None
-                ), f"Cannot find the path to the dataset {dataset_name} but this is required for remapping the labels"
                 original_mapping = LabelMapping.from_data_dir(settings.data_dirs[dataset_name])
                 label_mapping.map_tensor(label_indices, original_mapping)
                 df["label_index_mapped"] = label_indices
                 df["label_name_mapped"] = [label_mapping.index_to_name(i) for i in df["label_index_mapped"]]
 
         if needs_sorting:
             df = sort_labels(df, dataset_name=dataset_name)
 
         return df.reset_index(drop=True)
 
     if dataset_name is not None:
         return read_table(dataset_name, annotation_name)
 
-    def parse_paths(paths: list[DataPath]) -> tuple[list[str], dict[str, list[str]], list[str]]:
-        image_names_ordering = []
-        image_names_only = []
-        annotation_images = {}
-        for p in paths:
-            image_names_ordering.append(p.image_name())
-            names = p.annotation_names()
-
-            if len(names) > 0:
-                for a in names:
-                    if a not in annotation_images:
-                        annotation_images[a] = []
-                    annotation_images[a].append(p.image_name())
-            else:
-                image_names_only.append(p.image_name())
-
-        return image_names_only, annotation_images, image_names_ordering
-
-    def parse_image_names(names: list[str]) -> tuple[list[str], dict[str, list[str]], list[str]]:
-        image_names_ordering = []
-        image_names_only = []
-        annotation_images = {}
-        for name in names:
-            if "@" in name:
-                image_name, annotation_names = name.split("@")
-                annotation_names = annotation_names.split("&")
-                for a in annotation_names:
-                    if a not in annotation_images:
-                        annotation_images[a] = []
-                    annotation_images[a].append(image_name)
-                image_names_ordering.append(image_name)
-            else:
-                image_names_only.append(name)
-                image_names_ordering.append(name)
-
-        return image_names_only, annotation_images, image_names_ordering
-
-    if paths is not None:
-        assert image_names is None, "image_names must be None if paths is specified"
-        image_names_only, annotation_images, image_names_ordering = parse_paths(paths)
-    elif image_names is not None:
-        assert paths is None, "paths must be None if image_names is specified"
-        # Theoretically, we could also parse the image names to paths and only use the paths function
-        # However, it is faster to use the image names directly if available (and we need image names anyway for the table)
-        image_names_only, annotation_images, image_names_ordering = parse_image_names(image_names)
-    else:
-        raise ValueError("image_names or paths must be supplied if dataset_names is None")
-
-    image_names = image_names_only + list(itertools.chain.from_iterable(annotation_images.values()))
+    assert image_names is not None, "image_names must be supplied if dataset_names is None"
+    dfs = []
     image_names = pd.unique(image_names)  # Unique without sorting
-    image_names_ordering = pd.unique(image_names_ordering)
+    remaining_images = set(image_names)
+    considered_tables = []
 
-    # First all the images without annotation name requirements
-    dfs = []
-    remaining_images = set(image_names_only)
-    considered_tables = set()
     for dataset_name in tables.keys():
         df = read_table(dataset_name, annotation_name)
         df = df.query("image_name in @remaining_images")
 
         if len(df) > 0:
             dfs.append(df)
             remaining_images = remaining_images - set(df["image_name"].values)
-            considered_tables.add(dataset_name)
+            considered_tables.append(dataset_name)
 
             if len(remaining_images) == 0:
                 # We already have all image_names, we can stop looping over the tables
                 break
 
-    # Then all images with annotation names
-    if len(annotation_images) > 0:
-        remaining_images = {name: set(images) for name, images in annotation_images.items()}
-        is_done = False
-        for dataset_name in tables.keys():
-            if is_done:
-                break
-
-            for table_annotation_name in tables[dataset_name].keys():
-                if table_annotation_name not in annotation_images.keys():
-                    # If the table does not contain any of the requested annotations, we can skip it
-                    continue
-
-                df = read_table(dataset_name, table_annotation_name)
-                df = df.query("image_name in @remaining_images[@table_annotation_name]")
-
-                if len(df) > 0:
-                    dfs.append(df)
-                    remaining_images[table_annotation_name] = remaining_images[table_annotation_name] - set(
-                        df["image_name"].values
-                    )
-                    considered_tables.add(dataset_name)
-
-                    if all(len(r) == 0 for r in remaining_images.values()):
-                        is_done = True
-                        # We already have all image_names, we can stop looping over the tables
-                        break
-
-    # We cannot assert that there are no remaining images anymore because some images may get excluded due to the label mapping or some images maybe don't even have annotations (so they can't be included)
     assert len(dfs) > 0, (
-        f"Could not find any of the requested images ({image_names = }, {annotation_images = }) in the tables"
-        f" ({considered_tables = }). This could mean that some of the intermediate files are missing or that you do not"
-        " have access to them (e.g. human data)"
+        f"Could not find all the requested images ({remaining_images = }). This could mean that some of the"
+        " intermediate files are missing or that you do not have access to them (e.g. human data)"
     )
-
     with warnings.catch_warnings():
         # The same columns might have different dtypes in the dataframes depending on missing values
         warnings.filterwarnings("ignore", message=".*object-dtype columns with all-bool values", category=FutureWarning)
         df = pd.concat(dfs)
     if len(dfs) > 1:
         # label_index is potentially incorrect when paths from multiple datasets are used, so it is safer to remove it
         df.drop(columns="label_index", inplace=True)
 
     # Same order as defined by the paths
     df["image_name"] = df["image_name"].astype("category")
-    df["image_name"] = df["image_name"].cat.set_categories(image_names_ordering)
+    df["image_name"] = df["image_name"].cat.set_categories(image_names)
     df.sort_values("image_name", inplace=True, ignore_index=True)
 
     # Make sure we have all requested image_names (it is possible that some image_names are missing if they contain only labels which were filtered out by the label mapping)
     image_names_df = set(df["image_name"].unique())
     assert image_names_df.issubset(image_names), (
         "Could not find all image_names in the median spectra tables. Please make sure that the median table exists for"
         " every dataset where the image_names come from"
@@ -508,27 +382,28 @@
     if label_ordering is None:
         # The masks dataset has a very comprehensive list of label order, try to use this as first default
         dsettings = DatasetSettings(settings.data_dirs.masks)
         label_ordering = dsettings.get("label_ordering", None)
 
     if label_ordering is None:
         # Last option, check every available dataset
-        for _, entry in settings.datasets:
+        for _, entry in settings.data_dirs:
             dsettings = DatasetSettings(entry["path_data"])
             label_ordering = dsettings.get("label_ordering", None)
             if label_ordering is not None:
                 break
 
     if label_ordering is None:
         settings.log.warning("Could not find a label ordering. Storage remains unsorted")
         return storage
 
     # 9999_ unknown labels are sorted alphabetically after the known labels
     if type(storage) == dict:
-        storage = dict(sorted(storage.items(), key=lambda pair: label_ordering.get(pair[0], f"9999_{pair[0]}")))
+        storage = sorted(storage.items(), key=lambda pair: label_ordering.get(pair[0], f"9999_{pair[0]}"))
+        storage = {key: value for key, value in storage}
     elif type(storage) == list or type(storage) == np.ndarray or type(storage) == set:
         storage = sorted(storage, key=lambda element: label_ordering.get(element, f"9999_{element}"))
     elif type(storage) == pd.DataFrame:
         sorter = lambda col: [label_ordering.get(v, f"9999_{v}") for v in col] if col.name == "label_name" else col
         if sorting_cols is None:
             sorting_cols = ["label_name"]
             if "image_name" in storage:
@@ -538,123 +413,98 @@
         storage = storage.sort_values(by=sorting_cols, key=sorter, ignore_index=True)
     else:
         settings.log.warning(f"Unsupported input type: {type(storage)}")
 
     return storage
 
 
-@automatic_numpy_conversion
-def sort_labels_cm(
-    cm: Union[torch.Tensor, np.ndarray], cm_order: list[str], target_order: list[str]
-) -> Union[torch.Tensor, np.ndarray]:
+def sort_labels_cm(cm: np.ndarray, cm_order: np.ndarray, target_order: np.ndarray) -> np.ndarray:
     """
     Sorts the rows/columns in a cm to a target order.
 
     >>> cm = np.array([[0, 10, 3], [1, 2, 3], [8, 6, 4]])
     >>> cm_order = ['b', 'a', 'c']
     >>> target_order = ['a', 'b', 'c']
     >>> sort_labels_cm(cm, cm_order, target_order)
-    array([[ 2,  1,  3],
-           [10,  0,  3],
-           [ 6,  8,  4]])
+    array([[ 2.,  1.,  3.],
+           [10.,  0.,  3.],
+           [ 6.,  8.,  4.]])
 
     Args:
         cm: Confusion matrix.
         cm_order: Name of the current rows/columns in the confusion matrix.
         target_order: Name of the new ordering of the confusion matrix.
 
     Returns: Sorted confusion matrix (based on the target order).
     """
     assert len(cm.shape) == 2 and cm.shape[0] == cm.shape[1], "cm must be square"
     assert len(cm_order) == len(target_order) and set(cm_order) == set(
         target_order
     ), "The same names must occur in the cm and the target order"
-    assert sorted(set(cm_order)) == sorted(cm_order), "The names must be unique"
+    assert np.unique(cm_order).tolist() == sorted(cm_order), "The names must be unique"
 
     # Swap rows
-    switched_cm = torch.zeros_like(cm)
+    switched_cm = np.zeros(np.shape(cm))
     ordering_indices = [cm_order.index(l) for l in target_order]
     for i, id in enumerate(ordering_indices):
         switched_cm[i, :] = cm[id, :]
 
     # Swap columns
-    switched_cm_final = torch.zeros_like(cm)
+    switched_cm_final = np.zeros(np.shape(cm))
     for j, id in enumerate(ordering_indices):
         switched_cm_final[:, j] = switched_cm[:, id]
 
     return switched_cm_final
 
 
-def execute_notebook(
-    notebook_path: Path, output_path: Path = None, parameters: dict[str, Any] = None, html_only: bool = True
+def run_experiment_notebook(
+    notebook_path: Path, output_path: Path, html_only: bool = False, **papermill_kwargs
 ) -> None:
     """
-    Runs the given notebook and stores it in a new location. Additionally, a compressed HTML version of the notebook can be stored at the output location.
-
-    Note: This function provides similar functionality to papermill (https://papermill.readthedocs.io/en/latest/).
+    Runs the experiment notebook and stores it in a new location. Additionally, a compressed HTML version of the notebook is stored at the same location.
 
     Args:
-        notebook_path: Path to the base notebook (e.g. ExperimentAnalysis.ipynb).
-        output_path: Path to the output file. If a directory, the same name as the notebook will be used. If None, the notebook is only executed without saving it.
-        parameters: Dictionary with (key, value) pairs denoting the parameters for the notebook. Please add a cell to your notebook with the tag `parameters` and add your defaults there. A new cell will be inserted after the tagged cell so that the new values instead of the defaults are used. If None, then the notebook is not changed.
+        notebook_path: Path to the base notebook.
+        output_path: Path to the output file. If a directory, the same name as the notebook will be used.
         html_only: If True, store only the HTML file and no notebook at the output location. Note that the notebook is stored uncompressed and can hence be much larger than the HTML file so this option is useful to save some space.
+        **papermill_kwargs: Additional keyword arguments passed to papermill.
     """
     from htc.utils.visualization import compress_html
 
-    nb = nbformat.read(notebook_path, as_version=nbformat.NO_CONVERT)
-
-    if parameters is not None:
-        # Create a new cell with the parameters
-        src = "# Injected parameters\n"
-        src += "\n".join([f"{k} = {v!r}" for k, v in parameters.items()])
-        parameters_cell = nbformat.v4.new_code_cell(src)
-
-        # We need to insert the parameter cell after the existing parameter cell so that we overwrite existing defaults
-        insertion_index = None
-
-        for i, cell in enumerate(nb.cells):
-            if "tags" in cell["metadata"] and "parameters" in cell["metadata"]["tags"]:
-                insertion_index = i
-                break
-        assert insertion_index is not None
-        insertion_index += 1
+    if output_path.is_dir():
+        output_path = output_path / notebook_path.name
+    output_path.parent.mkdir(parents=True, exist_ok=True)
+
+    # Ignore non-applicable warning
+    logging.getLogger("papermill.translators").addFilter(lambda record: "Black is not installed" not in record.msg)
+
+    pm.execute_notebook(
+        input_path=notebook_path,
+        output_path=output_path,
+        **papermill_kwargs,
+    )
 
-        nb.cells = [*nb.cells[:insertion_index], parameters_cell, *nb.cells[insertion_index:]]
+    res = subprocess.run(f"jupyter nbconvert --stdout --to html {output_path}", shell=True, capture_output=True)
+    assert res.returncode == 0, f"Could not convert the notebook {output_path} to html"
+    html = res.stdout.decode()
+    compress_html(output_path.with_suffix(".html"), html)
 
-    # Notebook execution docs: https://nbclient.readthedocs.io/en/latest/client.html
-    # Exporter docs: https://nbconvert.readthedocs.io/en/latest/nbconvert_library.html
-    c = NBConfig()
-    execution_dir = notebook_path.parent
-    c.HTMLExporter.preprocessors = [ExecutePreprocessor(timeout=600, resources={"metadata": {"path": execution_dir}})]
-    exporter = HTMLExporter(config=c)
-
-    # Run the notebook
-    (html, _) = exporter.from_notebook_node(nb)
-
-    if output_path is not None:
-        if output_path.is_dir():
-            output_path = output_path / notebook_path.name
-        output_path.parent.mkdir(parents=True, exist_ok=True)
-
-        # Save everything
-        compress_html(output_path.with_suffix(".html"), html)
-        if not html_only:
-            nbformat.write(exporter.preprocessors[0].nb, output_path)
+    if html_only:
+        output_path.unlink()
 
 
 def get_valid_run_dirs(training_dir: Path = None) -> list[Path]:
     # If a run folder starts with one of the following prefixes, it should be ignored
-    # The dataset size experiment is very special and does not have all the required files
-    excluded_prefixes = ("running", "test", "special", "error", settings_seg.dataset_size_timestamp)
+    excluded_prefixes = ("running", "test", "special", "error")
 
     run_dirs = []
     if training_dir is None:
         training_dir = settings.training_dir
     for run_dir in sorted(training_dir.glob("*/*")):
-        if settings.datasets.network in run_dir.parents:
+        if settings.data_dirs.network in run_dir.parents:
             continue
         if not run_dir.is_dir():
             continue
         if run_dir.stem.startswith(excluded_prefixes):
             continue
 
         run_dirs.append(run_dir)
```

## htc/utils/import_extra.py

```diff
@@ -36,15 +36,14 @@
 
                     pytest.skip(f"{missing_library} is missing but required for the test")
                 except ImportError:
                     pass
 
                 raise ImportError(
                     f"{missing_library} library is missing. You can fix this by installing the extra dependencies"
-                    " (`imsy-htc[extra]`) as described in the README or by installing additional dependencies as described"
-                    " in the documentation of the class/function you want to use."
+                    " (`imsy-htc[extra]`) as described in the README."
                 )
             return function(*args, **kwargs)
 
         return wrapper
 
     return decorator
```

## htc/utils/parallel.py

```diff
@@ -36,15 +36,14 @@
 
 def p_imap(
     func: Callable,
     *iterables: Iterable,
     num_cpus: Union[int, float] = None,
     task_name: str = "Working...",
     hide_progressbar: bool = False,
-    use_threads: bool = False,
 ) -> Generator:
     """
     Iterate in parallel over a function with one or more iterables. Items are processed and returned in order as soon as they are finished. A progress bar (using the rich library) will be printed during execution.
 
     This function is similar to p_imap from the p_tqdm package (https://github.com/swansonk14/p_tqdm) but offers Python 3.10 support and uses rich for the progress bar.
 
     >>> a = [1, 2, 3]
@@ -52,39 +51,34 @@
     >>> list(p_imap(pow, a, b))  # doctest: +ELLIPSIS
     Working...
     [1, 4, 27]
 
     Args:
         func: Function to call on each item by the subprocesses.
         iterables: One or more iterables to pass on to the function (multiple iterables map to multiple arguments).
-        num_cpus: Number of processes to use (defaults to the number of real (not logical) CPU cores in the system). May also be a factor to denote int(num_cpus * n_cpus).
+        num_cpus: Number of processes to use (defaults to the number of installed processes in the system). May also be a factor to denote int(num_cpus * n_cpus).
         task_name: Name of the task which will be printed left to the progress bar.
         hide_progressbar: If True, do not show a progress bar.
-        use_threads: If True, use a thread pool instead of a processing pool. Python does not have real multi-threading (due to the GIL) but multiple threads may still result in better CPU utilization if external libraries (like numpy or torch) are used or if the task is I/O-heavy. Threads may be more stable than processes which is useful in Jupyter notebooks (e.g. cells can be executed multiple times without kernel restarts).
 
     Yields: Processed items.
     """
     iterable_lengths = {len(i) for i in iterables}
     assert len(iterable_lengths) == 1, "All iterables must have the same length"
 
     if num_cpus is None:
         num_cpus = psutil.cpu_count(logical=False)
     elif type(num_cpus) == float:
         num_cpus = int(round(num_cpus * psutil.cpu_count(logical=False)))
 
     with Progress(
-        *Progress.get_default_columns(),
-        TimeElapsedColumn(),
-        ProcessingSpeedColumn(),
-        disable=hide_progressbar,
-        refresh_per_second=1,
+        *Progress.get_default_columns(), TimeElapsedColumn(), ProcessingSpeedColumn(), disable=hide_progressbar
     ) as progress:
         task_id = progress.add_task(f"[cyan]{task_name}[/]", total=next(iter(iterable_lengths)))
 
-        pool = mpp.ThreadPool(num_cpus) if use_threads else mpp.Pool(num_cpus)
+        pool = mpp.Pool(num_cpus)
         for item in istarmap(pool, func, zip(*iterables)):
             progress.advance(task_id)
             yield item
 
         # We need to properly close the pool for correct coverage (https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html)
         pool.close()
         pool.join()
```

## htc/utils/paths.py

```diff
@@ -9,15 +9,15 @@
 from htc.settings import settings
 from htc.settings_seg import settings_seg
 from htc.tivita.DataPath import DataPath
 
 
 def filter_semantic_labels_only(path: "DataPath") -> bool:
     labels = path.annotated_labels()
-    if any(l in settings_seg.labels[1:] for l in labels):
+    if any([l in settings_seg.labels[1:] for l in labels]):
         # We are only interested in images with one of the organs which we use for training (without background)
         return True
     else:
         return False
 
 
 def filter_min_labels(path: "DataPath", min_labels: int = 1) -> bool:
@@ -37,15 +37,15 @@
 
 class ParserPreprocessing:
     def __init__(self, description: str):
         self.parser = argparse.ArgumentParser(
             description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter
         )
         self.parser.add_argument(
-            "--spec",
+            "--specs",
             required=False,
             type=Path,
             default=None,
             help=(
                 "Path or name to the data specification file in which case all paths from this file will be used"
                 " (including the test set). Please note that you should still either provide the --dataset-name"
                 " argument for a default intermediates folder or the --output-path argument."
@@ -65,15 +65,15 @@
         self.parser.add_argument(
             "--dataset-name",
             required=False,
             default=None,
             help=(
                 "Name of the dataset (e.g. name of the corresponding folder on the network drive). This will also be"
                 " used to set the default intermediates directory, i.e. the generated files will be stored in the"
-                " intermediates directory corresponding to the given dataset name. If both --spec and --dataset-path"
+                " intermediates directory corresponding to the given dataset name. If both --specs and --dataset-path"
                 " are None, then the paths will be collected from the dataset."
             ),
         )
         self.parser.add_argument(
             "--output-path",
             required=False,
             type=Path,
@@ -98,27 +98,27 @@
                 "To regenerate the files, even if the files are already stored in the output location (for scripts with"
                 " output files e.g. L1 processing)."
             ),
         )
 
     def get_paths(self, filters: Union[list[Callable[["DataPath"], bool]], None] = None) -> list[DataPath]:
         self.args = self.parser.parse_args()
-        if self.args.spec is not None:
-            assert self.args.dataset_path is None, "--dataset-path is not used if --spec is given"
-            specs = DataSpecification(self.args.spec)
+        if self.args.specs is not None:
+            assert self.args.dataset_path is None, "--dataset-path is not used if --specs is given"
+            specs = DataSpecification(self.args.specs)
             specs.activate_test_set()
             paths = specs.paths()
         elif self.args.dataset_path is not None:
-            assert self.args.spec is None, "--spec is not used if --dataset-path is given"
+            assert self.args.specs is None, "--specs is not used if --dataset-path is given"
             paths = list(DataPath.iterate(self.args.dataset_path))
         else:
             if self.args.dataset_name == "2021_02_05_Tivita_multiorgan_masks":
                 paths = list(DataPath.iterate(settings.data_dirs[self.args.dataset_name], filters))
                 paths += list(DataPath.iterate(settings.data_dirs[self.args.dataset_name] / "overlap", filters))
             else:
                 paths = list(DataPath.iterate(settings.data_dirs[self.args.dataset_name], filters))
 
         if self.args.dataset_name is not None:
             # From now on, we write to the intermediates directory of the selected dataset
-            settings.intermediates_dir_all.set_default_location(self.args.dataset_name)
+            settings.intermediates_dir.set_default_location(self.args.dataset_name)
 
         return paths
```

## htc/utils/unify_path.py

```diff
@@ -1,41 +1,30 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import os
 from pathlib import Path
 from typing import Union
 
 
-def unify_path(path: Union[str, Path], resolve_symlinks: bool = True) -> Path:
+def unify_path(path: Union[str, Path]) -> Path:
     """
     Tries to bring some consistency to paths:
         - Resolve home directories (~  /home/username).
         - Make paths absolute.
-        - Resolves symbolic links (optional).
 
     Note: this requires access to the filesystem and when the path points to a high-latency location, then this may introduce a lag.
 
     Args:
         path: The original path.
-        resolve_symlinks: If true, also resolve symbolic links.
 
-    Returns: The unified path.
+    Returns:
+        The unified path.
     """
     if isinstance(path, str):
         path = Path(path)
 
-    # Unfortunately, the resolve() function cannot handle paths starting with ~. The workaround is to expand ~ to the home path in this case
-    path = path.expanduser()
+    if str(path).startswith("~"):
+        # Unfortunately, the resolve() function cannot handle paths starting with ~. The workaround is to expand ~ to the home path in this case
+        path = path.expanduser()
 
-    if resolve_symlinks:
-        # Normalize, make absolute and resolve symlinks
-        return path.resolve()
-    else:
-        path = str(path)
-        if path.startswith("//"):
-            # This is not done by abspath
-            path = path.replace("//", "/")
-
-        # Normalize and make absolute
-        path = os.path.abspath(path)  # noqa: PL100
-        return Path(path)
+    # Normalize the path (this makes it also absolute)
+    return path.resolve()
```

## htc/utils/visualization.py

```diff
@@ -19,15 +19,15 @@
 from matplotlib.colors import to_rgba
 from PIL import Image
 from plotly.colors import n_colors as generate_n_colors
 from plotly.subplots import make_subplots
 from scipy import stats
 
 from htc.cpp import tensor_mapping
-from htc.evaluation.metrics.ECE import ECE
+from htc.evaluation.metrics.ECELoss import ECELoss
 from htc.evaluation.metrics.scores import dice_from_cm
 from htc.models.common.MetricAggregation import MetricAggregation
 from htc.models.data.DataSpecification import DataSpecification
 from htc.settings import settings
 from htc.settings_seg import settings_seg
 from htc.tivita.colorscale import tivita_colorscale
 from htc.tivita.DataPath import DataPath
@@ -52,28 +52,28 @@
     >>> path = DataPath.from_image_name("P044#2020_02_01_09_51_15")
     >>> sample = DatasetImage([path], train=False, config=Config({"input/no_features": True}))[0]
     >>> fig = create_segmentation_overlay(sample['labels'].numpy(), path)
     >>> html = fig.to_html(include_plotlyjs="cdn", div_id='segmentation')
 
     Original file size in MiB:
     >>> len(html) / 2**20  # doctest: +ELLIPSIS
-    5.10...
+    4.70...
     >>> import tempfile
     >>> with tempfile.NamedTemporaryFile() as tmpfile:
     ...    tmpfile = Path(tmpfile.name)
     ...    compress_html(tmpfile, html)
     ...    compressed_size = tmpfile.stat().st_size
 
     Compressed file size in MiB:
     >>> compressed_size / 2**20  # doctest: +ELLIPSIS
-    0.63...
+    0.62...
 
     Compression ratio:
     >>> compressed_size / len(html)  # doctest: +ELLIPSIS
-    0.12...
+    0.131...
 
     Args:
         file: Path to the output file. If none, the resulting HTML string will be returned.
         fig_or_html: Either a Plotly figure object or an HTML string.
     """
 
     # The following code is based on https://github.com/six-two/self-unzip.html/blob/main/python/self_unzip_html/__init__.py
@@ -132,64 +132,23 @@
     html_encoded = template.replace("{{DATA}}", compressed_data)
     JS_REPLACE = (
         "const og_text=new TextDecoder().decode(og_data);window.onload=()=>{let"
         ' n=document.open("text/html","replace");n.write(og_text);n.close()}'
     )
     html_encoded = html_encoded.replace("{{CODE}}", JS_REPLACE)
 
-    html = (
-        html_encoded
-        if len(bytes(html_encoded, encoding="utf-8")) < len(bytes(html_content, encoding="utf-8"))
-        else html_content
-    )
+    html = html_encoded if len(html_encoded) < len(html_content) else html_content
 
     if file is None:
         return html
     else:
-        with file.open("w", encoding="utf-8") as f:
+        with file.open("w") as f:
             f.write(html)
 
 
-def add_figcaption(fig_html: str, title: str, caption: str) -> str:
-    css_figcaption = """
-figure {
-    /* Same font as plotly */
-    font-family: "Open Sans", verdana, arial, sans-serif;
-    width: min-content;
-    margin-bottom: 25px;
-}
-figure > figcaption {
-    margin-left: 15px;
-    margin-right: 15px;
-}
-figcaption {
-    text-align: center;
-    margin-top: 10px;
-}
-"""
-
-    return f"""
-<!DOCTYPE html>
-<html lang="en">
-    <head>
-        <title>{title}</title>
-        <meta charset="utf-8">
-        <style>
-        {css_figcaption}
-        </style>
-    </head>
-    <body>
-        <figure>
-            {fig_html}
-            <figcaption>{caption}</figcaption>
-        </figure>
-    </body>
-</html>"""
-
-
 def visualize_dict(data: Union[dict, list]) -> None:
     """
     Interactive visualization of a Python dictionary (to be used in a Jupyter notebook). The interactive visualization also works in exported HTML notebooks.
 
     Args:
         data: Python dict to visualize.
     """
@@ -198,21 +157,25 @@
 
     # Embed renderjson code (from here: https://github.com/caldwell/renderjson)
     with (Path(__file__).parent / "renderjson.js").open() as f:
         renderjson = f.read()
 
     # Embed everything into the HTML (this way it also works in the exported HTML notebook)
     div_id = uuid.uuid4()
-    display(HTML(f"""
+    display(
+        HTML(
+            f"""
 <div id="{div_id}" style="height: auto; width:100%;"></div>
 <script>
     {renderjson}
     renderjson.set_show_to_level(1);
     document.getElementById('{div_id}').appendChild(renderjson({json_str}));
-</script>"""))
+</script>"""
+        )
+    )
 
 
 def create_running_metric_plot(df: pd.DataFrame, metric_name: str = "dice_metric") -> go.Figure:
     fig = go.Figure()
 
     fold_values = []
 
@@ -321,15 +284,15 @@
                 df_epoch = df_fold.query(f"epoch_index == {epoch}")
 
                 # Collect all ece values from all images and calculate the ece error based on these values
                 # This is more correct than averaging over the ece errors from all images
                 acc_mat = np.stack([v["accuracies"] for v in df_epoch["ece"]])
                 conf_mat = np.stack([v["confidences"] for v in df_epoch["ece"]])
                 prob_mat = np.stack([v["probabilities"] for v in df_epoch["ece"]])
-                ece = ECE.aggregate_vectors(acc_mat, conf_mat, prob_mat)
+                ece = ECELoss.aggregate_vectors(acc_mat, conf_mat, prob_mat)
                 ece_error.append(ece["error"])
 
             line = {
                 "color": plotly.colors.DEFAULT_PLOTLY_COLORS[f % len(plotly.colors.DEFAULT_PLOTLY_COLORS)],
                 "width": 2,
             }
             marker = {"size": 6}
@@ -357,15 +320,15 @@
 
 
 def show_activation_image(df: pd.DataFrame, hist_config: dict, dataset_index: int, epoch: int = None) -> None:
     # Combine activations from all images
     if epoch is None:
         activations = df[(df["dataset_index"] == dataset_index)]["val/activations"].values
     else:
-        activations = df[(df["epoch_index"] == epoch - 1) & (df["dataset_index"] == dataset_index)][
+        activations = df[(df["epoch_indexx"] == epoch - 1) & (df["dataset_index"] == dataset_index)][
             "val/activations"
         ].values
     layer_counts = {}
 
     for key in activations[0].keys():
         layer_counts[key] = np.sum(
             [a[key]["counts"] for a in activations], axis=0
@@ -396,15 +359,15 @@
             values_range, np.ceil(counts * 5000).astype(np.int)
         )  # ceil ensures that every value with a probability > 0 gets sampled at least once
         layer_mean[name] = np.mean(samples)
         layer_std[name] = np.std(samples)
 
         fig.add_trace(go.Violin(x=samples, line_color=color, bandwidth=hist_config["step"], name=name), row=1, col=1)
 
-        if all(excluded not in name for excluded in ["pool", "logits", "input", "Model"]):
+        if all([excluded not in name for excluded in ["pool", "logits", "input", "Model"]]):
             samples = F.elu(torch.from_numpy(samples))
             fig.add_trace(go.Violin(x=samples, line_color=color, name=f"elu({name})"), row=1, col=1)
 
     fig.update_traces(orientation="h", side="positive", width=3, points=False, row=1, col=1)
     fig.update_xaxes(title_text="Activations", row=1, col=1)
     fig.update_yaxes(title_text="Layer", row=1, col=1)
 
@@ -578,19 +541,18 @@
             col=1,
         )
         line_ids.append(f)
 
         # Plot a line for each class label
         for label in df_epochs["label"].unique():
             df_label = df_epochs.query(f'label == "{label}"').sort_values(by=["epoch_index"])
-            df_dice = df_label.groupby("epoch_index")["dice"]
 
-            mean_dice = df_dice.mean().values
-            min_dice = mean_dice - df_dice.min().values
-            max_dice = df_dice.max().values - mean_dice
+            mean_dice = df_label.groupby("epoch_index").mean()["dice"].values
+            min_dice = mean_dice - df_label.groupby("epoch_index").min()["dice"].values
+            max_dice = df_label.groupby("epoch_index").max()["dice"].values - mean_dice
 
             fig.add_trace(
                 go.Scatter(
                     x=df_label["epoch_index"].unique(),
                     y=mean_dice,
                     error_y={"type": "data", "symmetric": False, "array": max_dice, "arrayminus": min_dice},
                     mode="lines+markers",
@@ -723,15 +685,15 @@
     Args:
         colors: List of color tuples in the rgb format, e.g. (0.1, 0.5, 1.0) or rgba, e.g. (0.1, 0.5, 1.0, 0.8).
 
     Returns: Colormapping which can be passed to Plotly's colorscale argument.
     """
     assert len(colors) >= 1, "At least one color required"
     assert len(colors[0]) == 3 or len(colors[0]) == 4, "Exactly 3 (rgb) or 4 (rgba) color values required"
-    assert all(all(0 <= v <= 1 for v in c) for c in colors), "All color values must be in the range [0;1]"
+    assert all([all([0 <= v <= 1 for v in c]) for c in colors]), "All color values must be in the range [0;1]"
 
     # Create a color mapping according to plotlys documentation (list of normalized values and corresponding colors)
     color_mapping = []
     max_label = len(colors) - 1
     default_opacity = 1
 
     if max_label == 0:
@@ -854,31 +816,28 @@
 
 
 def create_overview_document(
     path: DataPath,
     include_tpi: bool = False,
     navigation_paths: list[DataPath] = None,
     navigation_link_callback: Callable[[str, str, DataPath], str] = None,
-    nav_width: str = "23em",
 ) -> str:
     """
     Create an overview figure for the given image. It will show the RGB image with all the available annotations plus the tissue parameter images.
 
     Args:
         path: Data path to the image.
         include_tpi: If True, TPI images are included in the overview document (adds around 10 MiB to the output).
         navigation_paths: If not None, will add a navigation bar with all links sorted by organ. The user can use this navigation bar to easily switch between images.
         navigation_link_callback: Callback which receives the label name, number and data path of the target image and should create a relative link where the corresponding local html file for the target image can be found. If parts of the link contain invalid URL characters (e.g. # in image name), then please wrap it in quote_plus before (e.g. quote_plus(p.image_name())). For example, ('spleen', '08', DataPath) --> '../08_spleen/P086%232021_04_15_09_22_20.html'.
-        nav_width: Width of the navigation bar (in CSS units).
 
     Returns: HTML string which is best saved with the `compress_html()` function.
     """
     seg = path.read_segmentation(annotation_name="all")
-    if seg is None or len(path.annotated_labels(annotation_name="all")) == 0:
-        # No annotations available, only show the RGB image
+    if seg is None:
         rgb_image = path.read_rgb_reconstructed()
         fig_seg = px.imshow(rgb_image)
 
         # Similar size as create_segmentation_overlay()
         img_height, img_width = rgb_image.shape[:2]
         fig_seg.update_layout(
             height=img_height * 1.5, width=img_width * 1.53, template="plotly_white", margin=dict(t=40)
@@ -925,102 +884,68 @@
         meta_html = ""
 
     if navigation_paths is not None:
         assert (
             navigation_link_callback is not None
         ), "navigation_link_callback must be provided if a navigation pane should be created"
 
-        # Use the label ordering if available
-        masks_settings = DatasetSettings(settings.data_dirs.masks)
-        if "label_ordering" in path.dataset_settings:
-            label_ordering = path.dataset_settings.get("label_ordering", {})
-        else:
-            label_ordering = masks_settings.get("label_ordering", {})
-
         # All paths per organ (there may be more than one organ per image)
         image_labels = path.annotated_labels(annotation_name="all")
         label_paths = {}
-        used_paths = []
         for p in navigation_paths:
             labels = p.annotated_labels(annotation_name="all")
-
-            # We only create overview files for images which have at least one label
-            if len(labels) > 0:
-                used_paths.append(p)
-
             for l in labels:
                 if l not in label_paths:
                     label_paths[l] = []
                 label_paths[l].append(p)
 
         label_paths = sort_labels(label_paths)
-
-        # Previous/Next image according to the sorting of the navigation paths
-        prev_image = None
-        next_image = None
-        for i, p in enumerate(used_paths):
-            if p == path:
-                if i > 0:
-                    prev_path = used_paths[i - 1]
-                    label_name = prev_path.annotated_labels(annotation_name="all")[0]
-                    prev_image = {
-                        "path": prev_path,
-                        "label_name": label_name,
-                        "label_number": label_ordering.get(label_name, None),
-                    }
-                if i < len(used_paths) - 1:
-                    next_path = used_paths[i + 1]
-                    label_name = next_path.annotated_labels(annotation_name="all")[0]
-                    next_image = {
-                        "path": next_path,
-                        "label_name": label_name,
-                        "label_number": label_ordering.get(label_name, None),
-                    }
+        masks_settings = DatasetSettings(settings.data_dirs.masks)
 
         details_html = ""
         link_index = 0  # Global index for each link in the list (for scrolling)
         for l in label_paths.keys():
             label_paths[l] = sorted(label_paths[l])
 
             # Use the label ordering if available
-            label_number = label_ordering.get(l, None)
+            if "label_ordering" in path.dataset_settings:
+                label_number = path.dataset_settings["label_ordering"][l]
+            else:
+                if l in masks_settings["label_ordering"]:
+                    label_number = masks_settings["label_ordering"][l]
+                else:
+                    label_number = None
 
             # List with links to all paths of the current label
             paths_html = ""
             for p in label_paths[l]:
                 selected = 'class="selected" ' if p == path else ""
 
                 label_meta = p.meta(f"label_meta/{l}")
-                if label_meta is not None and "situs" in label_meta:
+                if label_meta is not None:
                     meta = (
                         f"<br>situs={label_meta['situs']}, angle={label_meta['angle']},"
                         f" repetition={label_meta['repetition']}"
                     )
                 else:
                     meta = ""
 
-                invisible_meta = p.meta("annotation_name")
-                if invisible_meta is not None:
-                    invisible_meta = " ".join(invisible_meta)
-                    invisible_meta = f'<span class="invisible">{invisible_meta}</span>'
-                else:
-                    invisible_meta = ""
-
                 link = navigation_link_callback(l, label_number, p) + f"?nav=show&link_index={link_index}"
-                paths_html += (
-                    f'<li><a id="link_{link_index}"'
-                    f' {selected}href="{link}">{p.image_name()}{meta}{invisible_meta}</a></li>'
-                )
+                paths_html += f'<li><a id="link_{link_index}" {selected}href="{link}">{p.image_name()}{meta}</a></li>'
                 link_index += 1
 
             # Add an image for the current label if available
             if (path.data_dir / "extra_label_symbols").exists():
                 svg_path = path.data_dir / "extra_label_symbols" / f"Cat_{label_number}_{l}.svg"
             else:
-                svg_path = settings.data_dirs.masks / "extra_label_symbols" / f"Cat_{label_ordering.get(l, '')}_{l}.svg"
+                svg_path = (
+                    settings.data_dirs.masks
+                    / "extra_label_symbols"
+                    / f"Cat_{masks_settings['label_ordering'].get(l, '')}_{l}.svg"
+                )
 
             if svg_path.exists():
                 with svg_path.open("r") as f:
                     svg = f.read()
                     svg_base64 = base64.b64encode(svg.encode()).decode()
                 label_image = f" <img class='label_image' src='data:image/svg+xml;base64,{svg_base64}' />"
             else:
@@ -1034,205 +959,76 @@
                 summary_name = f"{l}{label_image}"
 
             details_html += (
                 f"<details id='{l}'"
                 f" {details_open}><summary{selected}>{summary_name}</summary><ul>{paths_html}</ul></details>"
             )
 
-        if prev_image is not None:
-            prev_link = navigation_link_callback(
-                prev_image["label_name"], prev_image["label_number"], prev_image["path"]
-            )
-            prev_link = (
-                f'<a class="prev_next" href="{prev_link}" title="Previous image (according to the sorting of the'
-                f' filesystem): {prev_image["path"].image_name()}"> previous</a>'
-            )
-        else:
-            prev_link = (
-                '<span class="prev_next no_link" title="No previous image available (first image of the dataset)">'
-                " previous</span>"
-            )
-        if next_image is not None:
-            next_link = navigation_link_callback(
-                next_image["label_name"], next_image["label_number"], next_image["path"]
-            )
-            next_link = (
-                f'<a class="prev_next" href="{next_link}" title="Next image (according to the sorting of the'
-                f' filesystem): {next_image["path"].image_name()}">next </a>'
-            )
-        else:
-            next_link = (
-                '<span class="prev_next no_link" title="No next image available (last image of the dataset)">'
-                " next</span>"
-            )
-
-        search_function = """
-function searchAll() {
-    // We first search all details elements (labels) and then all list elements (images)
-    const needle = document.getElementById("nav_search").value.toLowerCase();
-    document.querySelectorAll("#image_navigation details").forEach(function (detailsElement) {
-        if (detailsElement.textContent.toLowerCase().includes(needle)) {
-            detailsElement.style.display = "";
-
-            // Show only the images which match
-            // If no image matches (e.g. search for organs), show all
-            let imageList = detailsElement.getElementsByTagName("ul")[0];
-            if (imageList.textContent.toLowerCase().includes(needle)) {
-                for (let listElement of imageList.getElementsByTagName("li")) {
-                    if (listElement.textContent.toLowerCase().includes(needle)) {
-                        listElement.style.display = "";
-                    } else {
-                        listElement.style.display = "none";
-                    }
-                }
-            }
-        } else {
-            detailsElement.style.display = "none";
-        }
-    });
-
-    // Find out which elements of the list are visible
-    let nVisible = 0;
-    document.querySelectorAll("#image_navigation details").forEach(function (detailsElement) {
-        if (detailsElement.style.display != "none") {
-            for (let listElement of detailsElement.getElementsByTagName("li")) {
-                if (listElement.style.display != "none") {
-                    nVisible++;
-                }
-            }
-        }
-    });
-
-    // If filtering is active, display the number of visible elements
-    let searchCount = document.getElementById("search_count");
-    const allListElements = document.querySelectorAll("#image_navigation li");
-    searchCount.innerText = nVisible + " of " + allListElements.length + " elements shown";
-    if (nVisible < allListElements.length) {
-        searchCount.style.display = "";
-    } else {
-        searchCount.style.display = "none";
-    }
-}
-"""
-        nav_html = """
-<span id="nav_link" onclick="openNav()">&#9776; Image selection</span>{}{}
+        nav_html = (
+            """
+<span style="font-size:30px;cursor:pointer" onclick="openNav()">&#9776; Image selection</span>
 
 <nav id="image_navigation">
   <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
-  <input type="text" id="nav_search" onkeyup="searchAll()" placeholder="Search all.." title="Search for everything which is visible in the navigation panel plus the following invisible attributes: annotation_name">
-  <p id="search_count"></p>
-  {}
+  %s
 </nav>
 <script>
-{}
-function openNav() {{
-  document.getElementById("image_navigation").style.width = "{}";
-}}
+function openNav() {
+  document.getElementById("image_navigation").style.width = "23em";
+}
 
-function closeNav() {{
+function closeNav() {
   document.getElementById("image_navigation").style.width = "0px";
-}}
+}
 
-document.addEventListener('DOMContentLoaded', function() {{
+document.addEventListener('DOMContentLoaded', function() {
   // Set previous navigation state
   const urlParams = new URLSearchParams(location.search);
-  if (urlParams.has("nav") && urlParams.get("nav") == "show") {{
+  if (urlParams.has("nav") && urlParams.get("nav") == "show") {
     openNav();
-  }}
+  }
 
   let isNavOpen = false;
   let initialScroll = false;  // Scroll to the selected link on page load
-  document.getElementById("image_navigation").addEventListener('transitionend', function() {{
+  document.getElementById("image_navigation").addEventListener('transitionend', function() {
     isNavOpen = document.getElementById("image_navigation").style.width != "0px";
 
-    if (!initialScroll && urlParams.has("link_index")) {{
+    if (!initialScroll && urlParams.has("link_index")) {
       let current_link = document.getElementById("link_" + urlParams.get("link_index"));
-      current_link.scrollIntoView({{behavior: "smooth", block: "center", inline: "nearest"}});
+      current_link.scrollIntoView({behavior: "smooth", block: "center", inline: "nearest"});
       initialScroll = true;
-    }}
-  }});
+    }
+  });
 
-  window.addEventListener('click', function(e) {{
-    if (isNavOpen && !document.getElementById('image_navigation').contains(e.target)) {{
+  window.addEventListener('click', function(e) {
+    if (isNavOpen && !document.getElementById('image_navigation').contains(e.target)) {
       // If the user clicks outside the navigation bar, the navigation should close
       closeNav();
-    }}
-  }});
-
-  if (urlParams.has("search_text")) {{
-    // Apply an existing search text to the navigation bar
-    document.getElementById("nav_search").value = urlParams.get("search_text");
-    searchAll();
-  }}
-
-  // Pass the current search text to every link in the navigation bar
-  const changeURL = function(event) {{
-    // Stop the link from redirecting
-    event.preventDefault();
-
-    // Redirect instead with JavaScript
-    const needle = document.getElementById("nav_search").value.toLowerCase();
-    if (needle != "") {{
-        let url = new URL(event.target.href);
-        url.searchParams.set("search_text", needle);
-        return url.href;
-    }} else {{
-        return event.target.href;
-    }}
-  }};
-
-  // We need to manually handle click (and middle click) events for the links
-  for (let link of document.querySelectorAll("#image_navigation a")) {{
-    link.addEventListener('click', function(event) {{
-        // Open link in the same window
-        const newURL = changeURL(event);
-        window.location.href = newURL;
-    }}, false);
-    link.addEventListener('auxclick', function(event) {{
-        if (event.button == 1) {{
-            // Middle click (open page in a new window)
-            const newURL = changeURL(event);
-            window.open(newURL, '_blank');
-        }}
-    }}, false);
-  }}
-}});
+    }
+  });
+});
 </script>
-""".format(prev_link, next_link, details_html, search_function, nav_width)
+"""
+            % details_html
+        )
 
         nav_css = """
 <style>
 nav {
   height: 100%;
   width: 0;
   position: fixed;
   z-index: 1;
   top: 0;
-  right: 0;
+  left: 0;
   background-color: #f7f7f7;
-  box-shadow: 2px 0px 5px #9b9b9b;
+  overflow-x: hidden;
   transition: 0.5s;
-  overflow-x: scroll;
   padding-top: 30px;
-  padding-left: 5px;
-  /* Move the scrollbar to the left side */
-  direction: rtl;
-}
-
-/* The remaining elements should still be left-aligned */
-nav * {
-    direction: ltr;
-}
-#nav_search {
-    display: block;
-    margin-right: auto;
-}
-#nav_search, #search_count {
-    margin-left: 8px;
-    margin-bottom: 5px;
+  box-shadow: 2px 0px 5px #9b9b9b;
 }
 
 #prev_image {
   margin-left: 5px;
 }
 
 #next_image {
@@ -1246,82 +1042,46 @@
   text-decoration: none;
   font-size: 1.3em;
   color: #818181;
   transition: 0.3s;
   cursor: pointer;
 }
 
-nav > details:last-child {
-    /* Leave enough space at the bottom so that the user knows when the list ends */
-    padding-bottom: 60px;
-}
-
 nav ul {
   margin-top: 0;
   margin-bottom: 0;
 }
 
 nav a {
   color: gray;
   text-decoration: none;
 }
 
 nav a:hover {
   text-decoration: underline;
 }
 
-nav details a {
-  /* Leave enough space on the right so that it is still possible to select image names */
-  margin-right: 2em;
-}
-
 nav .closebtn {
   position: absolute;
   top: 0;
-  right: 15px;
+  right: 25px;
   font-size: 36px;
   margin-left: 50px;
 }
 
 .selected {
   font-weight: bold;
 }
 
 .label_image {
   width: 1.5em;
   position: relative;
   top: 0.35em;
 }
 
-.invisible {
-    display: none;
-}
-
-a {
-  text-decoration: none;
-}
-a:hover {
-  text-decoration: underline;
-}
-#nav_link {
-  font-size: 2em;
-  cursor: pointer;
-  padding-right: 10px;
-}
-#nav_link:hover {
-    text-decoration: underline;
-}
-.prev_next {
-    font-size: 1.5em;
-    padding-right: 10px;
-}
-.no_link {
-    color: #aaa;
-}
-
 @media screen and (max-height: 450px) {
   nav {padding-top: 15px;}
   nav a {font-size: 18px;}
 }
 </style>
 """
     else:
@@ -1391,34 +1151,28 @@
     if label_mapping is None:
         assert path is not None, "A path is required if no label mapping is given"
         label_mapping = LabelMapping.from_path(path)
 
     # Construct segmentation layers
     layer1 = {}
     layer2 = {}
-    for name, seg in segmentation.items():
-        assert np.issubdtype(seg.dtype, np.integer), f"Segmentation must be integer type ({seg.dtype = }, {path = })"
+    for name, s in segmentation.items():
+        assert np.issubdtype(s.dtype, np.integer), f"Segmentation must be integer type ({s.dtype = }, {path = })"
 
-        if not label_mapping.is_index_valid(seg).any():
-            # If there are no valid labels in the segmentation mask, we cannot show it
-            continue
-
-        if seg.ndim == 3:
-            assert seg.shape[0] == 2, f"Can only handle two-layer segmentations (not {seg.shape[0]})"
-            layer1[name] = seg[0]
-            layer2[name] = seg[1]
-            spatial_shape = seg.shape[1:]
+        if s.ndim == 3:
+            assert s.shape[0] == 2, f"Can only handle two-layer segmentations (not {s.shape[0]})"
+            layer1[name] = s[0]
+            layer2[name] = s[1]
+            spatial_shape = s.shape[1:]
         else:
-            layer1[name] = seg
-            spatial_shape = seg.shape
+            layer1[name] = s
+            spatial_shape = s.shape
 
         assert spatial_shape == rgb_image.shape[:2], "The segmentation image must have the same shape as the RGB image"
 
-    assert len(layer1) > 0, f"No valid segmentation found for the image {path}"
-
     img_height, img_width = rgb_image.shape[:2]
     opacity = 0.5
     fig = go.Figure()
 
     def label_remapping(seg: np.ndarray) -> tuple:
         # Color mapping based on the valid labels which occur in the image
         available_label_indices = np.unique(seg)
@@ -1434,18 +1188,14 @@
 
         return label_mapping_valid, label_colors, remapping_minimal
 
     def get_colorbar(label_mapping_valid: dict[int, str]) -> dict:
         if len(label_mapping_valid) == 1:
             tickvals = [0, 1]
         else:
-            assert (
-                len(label_mapping_valid) > 0
-            ), f"No valid labels found for the image:\n{path = }\n{path.annotated_labels(annotation_name='all') = }"
-
             # For example, with three colors, Plotly uses 0, 1, 2 and the colors change at 2/3, 4/3
             # We want the ticks to be placed in the middle of the color rectangles 2/6=2/3*0.5, 6/6=2/6+2/3
             tickstep = (len(label_mapping_valid) - 1) / len(label_mapping_valid)
             tickvals = np.arange(tickstep / 2, len(label_mapping_valid), tickstep)
 
         return {
             "title": "class",
@@ -1456,30 +1206,25 @@
             "y": 1.01,
         }
 
     default_annotation_name = path.dataset_settings.get("annotation_name_default", "default")
     default_index = (
         0 if default_annotation_name not in layer1.keys() else list(layer1.keys()).index(default_annotation_name)
     )
-
-    # The buttons hide or unhide individual plots but the number of plots per button state (=annotation_name) could be different due to multi-layer segmentations
-    # Plotly only sees a global list of all plots (heatmaps in this case) and we need to tell Plotly which plots should be hidden or unhidden for each button state
-    plot_index = 0  # Global plot index
-    name_plot_mapping = {name: [] for name in layer1.keys()}  # List of global plot indices for each annotation name
+    button_states = [] if len(layer1) > 1 else None
     for i, name in enumerate(layer1.keys()):
         visible = i == default_index
 
         # We need a colorbar per annotation since there is no gurantee that all annotators used the same labels (and having one colorbar with all labels is not possible in Plotly)
         label_mapping_valid, label_colors, remapping_minimal = label_remapping(layer1[name])
         colorbar = get_colorbar(label_mapping_valid)
 
         if name in layer2:
             label_mapping_valid2, label_colors2, remapping_minimal2 = label_remapping(layer2[name])
             colorbar2 = get_colorbar(label_mapping_valid2)
-            colorbar2["x"] = 1.25
 
             def create_text(label: int, label2: int) -> str:
                 text = f"layer1: {label_mapping.index_to_name(label)}<br>"
                 text += f"layer2: {label_mapping.index_to_name(label2)}"
                 return text
 
             hover_text_ref = np.vectorize(create_text)(layer1[name], layer2[name])
@@ -1491,55 +1236,47 @@
                     opacity=opacity,
                     colorscale=discrete_colorbar(label_colors2),
                     colorbar=colorbar2,
                     name="Segmentation",
                     visible=visible,
                 )
             )
-            name_plot_mapping[name].append(plot_index)
-            plot_index += 1
         else:
             hover_text_ref = np.vectorize(lambda label: label_mapping.index_to_name(label))(layer1[name])
 
         fig.add_trace(
             go.Heatmap(
                 z=np.flipud(consecutive_segmentation(layer1[name], remapping_minimal)),
                 opacity=opacity,
                 colorscale=discrete_colorbar(label_colors),
                 colorbar=colorbar,
                 text=np.flipud(hover_text_ref),
                 name="Segmentation",
                 visible=visible,
             )
         )
-        name_plot_mapping[name].append(plot_index)
-        plot_index += 1
 
-    # We only need buttons if there are multiple annotation names
-    button_states = [] if len(name_plot_mapping) > 1 else None
-    if button_states is not None:
-        for name, indices in name_plot_mapping.items():
-            visible_state = [False] * plot_index  # The global visible state for this annotation name
-            for i in indices:
-                visible_state[i] = True
+        if button_states is not None:
+            visible_state = [False] * len(layer1)
+            visible_state[i] = True
 
             button_states.append(
                 {
                     "label": name,
                     "method": "update",
                     "args": [{"visible": visible_state}],
                 }
             )
 
     fig.add_layout_image(
         source=Image.fromarray(rgb_image),
         xref="x",
         yref="y",
-        x=-0.5,
-        y=img_height - 0.5,
+        x=0,
+        y=img_height,
         sizex=img_width,
         sizey=img_height,
         sizing="stretch",
         layer="below",
     )
 
     # Create and add slider
@@ -1571,58 +1308,41 @@
     fig.update_xaxes(showgrid=False, zeroline=False)
     fig.update_yaxes(showgrid=False, zeroline=False)
 
     return fig
 
 
 def create_overlay(overlay: np.ndarray, path: DataPath) -> go.Figure:
-    """
-    General function to overlay an image with a heatmap.
-
-    Args:
-        overlay: Array with the same shape as the image. The values are used to color the image.
-        path: Path to the image.
-
-    Returns: Plotly figure showing the overlay.
-    """
     # Load original image
     rgb_image = path.read_rgb_reconstructed()
     assert overlay.shape == rgb_image.shape[:2], "The overlay image must have the same shape as the RGB image"
     rgb_image = Image.fromarray(rgb_image)
 
     img_height, img_width = path.dataset_settings["shape"][:2]
     opacity = 0.5
     fig = go.Figure()
 
-    valid_values = np.unique(overlay[~np.isnan(overlay)])
-    colorbar = {
-        "title": "value",
-        "ticks": "outside",
-        "yanchor": "top",
-        "y": 1.01,
-        "tickvals": valid_values.tolist(),
-        "ticktext": valid_values.tolist(),
-    }
+    colorbar = {"title": "value", "ticks": "outside", "yanchor": "top", "y": 1.01}
 
-    colors = generate_distinct_colors(len(valid_values))
+    colors = generate_distinct_colors(np.max(overlay) + 1)
     fig.add_trace(
         go.Heatmap(
             z=np.flipud(overlay),
             opacity=opacity,
             colorscale=discrete_colorbar(colors),
             colorbar=colorbar,
             name="overlay",
         )
     )
     fig.add_layout_image(
         source=rgb_image,
         xref="x",
         yref="y",
-        x=0.5,
-        y=img_height - 0.5,
+        x=0,
+        y=img_height,
         sizex=img_width,
         sizey=img_height,
         sizing="stretch",
         layer="below",
     )
 
     scale = 1.5
@@ -1692,16 +1412,16 @@
     )
 
     def add_background_image(fig: go.Figure, row: int, col: int) -> None:
         fig.add_layout_image(
             source=rgb_image,
             xref="x",
             yref="y",
-            x=0.5,
-            y=img_height - 0.5,
+            x=0,
+            y=img_height,
             sizex=img_width,
             sizey=img_height,
             sizing="stretch",
             layer="below",
             row=row,
             col=col,
         )
@@ -1847,15 +1567,15 @@
         df_fold = df_fold.query(f'epoch_index == {df_fold["best_epoch_index"].unique().item()}')
 
         # Aggregate information from all images
         acc_mat = np.stack([v["accuracies"] for v in df_fold["ece"]])
         conf_mat = np.stack([v["confidences"] for v in df_fold["ece"]])
         prob_mat = np.stack([v["probabilities"] for v in df_fold["ece"]])
 
-        ece = ECE.aggregate_vectors(acc_mat, conf_mat, prob_mat)
+        ece = ECELoss.aggregate_vectors(acc_mat, conf_mat, prob_mat)
 
         x = np.linspace(0, 1, len(ece["accuracies"]) + 1)
 
         hover_text = []
         for prob, conf in zip(ece["probabilities"], ece["confidences"]):
             hover_text.append(f"samples={prob:0.3f}, confidence={conf:0.3f}")
 
@@ -1942,30 +1662,30 @@
     stats = []
     titles = []
     fold_dirs = sorted(run_dir.glob("fold*"))
 
     datasets = [{} for i in range(len(fold_dirs))]
     for i, fold_dir in enumerate(fold_dirs):
         train_stats = np.load(fold_dir / "trainings_stats.npz", allow_pickle=True)["data"]
-
-        # Only used to get the number of training images for this fold, we don't need a mapping from image_index to image_name for this figure
-        image_names = set()
+        image_names = (
+            set()
+        )  # Only used to get the number of training images for this fold, we don't need a mapping from image_index to image_name for this figure
         for name, paths in data_specs.folds[fold_dir.name].items():
             if name.startswith("train"):
                 image_names.update([p.image_name() for p in paths])
                 datasets[i][paths[0].dataset_settings["dataset_name"]] = len(
                     paths
                 )  # as each dataset within the folds has the same
 
         assert len(image_names) > 0
         fold_stats = np.zeros((len(image_names), len(train_stats)), dtype=np.int64)
 
-        for epoch_index, stats_epoch in enumerate(train_stats):
+        for epoch_indexx, stats_epoch in enumerate(train_stats):
             for image_index in stats_epoch["img_indices"]:
-                fold_stats[image_index, epoch_index] += 1
+                fold_stats[image_index, epoch_indexx] += 1
 
         stats.append(fold_stats)
         n_used_images = np.count_nonzero(np.sum(fold_stats, axis=1))
         titles.append(f"{fold_dir.name} ({n_used_images} of {len(image_names)} images used in total)")
 
     # cumsum of dataset counts for line drawing
     for i in range(len(datasets)):
@@ -2111,58 +1831,57 @@
         channels = list(x)
 
     upper_border = list(mid_line + std_range)
     lower_border = list(mid_line - std_range)
     lower_border = lower_border[::-1]
 
     if "hovertemplate" not in scatter_kwargs:
-        scatter_kwargs["hovertemplate"] = (
-            "wavelength: %{x:.1f}<br>normalized reflectance: %{y:.5f}<br>standard deviation: %{text:.5f}"
-        )
-
-    legendgroup = scatter_kwargs.pop("legendgroup", label)
+        scatter_kwargs[
+            "hovertemplate"
+        ] = "wavelength: %{x:.1f}<br>normalized reflectance: %{y:.5f}<br>standard deviation: %{text:.5f}"
 
     fig.add_trace(
         go.Scatter(
             x=channels,
             y=mid_line,
             text=std_range,
             name=label,
             line_color=linecolor,
-            legendgroup=legendgroup,
+            legendgroup=label,
             **scatter_kwargs,
         ),
         row=row,
         col=col,
     )
     fig.add_trace(
         go.Scatter(
             x=channels + channels[::-1],
             y=upper_border + lower_border,
             fill="toself",
             fillcolor=linecolor,
             line_color=linecolor,
             opacity=0.15,
             name=label,
-            legendgroup=legendgroup,
+            legendgroup=label,
             showlegend=False,
             hoverinfo="skip",
         ),
         row=row,
         col=col,
     )
 
     fig.update_layout(hovermode="x")
     fig.update_xaxes(hoverformat=".1f")
 
     return fig
 
 
-def colorchecker_fig_styling(fig: go.Figure, yaxis_title: str = "L1-normalized<br>reflectance [a.u.]") -> go.Figure:
-    """This function takes a figure displaying colorchecker spectra in a grid of 6 rows and 4 columns and adds to each subplot a bar in the color of the corresponding colorchecker chip."""
+def colorchecker_fig_styling(fig: go.Figure) -> go.Figure:
+    """This function takes a figure displaying colorchecker spectra in a grid of 6 rows and 4 columns and adds to each subplot a bar in the color of the corresponding colorchecker chip.
+    """
     n_subplots = int(fig.data[-1]["xaxis"][1:])
     if n_subplots == 24:
         label_color_map = ColorcheckerReader.label_colors_classic
         width = 176
         y_offset = 0.006
         y_spacing = 0.086
         x_spacing = 0.255
@@ -2198,16 +1917,16 @@
     fig.update_xaxes(tickvals=[600, 700, 800, 900], ticktext=["600", "700", "800", "900"], tickmode="array")
     fig.update_yaxes(dtick=0.005)
 
     layout = {}
     for i in np.arange(n_subplots - 3, n_subplots + 1):
         layout[f"xaxis{i}"] = dict(title="wavelength [nm]")
     for i in np.arange(5, n_subplots, 4):
-        layout[f"yaxis{i}"] = dict(title=yaxis_title)
-    layout["yaxis"] = dict(title=yaxis_title)
+        layout[f"yaxis{i}"] = dict(title="L1-normalized<br>reflectance [a.u.]")
+    layout["yaxis"] = dict(title="L1-normalized<br>reflectance [a.u.]")
     fig.update_layout(**layout)
 
     fig.update_layout(
         height=820 * n_subplots // 24,
         width=850,
         template="plotly_white",
         font_family="Arial",
```

## Comparing `htc/evaluation/metrics/ECE.py` & `htc/evaluation/metrics/ECELoss.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,156 +4,137 @@
 from typing import Union
 
 import numpy as np
 import torch
 import torch.nn as nn
 
 
-class CalibrationLoss(nn.Module):
-    def _parse_input(
-        self, predictions: torch.Tensor, labels: torch.Tensor, confidences: torch.Tensor = None
-    ) -> torch.Tensor:
-        if predictions.is_floating_point():
-            # Softmax input
-            assert predictions.ndim >= 2, "Softmax values should be at least 2-dimensional"
-            assert (
-                predictions.shape[1:] == labels.shape[0:]
-            ), "The sample dimension does not match between softmaxes and labels"
-
-            softmaxes_sum = torch.sum(predictions, dim=0)
-            assert torch.allclose(softmaxes_sum, torch.ones_like(softmaxes_sum)), (
-                "All of the softmax should sum upto approx. 1 in the class dimension. Are you sure that you are not"
-                f" sending logits rather than softmaxes? {softmaxes_sum.unique(return_counts=True) = }"
-            )
-
-            confidences, predictions = torch.max(predictions, dim=0)
-            confidences = confidences.flatten()
-            predictions = predictions.flatten()
-            labels = labels.flatten()
-
-            assert predictions.shape == labels.shape, "The sample dimension does not match between softmaxes and labels"
-            accuracies = predictions.eq(labels).to(dtype=confidences.dtype)
-            assert len(confidences.shape) == 1, "Confidences should be a vector"
-        else:
-            # Predicted labels input
-            assert confidences is not None, "Confidences must be provided if predicted labels are passed"
-            assert predictions.shape == labels.shape, "The sample dimension does not match between softmaxes and labels"
-            accuracies = predictions.eq(labels).to(dtype=confidences.dtype)
-
-            confidences = confidences.flatten()
-            accuracies = accuracies.flatten()
-
-        return confidences, accuracies
-
-
-class ECE(CalibrationLoss):
+class ECELoss(nn.Module):
     def __init__(self, n_bins: int = 10):
         """
         Calculates the Expected Calibration Error (ECE) of a model.
 
-        This implementation is in line with the [multiclass_calibration_error()](https://torchmetrics.readthedocs.io/en/stable/classification/calibration_error.html#multiclass-calibration-error) function from torchmetrics but returns the raw data (e.g. accuracy counts) in addition to the ECE error. This allows for aggregation across multiple samples (e.g. for a whole dataset) using the `aggregate_vectors()` method.
-
         This metric can be used to estimate how well a model is calibrated with lower values indicating a better calibrated model (for more information see https://arxiv.org/abs/1706.04599). For this, it takes the predictions of the network (the softmax values) where the highest value in the softmax vector corresponds to the confidence for that prediction. Then, the predictions are binned into confidence intervals and for each bin the difference between the average confidence and accuracy is calculated. A model is calibrated when this difference is small. The intuition is that if there are, for example, 100 samples in the bin between 0.7 and 0.8 with an average confidence of 0.77, then around 77 % of those samples should be correctly classified assuming a classifier which knows about its unsureness.
 
         Note: The model's calibration can be improved by using the temperature scaling method (e.g. https://github.com/gpleiss/temperature_scaling).
 
         Args:
             n_bins: Number of confidence interval bins.
         """
         super().__init__()
         self.n_bins = n_bins
 
-    def forward(
-        self,
-        predictions: torch.Tensor,
-        labels: torch.Tensor,
-        confidences: torch.Tensor = None,
-    ) -> dict[str, Union[float, list[int], list[float], list[int]]]:
+    def forward(self, softmaxes: torch.Tensor, labels: torch.Tensor) -> dict[str, Union[float, list, list, list]]:
         """
         Calculates the ECE values for a set of samples.
 
         Args:
-            predictions: Softmax predictions of the network (class, *) or predicted labels of the network (*).
-            labels: Reference labels (*).
-            confidences: Confidence values of the predictions (*). Must not be None if predicted labels are passed.
+            softmaxes: The softmax output of your network (class, batch).
+            labels: Reference labels (batch).
 
         Returns:
             dict: Dictionary with the ECE "error" plus the unnormalized vectors ("accuracies", "confidences", "probabilities"). Please refer to the `aggregate_vectors()` method if you need to aggregate those values across multiple samples.
         """
-        confidences, accuracies = self._parse_input(predictions, labels, confidences)
-        accuracies = accuracies.to(torch.int32)
-        bin_boundaries = torch.linspace(0, 1, self.n_bins + 1, device=predictions.device)
-
-        accuracies_bins = torch.zeros(len(bin_boundaries) - 1, device=confidences.device, dtype=torch.int32)
-        confidences_bins = torch.zeros(len(bin_boundaries) - 1, device=confidences.device, dtype=confidences.dtype)
-        counts_bins = torch.zeros(len(bin_boundaries) - 1, device=confidences.device, dtype=torch.int32)
-
-        # For each value, the index of the corresponding bucket
-        indices = torch.bucketize(confidences, bin_boundaries) - 1
-
-        # Add every value to the corresponding bucket
-        counts_bins.scatter_add_(dim=0, index=indices, src=torch.ones_like(accuracies))
-        accuracies_bins.scatter_add_(dim=0, index=indices, src=accuracies)
-        confidences_bins.scatter_add_(dim=0, index=indices, src=confidences)
-
-        # Normalize via number of samples per bin
-        # Setting nan values to zero basically ignores those bins in the ECE calculation
-        prob_normalized = counts_bins / counts_bins.sum()
-        acc_normalized = torch.nan_to_num_(accuracies_bins / counts_bins)
-        conf_normalized = torch.nan_to_num_(confidences_bins / counts_bins)
+        assert len(softmaxes.shape) == 2 and len(labels.shape) == 1, "Invalid shape"
+        assert softmaxes.shape[1] == labels.shape[0], "The sample dimension does not match between softmaxes and labels"
+
+        bin_boundaries = torch.linspace(0, 1, self.n_bins + 1, device=softmaxes.device)
+        bin_lowers = bin_boundaries[:-1]
+        bin_uppers = bin_boundaries[1:]
+
+        softmaxes_sum = torch.sum(softmaxes, dim=0)
+        assert torch.allclose(softmaxes_sum, torch.ones_like(softmaxes_sum)), (
+            "All of the softmax should sum upto approx. 1 in the class dimension. Are you sure that you are not sending"
+            " logits rather than softmaxes?"
+        )
+
+        confidences, predictions = torch.max(softmaxes, dim=0)
+        accuracies = predictions.eq(labels)
+
+        accuracies_bins = []
+        confidences_bins = []
+        probabilities_bins = []
+        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
+            # Calculated |confidence - accuracy| in each bin
+
+            # Group values based on confidence
+            in_bin = confidences.gt(bin_lower) * confidences.le(bin_upper)
+
+            # Proportion of confidence values which ended up in this bin
+            prop_in_bin = in_bin.sum().item()
+            probabilities_bins.append(prop_in_bin)
+
+            if prop_in_bin > 0:
+                accuracy_in_bin = accuracies[in_bin].sum().item()
+                confidence_in_bin = confidences[in_bin].sum().item()
+
+                accuracies_bins.append(accuracy_in_bin)
+                confidences_bins.append(confidence_in_bin)
+            else:
+                # No values in the bin --> add zeros
+                accuracies_bins.append(0)
+                confidences_bins.append(0)
+
+        # Calculate the ece error
+        prob = torch.Tensor(probabilities_bins)
+        acc_normalized = torch.Tensor(accuracies_bins) / prob
+        conf_normalized = torch.Tensor(confidences_bins) / prob
+        prob_normalized = prob / prob.sum()
+        valid_bins = prob_normalized > 0
 
         # ECE is the absolute difference between the confidences and accuracies in the bin weighted by the number of samples which are in this bin
-        ece = torch.sum(torch.abs(conf_normalized - acc_normalized) * prob_normalized)
+        ece = torch.sum(
+            torch.abs(conf_normalized[valid_bins] - acc_normalized[valid_bins]) * prob_normalized[valid_bins]
+        )
 
         return {
             "error": ece.item(),
-            "accuracies": accuracies_bins.tolist(),
-            "confidences": confidences_bins.tolist(),
-            "probabilities": counts_bins.tolist(),
+            "accuracies": accuracies_bins,
+            "confidences": confidences_bins,
+            "probabilities": probabilities_bins,
         }
 
     @staticmethod
-    def aggregate_vectors(
-        acc_mat: np.ndarray, conf_mat: np.ndarray, prob_mat: np.ndarray
-    ) -> dict[str, Union[float, list[int], list[float], list[int]]]:
+    def aggregate_vectors(acc_mat: np.ndarray, conf_mat: np.ndarray, prob_mat: np.ndarray):
         """
         This function aggregates the ece vectors from multiple images. This is useful when the ece cannot be calculated for all samples at once. All matrices must have the shape (n_batches, n_bins). Note that only when the raw counts are passed to this function the real ece can be calculated. If normalized vectors are passed, then the assumption is made that the original number of samples before the normalization was the same for all images (which is an approximation).
 
         Args:
-            acc_mat: Matrix with either the counts representing the number of correctly classified samples per bin or the accuracy per bin (n_batches, n_bins).
-            conf_mat: Matrix with either the sum of the confidence values or the normalized confidence per bin (n_batches, n_bins).
-            prob_mat: Matrix with either the total counts or the ratio of samples per bin (n_batches, n_bins).
+            acc_mat: Matrix with either the counts representing the number of correctly classified samples per bin or the accuracy per bin.
+            conf_mat: Matrix with either the sum of the confidence values or the normalized confidence per bin.
+            prob_mat: Matrix with either the total counts or the ratio of samples per bin.
 
         Returns: A dictionary with "accuracies", "confidences" and "probabilities" vectors (all normalized) as well as the ece "error".
         """
         assert (
             acc_mat.shape == conf_mat.shape and acc_mat.shape == prob_mat.shape
         ), "All matrices must have the same shape"
         assert np.all(acc_mat >= 0) and np.all(conf_mat >= 0) and np.all(prob_mat >= 0), "All matrices must be positive"
 
-        with np.errstate(invalid="ignore"):
-            if acc_mat.dtype == np.int64:
-                # Raw counts
-                probabilities = prob_mat.sum(axis=0).astype(np.float64)
-
-                accuracies = acc_mat.sum(axis=0).astype(np.float64)
-                accuracies = np.nan_to_num(accuracies / probabilities, copy=False)
-                confidences = conf_mat.sum(axis=0)
-                confidences = np.nan_to_num(confidences / probabilities, copy=False)
-                probabilities = np.nan_to_num(probabilities / np.sum(probabilities), copy=False)
-            else:
-                # Normalized vectors --> approximate solution
-                prob_mat = prob_mat + 1e-10  # Avoid division by zero
+        if acc_mat.dtype == np.int64:
+            # Raw counts
+            probabilities = prob_mat.sum(axis=0).astype(np.float64)
+            valid = probabilities > 0  # Avoid division by zero
+
+            accuracies = acc_mat.sum(axis=0).astype(np.float64)
+            accuracies[valid] = accuracies[valid] / probabilities[valid]
+            confidences = conf_mat.sum(axis=0)
+            confidences[valid] = confidences[valid] / probabilities[valid]
+            probabilities[valid] = probabilities[valid] / np.sum(probabilities)
+        else:
+            # Normalized vectors --> approximate solution
+            prob_mat = prob_mat + 1e-10  # Avoid division by zero
 
-                accuracies = np.average(acc_mat, weights=prob_mat, axis=0)
-                confidences = np.average(conf_mat, weights=prob_mat, axis=0)
-                probabilities = np.sum(prob_mat, axis=0)
-                probabilities = np.nan_to_num(probabilities / np.sum(probabilities), copy=False)
+            accuracies = np.average(acc_mat, weights=prob_mat, axis=0)
+            confidences = np.average(conf_mat, weights=prob_mat, axis=0)
+            probabilities = np.sum(prob_mat, axis=0)
+            valid = probabilities > 0
+            probabilities[valid] = probabilities[valid] / np.sum(probabilities)
 
-        ece_error = np.sum(np.abs(confidences - accuracies) * probabilities).item()
+        ece_error = np.sum(np.abs(confidences[valid] - accuracies[valid]) * probabilities[valid]).item()
 
         return {
             "error": ece_error,
             "accuracies": accuracies,
             "confidences": confidences,
             "probabilities": probabilities,
         }
```

## Comparing `htc/utils/Datasets.py` & `htc/utils/DatasetDir.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,45 +1,44 @@
 # SPDX-FileCopyrightText: 2022 Division of Intelligent Medical Systems, DKFZ
 # SPDX-License-Identifier: MIT
 
-import copy
 import os
 import re
 from collections.abc import Iterator
 from pathlib import Path
 from typing import Union
 
 from htc.utils.unify_path import unify_path
 
 
-class Datasets:
+class DatasetDir:
     def __init__(
         self,
         network_dir: Path = None,
         network_project_folder: str = "",
         network_data_folder: str = "",
     ):
         """
         This class is a helper to access datasets which are either stored locally or on a common network drive. If a dataset is not available locally, then it is automatically accessed on the network drive (which may be slow).
 
         >>> from htc.settings import settings
         >>> str(settings.data_dirs.semantic)  # doctest: +ELLIPSIS
         '...2021_02_05_Tivita_multiorgan_semantic/data'
 
         There are also special variables to access common directories on the network drive:
-        >>> str(settings.datasets.network)  # doctest: +ELLIPSIS
+        >>> str(settings.data_dirs.network)  # doctest: +ELLIPSIS
         '.../E130-Projekte'
-        >>> str(settings.datasets.network_project)  # doctest: +ELLIPSIS
+        >>> str(settings.data_dirs.network_project)  # doctest: +ELLIPSIS
         '.../E130-Projekte/Biophotonics/Projects/2021_02_05_hyperspectral_tissue_classification'
-        >>> str(settings.datasets.network_data)  # doctest: +ELLIPSIS
+        >>> str(settings.data_dirs.network_data)  # doctest: +ELLIPSIS
         '.../E130-Projekte/Biophotonics/Data'
 
         Please do not access the parent of a data directory directly since data may be a symbolic link (e.g. due to filesystem size constraints). For example, if 2021_02_05_Tivita_multiorgan_semantic/data is a symbolic link pointing to /my_folder/data, then .parent.name is my_folder and not 2021_02_05_Tivita_multiorgan_semantic.
 
-        >>> entry = settings.datasets.semantic
+        >>> entry = settings.data_dirs.get("semantic", return_entry=True)
         >>> list(entry.keys())
         ['path_dataset', 'path_data', 'path_intermediates', 'location', 'has_unified_paths', 'shortcut', 'env_name']
         >>> str(entry["path_dataset"])  # doctest: +ELLIPSIS
         '...2021_02_05_Tivita_multiorgan_semantic'
         >>> str(entry["path_intermediates"])  # doctest: +ELLIPSIS
         '...2021_02_05_Tivita_multiorgan_semantic/intermediates'
 
@@ -62,29 +61,31 @@
 
     def __repr__(self) -> str:
         dirs = []
         for name, entry in self:
             shortcut = (
                 f"settings.data_dirs.{entry['shortcut']}" if entry["shortcut"] is not None else "(no shortcut set)"
             )
-            dirs.append(f"""- {shortcut}
+            dirs.append(
+                f"""- {shortcut}
     * full name: {name}
     * environment name: {self.path_to_env(entry['path_data'])}
-    * location: {entry['location']}""")
+    * location: {entry['location']}"""
+            )
 
         msg = f"Network directory: {self.network if self.network is not None else '(not set)'}\n"
         msg += "Registered data directories:\n" + "\n".join(dirs)
         return msg
 
     def __iter__(self) -> Iterator[tuple[str, dict]]:
         """
         Iterate over all registered data directories. Only the dataset names as they are used on the network drive are returned. If you need the environment variable name, please use path_to_env() or entry["env_name"].
 
         >>> from htc.settings import settings
-        >>> [(name, entry["env_name"]) for name, entry in settings.datasets]  # doctest: +ELLIPSIS
+        >>> [(name, entry["env_name"]) for name, entry in settings.data_dirs]  # doctest: +ELLIPSIS
         [('2021_07_26_Tivita_multiorgan_human', 'PATH_Tivita_multiorgan_human'), ('2021_03_30_Tivita_studies', 'PATH_Tivita_studies'), ...]
 
         Yields: Tuple with name of the dataset folder and entry object.
         """
         for name in self.dataset_names:
             yield name, self._dirs[name]
 
@@ -113,15 +114,15 @@
                 f"The environment variable {env_name} is already registered. Did you call this function twice? The"
                 " dataset is still added (again)"
             )
 
         # User may disable an env variable via PATH_Tivita_multiorgan_semantic=''
         if path_env := os.getenv(env_name, False):
             # Here, we are only interested in the path, not the options
-            path_env, _ = Datasets.parse_path_options(path_env)
+            path_env, _ = DatasetDir.parse_path_options(path_env)
             path_env = unify_path(path_env)
             path_env_data = path_env / data_folder
 
             if not path_env.exists():
                 self.log.warning(
                     f"The environment variable {env_name} was set to {path_env} but the path does not exist"
                 )
@@ -175,40 +176,38 @@
                 self._dirs[name] = path_entry
 
                 # Every uppercase version of a name should also match (--> Windows)
                 name_upper = name.upper()
                 if name != name_upper and name_upper not in self._dirs:
                     self._dirs[name_upper] = path_entry
 
-    def get(self, item: str, local_only: bool = False) -> Union[dict, None]:
+    def get(self, item: str, local_only: bool = False, return_entry: bool = False) -> Union[Path, dict, None]:
         """
         Access a data directory from this class.
 
         >>> from htc.settings import settings
-        >>> str(settings.datasets.masks["path_dataset"])  # doctest: +ELLIPSIS
-        '...2021_02_05_Tivita_multiorgan_masks'
+        >>> str(settings.data_dirs['2021_02_05_Tivita_multiorgan_masks'])  # doctest: +ELLIPSIS
+        '...2021_02_05_Tivita_multiorgan_masks/data'
+        >>> str(settings.data_dirs.masks)  # doctest: +ELLIPSIS
+        '...2021_02_05_Tivita_multiorgan_masks/data'
 
         Args:
             item: Name of the environment variable, network folder name or shortcut name.
             local_only: If True, network locations will not be considered.
-
-        Returns: Dictionary with information about the dataset (or None if the dataset could not be found):
+            return_entry: If True, the complete path entry dictionary will be returned. If False, it will be the path to the data folder (which contains the dataset_settings.json file). The entry dictionary has the following relevant values:
             - path_dataset: Path to the dataset root folder, e.g. my/path/2021_02_05_Tivita_multiorgan_masks.
             - path_data: Path to the data folder, e.g. my/path/2021_02_05_Tivita_multiorgan_masks/data.
             - path_intermediates: Path to the intermediates folder, e.g. my/path/2021_02_05_Tivita_multiorgan_masks/intermediates.
             - location: Whether this path is available locally or only on the network drive.
             - shortcut: Short name for the dataset.
             - env_name: Name of the environment variable which which was used to add this dataset.
-        """
-        if "#" in item:
-            # E.g. 2021_02_05_Tivita_multiorgan_semantic#context_experiments
-            item, subdata = item.split("#")
-        else:
-            subdata = None
 
+        Returns:
+            Path: The path to the data directory or None if it was not found.
+        """
         matched_location = self._find_match(item)
 
         if matched_location is not None:
             if matched_location["location"] == "network":
                 if local_only:
                     # In case we are not interested in network locations
                     return None
@@ -220,27 +219,22 @@
 
             if not matched_location["has_unified_paths"]:
                 # We only want to unify paths once as this can be a costly operation
                 matched_location["path_data"] = unify_path(matched_location["path_data"])
                 matched_location["path_dataset"] = unify_path(matched_location["path_dataset"])
                 matched_location["has_unified_paths"] = True
 
-            if subdata is not None:
-                match = copy.deepcopy(matched_location)
-                match["path_data"] = match["path_data"] / subdata
-                return match
-            else:
-                return matched_location
+            return matched_location if return_entry else matched_location["path_data"]
         else:
             return None
 
-    def __getitem__(self, item: str) -> Union[dict, None]:
+    def __getitem__(self, item: str) -> Union[Path, None]:
         return self.get(item)
 
-    def __getattr__(self, item: str) -> Union[dict, Path, None]:
+    def __getattr__(self, item: str) -> Union[Path, None]:
         if item.startswith("_"):
             # __getattr__ may be called for built-ins, in which we are not interested
             return super().__getattr__(item)
         elif item == "network":
             return unify_path(self.network_dir) if self.network_dir is not None else None
         elif item == "network_project":
             return unify_path(self.network_dir / self.network_project_folder) if self.network_dir is not None else None
@@ -261,41 +255,41 @@
         return self._find_match(item) is not None
 
     def network_location(self, item: str, path_data: bool = True) -> Path:
         """
         Similar to get() but always returns the path on the network drive.
 
         >>> from htc.settings import settings
-        >>> str(settings.datasets.network_location("semantic"))  # doctest: +ELLIPSIS
+        >>> str(settings.data_dirs.network_location("semantic"))  # doctest: +ELLIPSIS
         '.../E130-Projekte/Biophotonics/Data/2021_02_05_Tivita_multiorgan_semantic/data'
-        >>> str(settings.datasets.network_location("semantic", path_data=False))  # doctest: +ELLIPSIS
+        >>> str(settings.data_dirs.network_location("semantic", path_data=False))  # doctest: +ELLIPSIS
         '.../E130-Projekte/Biophotonics/Data/2021_02_05_Tivita_multiorgan_semantic'
 
         Args:
             item: Name to search for (similar to get()).
             path_data: If False, the path to the dataset is returned (i.e. the parent directory), else the path pointing to the data subfolder.
 
         Returns: Path on the network drive.
         """
         assert self.network_dir is not None, "No network directory set"
 
-        entry = self[item]
+        entry = self.get(item, return_entry=True)
         return (
             self.network_data / entry["path_dataset"].name / entry["path_data"].name
             if path_data
             else self.network_data / entry["path_dataset"].name
         )
 
     def path_to_env(self, path: Union[str, Path]) -> Union[str, None]:
         """
         Searches for the name of the environment variable which corresponds to the given path.
 
         >>> from htc.settings import settings
         >>> path = settings.data_dirs['2021_02_05_Tivita_multiorgan_masks']
-        >>> settings.datasets.path_to_env(path)
+        >>> settings.data_dirs.path_to_env(path)
         'PATH_Tivita_multiorgan_masks'
 
         Args:
             path: Path to one of the data directories.
 
         Returns: The name of the corresponding environment variable or None if there is no match for the path.
         """
@@ -305,27 +299,27 @@
 
         return None
 
     def env_keys(self) -> Iterator[str]:
         """
         Iterates over all data directory names. Only names starting with PATH will be returned. For example, only PATH_Tivita_multiorgan_semantic but not 2021_02_05_Tivita_multiorgan_semantic will be returned.
 
-        >>> data_dirs = Datasets()
+        >>> data_dirs = DatasetDir()
         >>> data_dirs.add_dir("PATH_Tivita_multiorgan_semantic", "2021_02_05_Tivita_multiorgan_semantic")
         >>> list(data_dirs.env_keys())
         ['PATH_Tivita_multiorgan_semantic']
         """
         for _, entry in self:
             yield entry["env_name"]
 
     def find_intermediates_dir(self, path: Union[str, Path], intermediates_folder: str = "intermediates") -> Path:
         """
         Searches for the intermediates directory given the path to a dataset or data folder by iterating over all known entry of this class.
 
-        Note: If you need the intermediates directory based on the name of the dataset, use `settings.intermediates_dirs.<dataset_name>`.
+        Note: If you need the intermediates directory based on the name of the dataset, use `get("dataset_name", return_entry=True)["path_intermediates"]`.
         Note: Similar to `find_entry()` but expects only paths to the data or intermediates directories and there is no guarantee that the found path exists.
 
         Args:
             path: Path to the dataset or the data folder.
             intermediates_folder: Name of the intermediates folder.
 
         Returns: Path (or string) to the intermediates directory. There is no guarantee that it exists.
@@ -380,15 +374,15 @@
         return matched_location
 
     @staticmethod
     def parse_path_options(path: str) -> tuple[Path, dict]:
         """
         Takes a filepath and searches for options in the form `/my/path:option=2;option2=value2`.
 
-        >>> path, options = Datasets.parse_path_options("/my/path:option1=2;option2=value2")
+        >>> path, options = DatasetDir.parse_path_options("/my/path:option1=2;option2=value2")
         >>> str(path)
         '/my/path'
         >>> options
         {'option1': '2', 'option2': 'value2'}
 
         Args:
             path: Filepath string usually coming from `os.environ`.
@@ -405,26 +399,7 @@
 
             for o in parts[1].split(";"):
                 match = re.search(r"(\w+)=(\w+)", o)
                 if match is not None:
                     options[match.group(1)] = match.group(2)
 
         return path, options
-
-
-class DatasetAccessor:
-    def __init__(self, datasets: Datasets, entry_field: str) -> None:
-        self.datasets = datasets
-        self.entry_field = entry_field
-
-    def __getitem__(self, item: str) -> Union[Path, None]:
-        entry = self.datasets.get(item)
-        if entry is None:
-            return None
-        else:
-            return entry[self.entry_field]
-
-    def __getattr__(self, item: str) -> Union[Path, None]:
-        return self[item]
-
-    def __contains__(self, item: str) -> bool:
-        return item in self.datasets
```

## Comparing `imsy_htc-0.0.11.dist-info/METADATA` & `imsy_htc-0.0.9.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: imsy-htc
-Version: 0.0.11
+Version: 0.0.9
 Summary: Framework for automatic classification and segmentation of hyperspectral images.
 Home-page: https://github.com/imsy-dkfz/htc
 Author: Division of Intelligent Medical Systems, DKFZ
 License: MIT
 Keywords: surgical data science,open surgery,hyperspectral imaging,organ segmentation,semantic scene segmentation,deep learning
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
@@ -24,29 +24,31 @@
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE.md
 Requires-Dist: numpy
 Requires-Dist: scipy
 Requires-Dist: scikit-learn
 Requires-Dist: Pillow
+Requires-Dist: scikit-image (>=0.19)
 Requires-Dist: pandas
 Requires-Dist: pyarrow
 Requires-Dist: matplotlib
 Requires-Dist: seaborn
 Requires-Dist: plotly
 Requires-Dist: kaleido
 Requires-Dist: jupyterlab
+Requires-Dist: papermill
 Requires-Dist: tensorboard
-Requires-Dist: lightning
+Requires-Dist: pytorch-lightning
 Requires-Dist: python-dotenv
 Requires-Dist: monai
 Requires-Dist: GPUtil
 Requires-Dist: psutil
 Requires-Dist: segmentation-models-pytorch
-Requires-Dist: kornia (>=0.6.12)
+Requires-Dist: kornia (>=0.6.5)
 Requires-Dist: rich
 Requires-Dist: threadpoolctl
 Requires-Dist: commentjson
 Requires-Dist: appdirs
 Requires-Dist: blosc
 Requires-Dist: torch
 Requires-Dist: lazy-imports
@@ -57,41 +59,40 @@
 Requires-Dist: pypdf ; extra == 'extra'
 Requires-Dist: xarray ; extra == 'extra'
 Requires-Dist: ipywidgets ; extra == 'extra'
 Requires-Dist: deskew ; extra == 'extra'
 Requires-Dist: paramiko ; extra == 'extra'
 Requires-Dist: opencv-python ; extra == 'extra'
 Requires-Dist: umap-learn ; extra == 'extra'
-Requires-Dist: xmltodict ; extra == 'extra'
-Requires-Dist: scikit-image ; extra == 'extra'
 Requires-Dist: challenger-pydocker ; extra == 'extra'
 
 <div align="center">
-<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/htc_logo.pdf"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/htc_logo.svg" alt="Logo" width="600" /></a>
+<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/htc_logo.svg"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/htc_logo.svg" alt="Logo" width="600" /></a>
 
 [![Python](https://img.shields.io/pypi/pyversions/imsy-htc.svg)](https://pypi.org/project/imsy-htc)
 [![PyPI version](https://badge.fury.io/py/imsy-htc.svg)](https://pypi.org/project/imsy-htc)
 [![Tests](https://github.com/IMSY-DKFZ/htc/actions/workflows/tests.yml/badge.svg)](https://github.com/IMSY-DKFZ/htc/actions/workflows/tests.yml)
 </div>
 
 # Hyperspectral Tissue Classification
 This package is a framework for automated tissue classification and segmentation on medical hyperspectral imaging (HSI) data. It contains:
 
 - The implementation of deep learning models to solve supervised classification and segmentation problems for a variety of different input spatial granularities (pixels, superpixels, patches and entire images, cf. figure below) and modalities (RGB data, raw and processed HSI data) from our paper [Robust deep learning-based semantic organ segmentation in hyperspectral images](https://doi.org/10.1016/j.media.2022.102488). It is based on [PyTorch](https://pytorch.org/) and [PyTorch Lightning](https://lightning.ai/).
 - Corresponding pretrained models.
 - A pipeline to efficiently load and process HSI data, to aggregate deep learning results and to validate and visualize findings.
 
 <div align="center">
-<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_model_overview.pdf"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_model_overview.svg" alt="Overview of deep learning models in the htc framework, here shown for HSI input." /></a>
+<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_model_overview.svg"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_model_overview.svg" alt="Overview of deep learning models in the htc framework, here shown for HSI input." /></a>
 </div>
 
 This framework is designed to work on HSI data from the [Tivita](https://diaspective-vision.com/en/) cameras but you can adapt it to different HSI datasets as well. Potential applications include:
 
+<!-- TODO: link to public dataset for all occurrences of HeiPorSPECTRAL -->
 - Use our data loading and processing pipeline to easily access image and meta data for any work utilizing Tivita datasets.
-- This repository is tightly coupled to work with the public [HeiPorSPECTRAL](https://heiporspectral.org/) dataset. If you already downloaded the data, you only need to perform the setup steps and then you can directly use the `htc` framework to work on the data (cf. [our tutorials](https://github.com/IMSY-DKFZ/htc/#tutorials)).
+- This repository is tightly coupled to work with the soon-to-be-public HeiPorSPECTRAL dataset. If you already downloaded the data, you only need to perform the setup steps and then you can directly use the `htc` framework to work on the data (cf. [our tutorials](https://github.com/IMSY-DKFZ/htc/#tutorials)).
 - Train your own networks and benefit from a pipeline offering e.g. efficient data loading, correct hierarchical aggregation of results and a set of helpful visualizations.
 - Apply deep learning models for different spatial granularities and modalities on your own semantically annotated dataset.
 - Use our pretrained models to initialize the weights for your own training.
 - Use our pretrained models to generate predictions for your own data.
 
 If you use the `htc` framework, please cite our paper [Robust deep learning-based semantic organ segmentation in hyperspectral images](https://doi.org/10.1016/j.media.2022.102488).
 
@@ -202,15 +203,15 @@
     ```
 - Recommended if you installed the package via pip: You can create user settings for this application. The location is OS-specific. For Linux the location might be at `~/.config/htc/variables.env`. Please run `htc info` upon package installation to retrieve the exact location on your system. The content of the file is of the same format as of the `.env` above.
 
 After setting your environment variables, it is recommended to run `htc info` to check that your variables are correctly registered in the framework.
 
 ## Tutorials
 A series of [tutorials](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials) can help you get started on the `htc` framework by guiding you through different usage scenarios.
->  The tutorials make use of our public HSI dataset [HeiPorSPECTRAL](https://heiporspectral.org/). If you want to directly run them, please download the dataset first and make it accessible via the environment variable `PATH_Tivita_HeiPorSPECTRAL` as described above.
+>  The tutorials make use of our soon-to-be-public HSI dataset HeiPorSPECTRAL. If you want to directly run them, please download the dataset first and make it accessible via the environment variable `PATH_Tivita_HeiPorSPECTRAL` as described above.
 
 - As a start, we recommend to take a look at this [general notebook](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/General.ipynb) which showcases the basic functionalities of the `htc` framework. Namely, it demonstrates the usage of the `DataPath` class which is the entry point to load and process HSI data. For example, you will learn how to read HSI cubes, segmentation masks and meta data. Among others, you can use this information to calculate the median spectrum of an organ.
 - If you want to use our framework with your own dataset, it might be necessary to write a custom `DataPath` class so that you can load and process your images and annotations. We [collected some tips](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/CustomDataPath.md) on how this can be achieved.
 - You have some HSI data at hand and want to use one of our pretrained models to generate predictions? Then our [prediction notebook](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/CreatingPredictions.ipynb) has got you covered.
 - You want to use our pretrained models to initialize the weights for your own training? See the section about [pretrained models](https://github.com/IMSY-DKFZ/htc/#pretrained-models) below for details.
 - You want to use our framework to train a network? The [network training notebook](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/network_training/NetworkTraining.ipynb) will show you how to achieve this on the example of a heart and lung segmentation network.
 - If you are interested in our technical validation (e.g. because you want to compare your colorchecker images with ours) and need to create a mask to detect the different colorchecker fields, you might find our automatic [colorchecker mask creation pipeline](https://github.com/IMSY-DKFZ/htc/tree/main/htc/utils/ColorcheckerMaskCreation.ipynb) useful.
@@ -243,20 +244,19 @@
 
 After successful installation of the `htc` package, you can use any of the pretrained models listed in the table. There are several ways to use them but the general principle is that models are always specified via their `model` and `run_folder`.
 
 ### Option 1: Use the models in your own training pipeline
 Every model class listed in the table has a static method [`pretrained_model()`](https://github.com/IMSY-DKFZ/htc/tree/main/htc/models/common/HTCModel.py) which you can use to create a model instance and initialize it with the pretrained weights. The model object will be an instance of `torch.nn.Module`. The function has examples for all the different model types but as a teaser consider the following example which loads the pretrained image HSI network:
 ```python
 import torch
-from htc import ModelImage, Normalization
+from htc import ModelImage
 
 run_folder = "2022-02-03_22-58-44_generated_default_model_comparison"  # HSI model
 model = ModelImage.pretrained_model(model="image", run_folder=run_folder, n_channels=100, n_classes=19)
 input_data = torch.randn(1, 100, 480, 640)  # NCHW
-input_data = Normalization(channel_dim=1)(input_data)  # Model expects L1 normalized input
 model(input_data).shape
 # torch.Size([1, 19, 480, 640])
 ```
 
 >  Please note that when initializing the weights as in this example, the segmentation head is initialized randomly. Meaningful predictions on your own data can thus not be expected out of the box, but you will have to train the model on your data first.
 
 ### Option 2: Use the models to create predictions for your data
@@ -282,15 +282,15 @@
 ## Papers
 This repository contains code to reproduce our publications listed below:
 
 ###  Robust deep learning-based semantic organ segmentation in hyperspectral images
 [https://doi.org/10.1016/j.media.2022.102488](https://doi.org/10.1016/j.media.2022.102488)
 
 <div align="center">
-<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_abstract.pdf"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_abstract.png" alt="Logo" width="800" /></a>
+<a href="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_abstract.svg"><img src="https://e130-hyperspectal-tissue-classification.s3.dkfz.de/figures/MIA_abstract.png" alt="Logo" width="800" /></a>
 </div>
 
 In this paper, we tackled fully automatic organ segmentation and compared deep learning models on different spatial granularities (e.g. patch vs. image) and modalities (e.g. HSI vs. RGB). Furthermore, we studied the required amount of training data and the generalization capabilities of our models across subjects. The pretrained networks are related to this paper. You can find the notebooks to generate the paper figures in [paper/MIA2021](https://github.com/IMSY-DKFZ/htc/tree/main/paper/MIA2021) (the folder also includes a [reproducibility document](https://github.com/IMSY-DKFZ/htc/tree/main/paper/MIA2021/reproducibility.md)) and the models in [htc/models](https://github.com/IMSY-DKFZ/htc/tree/main/htc/models). For each model, there are three configuration files, namely default, default_rgb and default_parameters, which correspond to the HSI, RGB and TPI modality, respectively.
 
 >  The dataset for this paper is not publicly available.
 
 <details closed>
@@ -310,17 +310,17 @@
 }
 ```
 </details>
 
 ###  Spectral organ fingerprints for machine learning-based intraoperative tissue classification with hyperspectral imaging in a porcine model
 [https://doi.org/10.1038/s41598-022-15040-w](https://doi.org/10.1038/s41598-022-15040-w)
 
-In this paper, we trained a classification model based on median spectra from HSI data. You can find the model code in [htc/tissue_atlas](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas) and the confusion matrix figure of the paper in [paper/NatureReports2021](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureReports2021) (including a reproducibility document).
+In this paper, we trained a classification model based on median spectra from HSI data. You can find the model code in [htc/tissue_atlas](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas) and the confusion matrix figure of the paper in [paper/NatureReports2021](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureReports2021).
 
->  The dataset for this paper is not fully publicly available, but a subset of the data is available through the public [HeiPorSPECTRAL](https://heiporspectral.org/) dataset.
+>  The dataset for this paper is not fully publicly available, but a subset of the data is available through the soon-to-be-public HeiPorSPECTRAL dataset.
 
 <details closed>
 <summary>Cite via BibTeX</summary>
 
 ```bibtex
 @article{Studier-Fischer2022,
     author = {Studier-Fischer, Alexander and Seidlitz, Silvia and Sellner, Jan and zdemir, Berkin and Wiesenfarth, Manuel and Ayala, Leonardo and Odenthal, Jan and Kndler, Samuel and Kowalewski, Karl Friedrich and Haney, Caelan Max and Camplisson, Isabella and Dietrich, Maximilian and Schmidt, Karsten and Salg, Gabriel Alexander and Kenngott, Hannes Gtz and Adler, Tim Julian and Schreck, Nicholas and Kopp-Schneider, Annette and Maier-Hein, Klaus and Maier-Hein, Lena and Mller-Stich, Beat Peter and Nickel, Felix},
@@ -335,43 +335,23 @@
     issn = {2045-2322},
     doi = {10.1038/s41598-022-15040-w},
     url = {https://doi.org/10.1038/s41598-022-15040-w}
 }
 ```
 </details>
 
-###  HeiPorSPECTRAL - the Heidelberg Porcine HyperSPECTRAL Imaging Dataset of 20 Physiological Organs
-[https://doi.org/10.1038/s41597-023-02315-8](https://doi.org/10.1038/s41597-023-02315-8)
+<!-- TODO: adjust title once set, change soon-to-be & soon -->
 
-This paper introduces the [HeiPorSPECTRAL](https://heiporspectral.org/) dataset containing 5756 hyperspectral images from 11 subjects. We are using these images in our tutorials. You can find the visualization notebook for the paper figures in [paper/NatureData2023](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023) (the folder also includes a [reproducibility document](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023/reproducibility.md)) and the remaining code in [htc/tissue_atlas_open](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas_open).
+###  HeiPorSPECTRAL - A dataset for hyperspectral imaging data of 20 physiological organs in a porcine model
 
-If you want to learn more about the [HeiPorSPECTRAL](https://heiporspectral.org/) dataset (e.g. the underlying data structure) or you stumbled upon a file and want to know how to read it, you might find this [notebook with low-level details](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas_open/FileReference.ipynb) helpful.
+This paper introduces the HeiPorSPECTRAL dataset containing 5756 hyperspectral images from 11 subjects. We are using these images in our tutorials. You can find the visualization notebook for the paper figures in [paper/NatureData2023](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023) (the folder also includes a [reproducibility document](https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023/reproducibility.md)) and the remaining code in [htc/tissue_atlas_open](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas_open).
 
->  The dataset for this paper is publicly available.
+If you want to learn more about the HeiPorSPECTRAL dataset (e.g. the underlying data structure) or you stumbled upon a file and want to know how to read it, you might find this [notebook with low-level details](https://github.com/IMSY-DKFZ/htc/tree/main/tissue_atlas_open/FileReference.ipynb) helpful.
 
-<details closed>
-<summary>Cite via BibTeX</summary>
-
-```bibtex
-@article{Studier-Fischer2023,
-  author={Studier-Fischer, Alexander and Seidlitz, Silvia and Sellner, Jan and Bressan, Marc and zdemir, Berkin and Ayala, Leonardo and Odenthal, Jan and Knoedler, Samuel and Kowalewski, Karl-Friedrich and Haney, Caelan Max and Salg, Gabriel and Dietrich, Maximilian and Kenngott, Hannes and Gockel, Ines and Hackert, Thilo and Mller-Stich, Beat Peter and Maier-Hein, Lena and Nickel, Felix},
-  title={HeiPorSPECTRAL - the Heidelberg Porcine HyperSPECTRAL Imaging Dataset of 20 Physiological Organs},
-  journal={Scientific Data},
-  year={2023},
-  month={Jun},
-  day={24},
-  volume={10},
-  number={1},
-  pages={414},
-  issn={2052-4463},
-  doi={10.1038/s41597-023-02315-8},
-  url={https://doi.org/10.1038/s41597-023-02315-8}
-}
-```
-</details>
+>  The dataset for this paper will soon be publicly available.
 
 ###  Knstliche Intelligenz und hyperspektrale Bildgebung zur bildgesttzten Assistenz in der minimal-invasiven Chirurgie
 [https://doi.org/10.1007/s00104-022-01677-w](https://doi.org/10.1007/s00104-022-01677-w)
 
 This paper presents several applications of intraoperative HSI, including our organ [segmentation](https://doi.org/10.1016/j.media.2022.102488) and [classification](https://doi.org/10.1038/s41598-022-15040-w) work. You can find the code generating our figure for this paper at [paper/Chirurg2022](https://github.com/IMSY-DKFZ/htc/tree/main/paper/Chirurg2022).
 
 >  The sample image used here is contained in the dataset from our paper [Robust deep learning-based semantic organ segmentation in hyperspectral images](https://doi.org/10.1016/j.media.2022.102488) and hence not publicly available.
@@ -392,11 +372,7 @@
     pages = {940-947},
     issn = {2731-698X},
     doi = {10.1007/s00104-022-01677-w},
     url = {https://doi.org/10.1007/s00104-022-01677-w}
 }
 ```
 </details>
-
-## Funding
-
-This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (NEURAL SPICING, grant agreement No. 101002198) and was supported by the German Cancer Research Center (DKFZ) and the Helmholtz Association under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for Health). It further received funding from the Surgical Oncology Program of the National Center for Tumor Diseases (NCT) Heidelberg.
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: imsy-htc Version: 0.0.11 Summary: Framework for
+Metadata-Version: 2.1 Name: imsy-htc Version: 0.0.9 Summary: Framework for
 automatic classification and segmentation of hyperspectral images. Home-page:
 https://github.com/imsy-dkfz/htc Author: Division of Intelligent Medical
 Systems, DKFZ License: MIT Keywords: surgical data science,open
 surgery,hyperspectral imaging,organ segmentation,semantic scene
 segmentation,deep learning Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research Classifier: Programming
 Language :: Python :: 3 Classifier: Programming Language :: Python :: 3.9
@@ -12,29 +12,29 @@
 System :: MacOS Classifier: Topic :: Scientific/Engineering Classifier: Topic
 :: Scientific/Engineering :: Artificial Intelligence Classifier: Topic ::
 Scientific/Engineering :: Image Processing Classifier: Topic :: Software
 Development Classifier: Topic :: Software Development :: Libraries Classifier:
 Topic :: Software Development :: Libraries :: Python Modules Requires-Python:
 >=3.9 Description-Content-Type: text/markdown License-File: LICENSE.md
 Requires-Dist: numpy Requires-Dist: scipy Requires-Dist: scikit-learn Requires-
-Dist: Pillow Requires-Dist: pandas Requires-Dist: pyarrow Requires-Dist:
-matplotlib Requires-Dist: seaborn Requires-Dist: plotly Requires-Dist: kaleido
-Requires-Dist: jupyterlab Requires-Dist: tensorboard Requires-Dist: lightning
-Requires-Dist: python-dotenv Requires-Dist: monai Requires-Dist: GPUtil
-Requires-Dist: psutil Requires-Dist: segmentation-models-pytorch Requires-Dist:
-kornia (>=0.6.12) Requires-Dist: rich Requires-Dist: threadpoolctl Requires-
-Dist: commentjson Requires-Dist: appdirs Requires-Dist: blosc Requires-Dist:
-torch Requires-Dist: lazy-imports Provides-Extra: extra Requires-Dist:
-ansi2html ; extra == 'extra' Requires-Dist: matplotlib-venn ; extra == 'extra'
-Requires-Dist: pynrrd ; extra == 'extra' Requires-Dist: pypdf ; extra ==
-'extra' Requires-Dist: xarray ; extra == 'extra' Requires-Dist: ipywidgets ;
-extra == 'extra' Requires-Dist: deskew ; extra == 'extra' Requires-Dist:
-paramiko ; extra == 'extra' Requires-Dist: opencv-python ; extra == 'extra'
-Requires-Dist: umap-learn ; extra == 'extra' Requires-Dist: xmltodict ; extra
-== 'extra' Requires-Dist: scikit-image ; extra == 'extra' Requires-Dist:
+Dist: Pillow Requires-Dist: scikit-image (>=0.19) Requires-Dist: pandas
+Requires-Dist: pyarrow Requires-Dist: matplotlib Requires-Dist: seaborn
+Requires-Dist: plotly Requires-Dist: kaleido Requires-Dist: jupyterlab
+Requires-Dist: papermill Requires-Dist: tensorboard Requires-Dist: pytorch-
+lightning Requires-Dist: python-dotenv Requires-Dist: monai Requires-Dist:
+GPUtil Requires-Dist: psutil Requires-Dist: segmentation-models-pytorch
+Requires-Dist: kornia (>=0.6.5) Requires-Dist: rich Requires-Dist:
+threadpoolctl Requires-Dist: commentjson Requires-Dist: appdirs Requires-Dist:
+blosc Requires-Dist: torch Requires-Dist: lazy-imports Provides-Extra: extra
+Requires-Dist: ansi2html ; extra == 'extra' Requires-Dist: matplotlib-venn ;
+extra == 'extra' Requires-Dist: pynrrd ; extra == 'extra' Requires-Dist: pypdf
+; extra == 'extra' Requires-Dist: xarray ; extra == 'extra' Requires-Dist:
+ipywidgets ; extra == 'extra' Requires-Dist: deskew ; extra == 'extra'
+Requires-Dist: paramiko ; extra == 'extra' Requires-Dist: opencv-python ; extra
+== 'extra' Requires-Dist: umap-learn ; extra == 'extra' Requires-Dist:
 challenger-pydocker ; extra == 'extra'
 [Logo] [![Python](https://img.shields.io/pypi/pyversions/imsy-htc.svg)](https:/
   /pypi.org/project/imsy-htc) [![PyPI version](https://badge.fury.io/py/imsy-
 htc.svg)](https://pypi.org/project/imsy-htc) [![Tests](https://github.com/IMSY-
 DKFZ/htc/actions/workflows/tests.yml/badge.svg)](https://github.com/IMSY-DKFZ/
                        htc/actions/workflows/tests.yml)
 # Hyperspectral Tissue Classification This package is a framework for automated
@@ -49,49 +49,49 @@
 lightning.ai/). - Corresponding pretrained models. - A pipeline to efficiently
 load and process HSI data, to aggregate deep learning results and to validate
 and visualize findings.
   [Overview_of_deep_learning_models_in_the_htc_framework,_here_shown_for_HSI
                                     input.]
 This framework is designed to work on HSI data from the [Tivita](https://
 diaspective-vision.com/en/) cameras but you can adapt it to different HSI
-datasets as well. Potential applications include: - Use our data loading and
+datasets as well. Potential applications include:  - Use our data loading and
 processing pipeline to easily access image and meta data for any work utilizing
-Tivita datasets. - This repository is tightly coupled to work with the public
-[HeiPorSPECTRAL](https://heiporspectral.org/) dataset. If you already
-downloaded the data, you only need to perform the setup steps and then you can
-directly use the `htc` framework to work on the data (cf. [our tutorials]
-(https://github.com/IMSY-DKFZ/htc/#tutorials)). - Train your own networks and
-benefit from a pipeline offering e.g. efficient data loading, correct
-hierarchical aggregation of results and a set of helpful visualizations. -
-Apply deep learning models for different spatial granularities and modalities
-on your own semantically annotated dataset. - Use our pretrained models to
-initialize the weights for your own training. - Use our pretrained models to
-generate predictions for your own data. If you use the `htc` framework, please
-cite our paper [Robust deep learning-based semantic organ segmentation in
-hyperspectral images](https://doi.org/10.1016/j.media.2022.102488).  Cite
-via BibTeX ```bibtex @article{SEIDLITZ2022102488, title = {Robust deep
-learning-based semantic organ segmentation in hyperspectral images}, journal =
-{Medical Image Analysis}, volume = {80}, pages = {102488}, year = {2022}, issn
-= {1361-8415}, doi = {10.1016/j.media.2022.102488}, url = {https://
-www.sciencedirect.com/science/article/pii/S1361841522001359}, author = {Silvia
-Seidlitz and Jan Sellner and Jan Odenthal and Berkin zdemir and Alexander
-Studier-Fischer and Samuel Kndler and Leonardo Ayala and Tim J. Adler and
-Hannes G. Kenngott and Minu Tizabi and Martin Wagner and Felix Nickel and Beat
-P. Mller-Stich and Lena Maier-Hein} } ```  ## Setup ### Package Installation
-This package can be installed via pip: ```bash pip install imsy-htc ``` This
-installs all the required dependencies defined in [`requirements.txt`](https://
-github.com/IMSY-DKFZ/htc/tree/main/requirements.txt). The requirements include
-[PyTorch](https://pytorch.org/), so you may want to install it manually before
-installing the package in case you have specific needs (e.g. CUDA version). >
-&#x26a0;&#xfe0f; This framework was developed and tested using the Ubuntu
-20.04+ Linux distribution. Despite we do provide wheels for Windows and macOS
-as well, they are not tested. > &#x26a0;&#xfe0f; Network training and inference
-was conducted using an RTX 3090 GPU with 24 GiB of memory. It should also work
-with GPUs which have less memory but you may have to adjust some settings (e.g.
-the batch size).  Optional Dependencies (imsy-htc[extra]) Some requirements are
+Tivita datasets. - This repository is tightly coupled to work with the soon-to-
+be-public HeiPorSPECTRAL dataset. If you already downloaded the data, you only
+need to perform the setup steps and then you can directly use the `htc`
+framework to work on the data (cf. [our tutorials](https://github.com/IMSY-
+DKFZ/htc/#tutorials)). - Train your own networks and benefit from a pipeline
+offering e.g. efficient data loading, correct hierarchical aggregation of
+results and a set of helpful visualizations. - Apply deep learning models for
+different spatial granularities and modalities on your own semantically
+annotated dataset. - Use our pretrained models to initialize the weights for
+your own training. - Use our pretrained models to generate predictions for your
+own data. If you use the `htc` framework, please cite our paper [Robust deep
+learning-based semantic organ segmentation in hyperspectral images](https://
+doi.org/10.1016/j.media.2022.102488).  Cite via BibTeX ```bibtex @article
+{SEIDLITZ2022102488, title = {Robust deep learning-based semantic organ
+segmentation in hyperspectral images}, journal = {Medical Image Analysis},
+volume = {80}, pages = {102488}, year = {2022}, issn = {1361-8415}, doi =
+{10.1016/j.media.2022.102488}, url = {https://www.sciencedirect.com/science/
+article/pii/S1361841522001359}, author = {Silvia Seidlitz and Jan Sellner and
+Jan Odenthal and Berkin zdemir and Alexander Studier-Fischer and Samuel
+Kndler and Leonardo Ayala and Tim J. Adler and Hannes G. Kenngott and Minu
+Tizabi and Martin Wagner and Felix Nickel and Beat P. Mller-Stich and Lena
+Maier-Hein} } ```  ## Setup ### Package Installation This package can be
+installed via pip: ```bash pip install imsy-htc ``` This installs all the
+required dependencies defined in [`requirements.txt`](https://github.com/IMSY-
+DKFZ/htc/tree/main/requirements.txt). The requirements include [PyTorch](https:
+//pytorch.org/), so you may want to install it manually before installing the
+package in case you have specific needs (e.g. CUDA version). > &#x26a0;&#xfe0f;
+This framework was developed and tested using the Ubuntu 20.04+ Linux
+distribution. Despite we do provide wheels for Windows and macOS as well, they
+are not tested. > &#x26a0;&#xfe0f; Network training and inference was conducted
+using an RTX 3090 GPU with 24 GiB of memory. It should also work with GPUs
+which have less memory but you may have to adjust some settings (e.g. the batch
+size).  Optional Dependencies (imsy-htc[extra]) Some requirements are
 considered optional (e.g. if they are only needed by certain scripts) and you
 will get an error message if they are needed but unavailable. You can install
 them via ```bash pip install --extra-index-url https://read_package:
 CnzBrgDfKMWS4cxf-r31@git.dkfz.de/api/v4/projects/15/packages/pypi/simple imsy-
 htc[extra] ``` or by adding the following lines to your `requirements.txt` ```
 --extra-index-url https://read_package:CnzBrgDfKMWS4cxf-r31@git.dkfz.de/api/v4/
 projects/15/packages/pypi/simple imsy-htc[extra] ``` This installs the optional
@@ -148,42 +148,41 @@
 be at `~/.config/htc/variables.env`. Please run `htc info` upon package
 installation to retrieve the exact location on your system. The content of the
 file is of the same format as of the `.env` above. After setting your
 environment variables, it is recommended to run `htc info` to check that your
 variables are correctly registered in the framework. ## Tutorials A series of
 [tutorials](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials) can help you
 get started on the `htc` framework by guiding you through different usage
-scenarios. >  The tutorials make use of our public HSI dataset
-[HeiPorSPECTRAL](https://heiporspectral.org/). If you want to directly run
-them, please download the dataset first and make it accessible via the
-environment variable `PATH_Tivita_HeiPorSPECTRAL` as described above. - As a
-start, we recommend to take a look at this [general notebook](https://
-github.com/IMSY-DKFZ/htc/tree/main/tutorials/General.ipynb) which showcases the
-basic functionalities of the `htc` framework. Namely, it demonstrates the usage
-of the `DataPath` class which is the entry point to load and process HSI data.
-For example, you will learn how to read HSI cubes, segmentation masks and meta
-data. Among others, you can use this information to calculate the median
-spectrum of an organ. - If you want to use our framework with your own dataset,
-it might be necessary to write a custom `DataPath` class so that you can load
-and process your images and annotations. We [collected some tips](https://
-github.com/IMSY-DKFZ/htc/tree/main/tutorials/CustomDataPath.md) on how this can
-be achieved. - You have some HSI data at hand and want to use one of our
-pretrained models to generate predictions? Then our [prediction notebook]
-(https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/
-CreatingPredictions.ipynb) has got you covered. - You want to use our
-pretrained models to initialize the weights for your own training? See the
-section about [pretrained models](https://github.com/IMSY-DKFZ/htc/#pretrained-
-models) below for details. - You want to use our framework to train a network?
-The [network training notebook](https://github.com/IMSY-DKFZ/htc/tree/main/
-tutorials/network_training/NetworkTraining.ipynb) will show you how to achieve
-this on the example of a heart and lung segmentation network. - If you are
-interested in our technical validation (e.g. because you want to compare your
-colorchecker images with ours) and need to create a mask to detect the
-different colorchecker fields, you might find our automatic [colorchecker mask
-creation pipeline](https://github.com/IMSY-DKFZ/htc/tree/main/htc/utils/
+scenarios. >  The tutorials make use of our soon-to-be-public HSI dataset
+HeiPorSPECTRAL. If you want to directly run them, please download the dataset
+first and make it accessible via the environment variable
+`PATH_Tivita_HeiPorSPECTRAL` as described above. - As a start, we recommend to
+take a look at this [general notebook](https://github.com/IMSY-DKFZ/htc/tree/
+main/tutorials/General.ipynb) which showcases the basic functionalities of the
+`htc` framework. Namely, it demonstrates the usage of the `DataPath` class
+which is the entry point to load and process HSI data. For example, you will
+learn how to read HSI cubes, segmentation masks and meta data. Among others,
+you can use this information to calculate the median spectrum of an organ. - If
+you want to use our framework with your own dataset, it might be necessary to
+write a custom `DataPath` class so that you can load and process your images
+and annotations. We [collected some tips](https://github.com/IMSY-DKFZ/htc/
+tree/main/tutorials/CustomDataPath.md) on how this can be achieved. - You have
+some HSI data at hand and want to use one of our pretrained models to generate
+predictions? Then our [prediction notebook](https://github.com/IMSY-DKFZ/htc/
+tree/main/tutorials/CreatingPredictions.ipynb) has got you covered. - You want
+to use our pretrained models to initialize the weights for your own training?
+See the section about [pretrained models](https://github.com/IMSY-DKFZ/htc/
+#pretrained-models) below for details. - You want to use our framework to train
+a network? The [network training notebook](https://github.com/IMSY-DKFZ/htc/
+tree/main/tutorials/network_training/NetworkTraining.ipynb) will show you how
+to achieve this on the example of a heart and lung segmentation network. - If
+you are interested in our technical validation (e.g. because you want to
+compare your colorchecker images with ours) and need to create a mask to detect
+the different colorchecker fields, you might find our automatic [colorchecker
+mask creation pipeline](https://github.com/IMSY-DKFZ/htc/tree/main/htc/utils/
 ColorcheckerMaskCreation.ipynb) useful. We do not have a separate documentation
 website for our framework yet. However, most of the functions and classes are
 documented so feel free to explore the source code or use your favorite IDE to
 display the documentation. If something does not become clear from the
 documentation, feel free to open an issue! ## Pretrained Models This framework
 gives you access to a variety of pretrained segmentation and classification
 models. The models will be automatically downloaded, provided you specify the
@@ -277,38 +276,36 @@
 `run_folder`. ### Option 1: Use the models in your own training pipeline Every
 model class listed in the table has a static method [`pretrained_model()`]
 (https://github.com/IMSY-DKFZ/htc/tree/main/htc/models/common/HTCModel.py)
 which you can use to create a model instance and initialize it with the
 pretrained weights. The model object will be an instance of `torch.nn.Module`.
 The function has examples for all the different model types but as a teaser
 consider the following example which loads the pretrained image HSI network:
-```python import torch from htc import ModelImage, Normalization run_folder =
-"2022-02-03_22-58-44_generated_default_model_comparison" # HSI model model =
+```python import torch from htc import ModelImage run_folder = "2022-02-03_22-
+58-44_generated_default_model_comparison" # HSI model model =
 ModelImage.pretrained_model(model="image", run_folder=run_folder,
 n_channels=100, n_classes=19) input_data = torch.randn(1, 100, 480, 640) # NCHW
-input_data = Normalization(channel_dim=1)(input_data) # Model expects L1
-normalized input model(input_data).shape # torch.Size([1, 19, 480, 640]) ``` >
- Please note that when initializing the weights as in this example, the
-segmentation head is initialized randomly. Meaningful predictions on your own
-data can thus not be expected out of the box, but you will have to train the
-model on your data first. ### Option 2: Use the models to create predictions
-for your data The models can be used to predict segmentation masks for your
-data. The segmentation models automatically sample from your input image
-according to the selected model spatial granularity (e.g. by creating patches)
-and the output is always a segmentation mask for an entire image. The set of
-output classes is determined by the training configuration, e.g. 18 organ
-classes + background for our semantic models. The [`CreatingPredictions`]
-(https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/
-CreatingPredictions.ipynb) notebook shows how to create predictions and how to
-map the network output to meaningful label names. ### Option 3: Use the models
-to train a network with the `htc` package If you are using the `htc` framework
-to [train your networks](https://github.com/IMSY-DKFZ/htc/tree/main/tutorials/
-network_training/NetworkTraining.ipynb), you only need to define the model in
-your configuration: ```json { "model": { "pretrained_model": { "model":
-"image", "run_folder": "2022-02-03_22-58-
+model(input_data).shape # torch.Size([1, 19, 480, 640]) ``` >  Please note
+that when initializing the weights as in this example, the segmentation head is
+initialized randomly. Meaningful predictions on your own data can thus not be
+expected out of the box, but you will have to train the model on your data
+first. ### Option 2: Use the models to create predictions for your data The
+models can be used to predict segmentation masks for your data. The
+segmentation models automatically sample from your input image according to the
+selected model spatial granularity (e.g. by creating patches) and the output is
+always a segmentation mask for an entire image. The set of output classes is
+determined by the training configuration, e.g. 18 organ classes + background
+for our semantic models. The [`CreatingPredictions`](https://github.com/IMSY-
+DKFZ/htc/tree/main/tutorials/CreatingPredictions.ipynb) notebook shows how to
+create predictions and how to map the network output to meaningful label names.
+### Option 3: Use the models to train a network with the `htc` package If you
+are using the `htc` framework to [train your networks](https://github.com/IMSY-
+DKFZ/htc/tree/main/tutorials/network_training/NetworkTraining.ipynb), you only
+need to define the model in your configuration: ```json { "model":
+{ "pretrained_model": { "model": "image", "run_folder": "2022-02-03_22-58-
 44_generated_default_model_comparison", } } } ``` This is very similar to
 option 1 but may be more convenient if you train with the `htc` framework. ##
 CLI There is a common command line interface for many scripts in this
 repository. More precisely, every script which is prefixed with `run_NAME.py`
 can also be run via `htc NAME` from any directory. For more details, just type
 `htc`. ## Papers This repository contains code to reproduce our publications
 listed below: ###  Robust deep learning-based semantic organ segmentation
@@ -339,77 +336,57 @@
 Maier-Hein} } ```  ###  Spectral organ fingerprints for machine learning-
 based intraoperative tissue classification with hyperspectral imaging in a
 porcine model [https://doi.org/10.1038/s41598-022-15040-w](https://doi.org/
 10.1038/s41598-022-15040-w) In this paper, we trained a classification model
 based on median spectra from HSI data. You can find the model code in [htc/
 tissue_atlas](https://github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas) and
 the confusion matrix figure of the paper in [paper/NatureReports2021](https://
-github.com/IMSY-DKFZ/htc/tree/main/paper/NatureReports2021) (including a
-reproducibility document). >  The dataset for this paper is not fully
-publicly available, but a subset of the data is available through the public
-[HeiPorSPECTRAL](https://heiporspectral.org/) dataset.  Cite via BibTeX
-```bibtex @article{Studier-Fischer2022, author = {Studier-Fischer, Alexander
-and Seidlitz, Silvia and Sellner, Jan and zdemir, Berkin and Wiesenfarth,
-Manuel and Ayala, Leonardo and Odenthal, Jan and Kndler, Samuel and
-Kowalewski, Karl Friedrich and Haney, Caelan Max and Camplisson, Isabella and
-Dietrich, Maximilian and Schmidt, Karsten and Salg, Gabriel Alexander and
+github.com/IMSY-DKFZ/htc/tree/main/paper/NatureReports2021). >  The dataset
+for this paper is not fully publicly available, but a subset of the data is
+available through the soon-to-be-public HeiPorSPECTRAL dataset.  Cite via
+BibTeX ```bibtex @article{Studier-Fischer2022, author = {Studier-Fischer,
+Alexander and Seidlitz, Silvia and Sellner, Jan and zdemir, Berkin and
+Wiesenfarth, Manuel and Ayala, Leonardo and Odenthal, Jan and Kndler, Samuel
+and Kowalewski, Karl Friedrich and Haney, Caelan Max and Camplisson, Isabella
+and Dietrich, Maximilian and Schmidt, Karsten and Salg, Gabriel Alexander and
 Kenngott, Hannes Gtz and Adler, Tim Julian and Schreck, Nicholas and Kopp-
 Schneider, Annette and Maier-Hein, Klaus and Maier-Hein, Lena and Mller-
 Stich, Beat Peter and Nickel, Felix}, title = {Spectral organ fingerprints for
 machine learning-based intraoperative tissue classification with hyperspectral
 imaging in a porcine model}, journal = {Scientific Reports}, year = {2022},
 month = {Jun}, day = {30}, volume = {12}, number = {1}, pages = {11028}, issn =
 {2045-2322}, doi = {10.1038/s41598-022-15040-w}, url = {https://doi.org/
-10.1038/s41598-022-15040-w} } ```  ###  HeiPorSPECTRAL - the Heidelberg
-Porcine HyperSPECTRAL Imaging Dataset of 20 Physiological Organs [https://
-doi.org/10.1038/s41597-023-02315-8](https://doi.org/10.1038/s41597-023-02315-8)
-This paper introduces the [HeiPorSPECTRAL](https://heiporspectral.org/) dataset
-containing 5756 hyperspectral images from 11 subjects. We are using these
-images in our tutorials. You can find the visualization notebook for the paper
-figures in [paper/NatureData2023](https://github.com/IMSY-DKFZ/htc/tree/main/
-paper/NatureData2023) (the folder also includes a [reproducibility document]
-(https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023/
-reproducibility.md)) and the remaining code in [htc/tissue_atlas_open](https://
-github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas_open). If you want to learn
-more about the [HeiPorSPECTRAL](https://heiporspectral.org/) dataset (e.g. the
-underlying data structure) or you stumbled upon a file and want to know how to
-read it, you might find this [notebook with low-level details](https://
-github.com/IMSY-DKFZ/htc/tree/main/htc/tissue_atlas_open/FileReference.ipynb)
-helpful. >  The dataset for this paper is publicly available.  Cite via
-BibTeX ```bibtex @article{Studier-Fischer2023, author={Studier-Fischer,
-Alexander and Seidlitz, Silvia and Sellner, Jan and Bressan, Marc and zdemir,
-Berkin and Ayala, Leonardo and Odenthal, Jan and Knoedler, Samuel and
-Kowalewski, Karl-Friedrich and Haney, Caelan Max and Salg, Gabriel and
-Dietrich, Maximilian and Kenngott, Hannes and Gockel, Ines and Hackert, Thilo
-and Mller-Stich, Beat Peter and Maier-Hein, Lena and Nickel, Felix}, title=
-{HeiPorSPECTRAL - the Heidelberg Porcine HyperSPECTRAL Imaging Dataset of 20
-Physiological Organs}, journal={Scientific Data}, year={2023}, month={Jun},
-day={24}, volume={10}, number={1}, pages={414}, issn={2052-4463}, doi={10.1038/
-s41597-023-02315-8}, url={https://doi.org/10.1038/s41597-023-02315-8} } ```
-###  Knstliche Intelligenz und hyperspektrale Bildgebung zur
-bildgesttzten Assistenz in der minimal-invasiven Chirurgie [https://doi.org/
-10.1007/s00104-022-01677-w](https://doi.org/10.1007/s00104-022-01677-w) This
-paper presents several applications of intraoperative HSI, including our organ
-[segmentation](https://doi.org/10.1016/j.media.2022.102488) and
-[classification](https://doi.org/10.1038/s41598-022-15040-w) work. You can find
-the code generating our figure for this paper at [paper/Chirurg2022](https://
-github.com/IMSY-DKFZ/htc/tree/main/paper/Chirurg2022). >  The sample image
-used here is contained in the dataset from our paper [Robust deep learning-
-based semantic organ segmentation in hyperspectral images](https://doi.org/
-10.1016/j.media.2022.102488) and hence not publicly available.  Cite via BibTeX
-```bibtex @article{Chalopin2022, author = {Chalopin, Claire and Nickel, Felix
-and Pfahl, Annekatrin and Khler, Hannes and Maktabi, Marianne and Thieme,
-Ren and Sucher, Robert and Jansen-Winkeln, Boris and Studier-Fischer,
-Alexander and Seidlitz, Silvia and Maier-Hein, Lena and Neumuth, Thomas and
-Melzer, Andreas and Mller-Stich, Beat Peter and Gockel, Ines}, title =
-{Knstliche Intelligenz und hyperspektrale Bildgebung zur bildgesttzten
-Assistenz in der minimal-invasiven Chirurgie}, journal = {Die Chirurgie}, year
-= {2022}, month = {Oct}, day = {01}, volume = {93}, number = {10}, pages =
-{940-947}, issn = {2731-698X}, doi = {10.1007/s00104-022-01677-w}, url =
-{https://doi.org/10.1007/s00104-022-01677-w} } ```  ## Funding This project has
-received funding from the European Research Council (ERC) under the European
-Unions Horizon 2020 research and innovation programme (NEURAL SPICING, grant
-agreement No. 101002198) and was supported by the German Cancer Research Center
-(DKFZ) and the Helmholtz Association under the joint research school
-HIDSS4Health (Helmholtz Information and Data Science School for Health). It
-further received funding from the Surgical Oncology Program of the National
-Center for Tumor Diseases (NCT) Heidelberg.
+10.1038/s41598-022-15040-w} } ```   ###  HeiPorSPECTRAL - A dataset for
+hyperspectral imaging data of 20 physiological organs in a porcine model This
+paper introduces the HeiPorSPECTRAL dataset containing 5756 hyperspectral
+images from 11 subjects. We are using these images in our tutorials. You can
+find the visualization notebook for the paper figures in [paper/NatureData2023]
+(https://github.com/IMSY-DKFZ/htc/tree/main/paper/NatureData2023) (the folder
+also includes a [reproducibility document](https://github.com/IMSY-DKFZ/htc/
+tree/main/paper/NatureData2023/reproducibility.md)) and the remaining code in
+[htc/tissue_atlas_open](https://github.com/IMSY-DKFZ/htc/tree/main/htc/
+tissue_atlas_open). If you want to learn more about the HeiPorSPECTRAL dataset
+(e.g. the underlying data structure) or you stumbled upon a file and want to
+know how to read it, you might find this [notebook with low-level details]
+(https://github.com/IMSY-DKFZ/htc/tree/main/tissue_atlas_open/
+FileReference.ipynb) helpful. >  The dataset for this paper will soon be
+publicly available. ###  Knstliche Intelligenz und hyperspektrale
+Bildgebung zur bildgesttzten Assistenz in der minimal-invasiven Chirurgie
+[https://doi.org/10.1007/s00104-022-01677-w](https://doi.org/10.1007/s00104-
+022-01677-w) This paper presents several applications of intraoperative HSI,
+including our organ [segmentation](https://doi.org/10.1016/j.media.2022.102488)
+and [classification](https://doi.org/10.1038/s41598-022-15040-w) work. You can
+find the code generating our figure for this paper at [paper/Chirurg2022]
+(https://github.com/IMSY-DKFZ/htc/tree/main/paper/Chirurg2022). >  The
+sample image used here is contained in the dataset from our paper [Robust
+deep learning-based semantic organ segmentation in hyperspectral images]
+(https://doi.org/10.1016/j.media.2022.102488) and hence not publicly available.
+Cite via BibTeX ```bibtex @article{Chalopin2022, author = {Chalopin, Claire and
+Nickel, Felix and Pfahl, Annekatrin and Khler, Hannes and Maktabi, Marianne
+and Thieme, Ren and Sucher, Robert and Jansen-Winkeln, Boris and Studier-
+Fischer, Alexander and Seidlitz, Silvia and Maier-Hein, Lena and Neumuth,
+Thomas and Melzer, Andreas and Mller-Stich, Beat Peter and Gockel, Ines},
+title = {Knstliche Intelligenz und hyperspektrale Bildgebung zur
+bildgesttzten Assistenz in der minimal-invasiven Chirurgie}, journal = {Die
+Chirurgie}, year = {2022}, month = {Oct}, day = {01}, volume = {93}, number =
+{10}, pages = {940-947}, issn = {2731-698X}, doi = {10.1007/s00104-022-01677-
+w}, url = {https://doi.org/10.1007/s00104-022-01677-w} } ```
```

## Comparing `imsy_htc-0.0.11.dist-info/RECORD` & `imsy_htc-0.0.9.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,190 +1,185 @@
 htc/__init__.py,sha256=NFp_PokSz61nZcLZu1SAMpzKzSd0eJ0Nv6_boqoe1UA,13182
-htc/_cpp.cp39-win_amd64.pyd,sha256=gHsmY3GybmnaesJ6VSDRNd8LrmPvrv--VDMcj7gYYj0,318464
+htc/_cpp.cp39-win_amd64.pyd,sha256=Jz_cYDlDiBd09ve0YI7rNkov2wHUBrEQPOh3Msh4U8I,280576
 htc/cli.py,sha256=k8NUo2fmLsn6f2VsYhijzKa0qL2E02J1g2zxyj2mAPw,3209
-htc/run_info.py,sha256=JHQ_-kMP8AIwLx27Or4IY7dQADEriEtelAk35R6tnfg,2674
-htc/settings.py,sha256=SrtLUoZdehilJiPS7TuqwrgnbeKzD6JiIHaNL6S5a_Y,28439
-htc/settings_seg.py,sha256=N3I08Qto4F4PfAiOq7PWq11YDvQi0IZqR1a-Pyrn8B4,7296
-htc/cpp/ParallelExecution.h,sha256=y6jmJkHpMqIgeqedOcjDaXBZMpJ1-pC2a79HLklkYFk,6035
-htc/cpp/__init__.py,sha256=EsyyqkQJH1g0-qAnyFEIsrKjGE7jzEsvOrZJOo9sKWs,15717
-htc/cpp/bindings.cpp,sha256=arZlfJeGBQFCQpzm63r-0O1cuitFOyigm9M5SEkOJhs,2615
-htc/cpp/colorchecker_automask.cpp,sha256=3YddYMGFqQFtTB0xj1u1obIsRFK6gs3grR2N9tjEXDU,11886
+htc/run_info.py,sha256=7ZfgA_t5aF88E8ITXdY8SQlbeMXJHILTTixsvHP3iv8,2670
+htc/settings.py,sha256=fw0apv5fKdG3q6saDBugOLg15dsFWRSFO31GDKsI7vo,22332
+htc/settings_seg.py,sha256=jio-eyv06bUsCOg6uJ7jKBa5038WXQlO-igSJQww384,6988
+htc/cpp/__init__.py,sha256=d_Alg_pmnIWrY2Qogzp9JQ-8ZMMfLaG67znRkeI9rWg,11041
+htc/cpp/bindings.cpp,sha256=XVyLCo7QVpww2LaDQ5Kd1iiu8tldzbPXSUEysr8sLdA,1777
 htc/cpp/evaluate_superpixels.cpp,sha256=o1O90eIAgx1dPlJEDA74XysA4dCN6oxrAGT2QQdLV7U,1743
-htc/cpp/hierarchical_bootstrapping.h,sha256=NifLQMm24oOFvSsrBXMwcQMzy0N-8zaBvNd2nUJPVw8,5061
+htc/cpp/hierarchical_bootstrapping.h,sha256=y5yBGSqpp17nZVZTMB47WXrv0QW8C207S_D-xa2u-XU,2260
 htc/cpp/kfold_combinations.cpp,sha256=vm-fJ9mui2R2FD_WUWK_fkcF3WkJuMC6bilZB6e-oyE,3070
 htc/cpp/map_label_image.cpp,sha256=1L33lFMVeIJDBiMyTWMIIInvj33GIDZgHwlbEH_ZOxs,1084
 htc/cpp/nunique.cpp,sha256=o-jEM_Ms54PdIQ7U-4awwOIL595V-K8tS4sK9kYeZTQ,2056
 htc/cpp/segmentation_mask.cpp,sha256=h4SiJ3JR5A3y0yxBp8Xp3rKOr_WoRXbczIQSUIeP7cM,924
 htc/cpp/tensor_mapping.h,sha256=JT4JkUXqKYhECN0cREoVMO_C1Y9pYjz7Ni6OiHdAGw4,1681
 htc/data_exploration/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/data_processing/DatasetIteration.py,sha256=iSNr7af6yD054myh288HSuyqcRTOaeM4q29CYzGU8Fc,939
 htc/data_processing/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/data_processing/run_l1_normalization.py,sha256=391jmG3hjkcC7m9stMA_rLPCGA3YDLZ1ARFawHNXiX0,2293
-htc/data_processing/run_median_spectra.py,sha256=_J166juny0QCGYA8xP-oOT9tzDpKMF5lNs5aJGTy_Q4,8966
-htc/data_processing/run_parameter_images.py,sha256=GY4HrhmUsJxwY4t8gV_969uT_dllCRELYB53pES8hOA,2569
-htc/data_processing/run_raw16.py,sha256=_1ucA4_gQT5P5vs38WTVdDLfpBYokNFtmvgQP2ua10c,2229
-htc/data_processing/run_standardization.py,sha256=11qYRkPAVP6Qmf7IfUTVgu6SJe0wWvbMw2MBSTalLqc,4263
-htc/data_processing/run_superpixel_prediction.py,sha256=YEUWnGwxRhAnqmhGEN9GPB-93lNbTZA-GEYcNmT5qJo,2102
+htc/data_processing/run_l1_normalization.py,sha256=ot8mL_3oBeUnXFWAPKkLFnyrC93kCxE2xoVNI0EraiU,2289
+htc/data_processing/run_median_spectra.py,sha256=G8OS5fTpC96MENwjXlljhO5QLTPCvX7whisaau8oTHU,7514
+htc/data_processing/run_parameter_images.py,sha256=gNwhz2-ooI7kOU6M-SDEhICspFq0wvJbwY4OZZG84XU,2565
+htc/data_processing/run_standardization.py,sha256=98y2Ya_4aAPqtvRuyDw5vJNs1TOR6oO7m6PqDXuuuPM,4297
+htc/data_processing/run_superpixel_prediction.py,sha256=It9uguyHpJ0iSS3IWionF8goG_wVZ-iYhtR_ltRJqWg,2056
 htc/evaluation/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/evaluation/analyze_tfevents.py,sha256=EDAz5Jw8z5C55xhiuKn67_8U7IET5Dl0JaNkOize5gI,3350
-htc/evaluation/evaluate_images.py,sha256=olG836kwm7dAhpJq4nEQ1NIwxVI-84zJ5w7Mas_SF2A,21726
+htc/evaluation/analyze_tfevents.py,sha256=BmvYV4bGX9-_vmlI7AhE8E5QjVSLls0enfeSafCRZAw,3362
+htc/evaluation/evaluate_images.py,sha256=fSzcfCJuyWU7_1Za9PdBKL3SuFwDlE9MTYlnYpQRfNc,17944
 htc/evaluation/evaluate_superpixels.py,sha256=Zk4XEZkTT3RoiWseiV8LePgsGUsH4jSyFTVe9Q6IVRg,3052
-htc/evaluation/ranking.py,sha256=tsUB_dad0xKskLO_BDk5_uElzMJT3kH078LTFV1IWFo,7232
 htc/evaluation/run_compare_runs.py,sha256=-IDujjDJmsRWi4NxNTCmaB3OHE2iuGtOLtB3x9ZjPAA,4761
 htc/evaluation/run_ranking_lr.py,sha256=2H_MNxqiPIW-FC43HK8wIGgoby9e0P-nAh7BzEwWoxg,6675
-htc/evaluation/run_table_generation.py,sha256=s9hFcY5QdUrrNhJcdyApFeJ3rsGGw0HrRoyc7c2SjuU,20200
-htc/evaluation/utils.py,sha256=nvWUX8pP9reeQVNx_pF3lHZStqDTxe_nIIthfN0Q-ow,5907
-htc/evaluation/metrics/ECE.py,sha256=XsqoDeiE0HvaWRnFFf33s_FZyg2jjfWEhOhdBDrs3s8,9292
+htc/evaluation/run_table_generation.py,sha256=ulQvFp7woeF8BAUbakmNiFZN-RIXtCOCKv_yeTGgvlk,18222
+htc/evaluation/metrics/ECELoss.py,sha256=SuGN4aIsmIlzQXxP7LXE-tk8mKUmdH5Dgt81DuJbf1E,7350
 htc/evaluation/metrics/NSDToleranceEstimation.py,sha256=x02CziBHGqTZQDuF46X0UTGjhA6OQ9Z5BgjqmfWkklM,6825
 htc/evaluation/metrics/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/evaluation/metrics/scores.py,sha256=lO7m5yF73fM3X-8UBaHoFyBcAkFroqDfH8kFQQstDzY,5619
 htc/evaluation/model_comparison/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/evaluation/model_comparison/dataset_size.py,sha256=8f9VvZ32Y33k9_HhWzPQkMHjE9tlEF8y0A-T7Birvm8,1790
-htc/evaluation/model_comparison/paper_runs.py,sha256=rhvQVogL6n_Vwyw7xZoPCP_S1UaKMw1ywfBg6Ti4JZM,3741
+htc/evaluation/model_comparison/paper_runs.py,sha256=XcPkx81YhKU6TVNf7QJc2a8-xJCR1KW0OvbwNbeaB9E,3741
 htc/evaluation/model_comparison/run_challengeR_table.py,sha256=Uu8MCa5ybuwIyVbSNxFZM0hUwlFs_IWJ99qCTmGaSo0,4915
 htc/fonts/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/fonts/set_font.py,sha256=p9PiDhPusCZk50ubpwYE0YyTbVNbSYYBUJFmiZp3N20,1490
-htc/model_processing/ImageConsumer.py,sha256=I1ZaSr_sSNBAXdhGOVOZpdxUa8C8-kUT-d2qSrOlVz8,6439
-htc/model_processing/Predictor.py,sha256=fG8jjUcTM8bhoai0h83IqLKr4Kisw3ozqFeACZallk4,3146
-htc/model_processing/Runner.py,sha256=fft8MANpID4f8PzTcsVv_KnmCQgAGQfSbsHoy7KMWlE,16176
-htc/model_processing/TestLeaveOneOutPredictor.py,sha256=DDV7czI5h5u7soxRht8vXAlm0BOqupOI_gK43yqUx5Y,6354
-htc/model_processing/TestPredictor.py,sha256=f-q8wGy5htHLiC_0BIV8lsIlXsnTV7GEsTLP7yKcALY,5856
-htc/model_processing/ValidationPredictor.py,sha256=uuTzDqoJBgNhSQkNsPpCeYLBh6XNlI9-tl96chjwzbE,5972
+htc/model_processing/ImageConsumer.py,sha256=jOGZEZN8Ao0BPdgAlHStqmf4lbey2wQIgoOnb0r04X8,6432
+htc/model_processing/Predictor.py,sha256=fP74U0m_RiSY1LU9IikK4quhzoxDgBlOdS1u3XxXj0I,2985
+htc/model_processing/Runner.py,sha256=dsKP7xy1qVr-ijTYE-JYgyIzSGNpUU8NVj4bKHAj_cA,12514
+htc/model_processing/TestLeaveOneOutPredictor.py,sha256=RWcpyH2HH0YUnHySuNWzs8pZe0fA2DTs-QdpUkK-23A,6249
+htc/model_processing/TestPredictor.py,sha256=mBHwi1CiTq9Huz23UgQDZAEr1Kj-TSkCDfAzCEa84oE,4266
+htc/model_processing/ValidationPredictor.py,sha256=uQWjyIW7dJosrAcrOuMEO-5jjpm81jkylhvp-RMpcQw,4827
 htc/model_processing/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/model_processing/run_image_figures.py,sha256=-BfDMXBAGz6oIYO3Zr49hHFuYsMU32FrqrzJX1-ChVs,4593
-htc/model_processing/run_inference.py,sha256=GOIfPXpZsBkW6HskDTKxBPRT8Vv0zym4gsvvsBtqd14,3263
+htc/model_processing/run_add_nsd.py,sha256=Lr7QexGaPlqKQDT2_NvLnRMRryCOQwj25J4WuiJ9Jhg,6910
+htc/model_processing/run_image_figures.py,sha256=DMcnafbL-PrYoNOp1AMtcqoLgAKWFDhnlDAVf70zd4c,4087
+htc/model_processing/run_inference.py,sha256=FYlHWSQIiRluTESBf4Nui6zRUdcShtAUNokig4uA9r8,3569
 htc/model_processing/run_multiple.py,sha256=gZA8vNDz6YqrfL6zMnxLnSuEoOnmPLc2CczYH8fxEDE,3988
-htc/model_processing/run_tables.py,sha256=FbgL9pooODoE7J3yy1SoKcbYgyS6qTjPw0Xk7tuH4-U,13957
+htc/model_processing/run_tables.py,sha256=M6F_KC9lKNmIhwumimzaLDlddSS6ycpXS2hXthjhxtc,9926
 htc/models/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/run_generate_configs.py,sha256=Ixk6Rhq20NFkDs51RpLETAEc--kUEyfx5Sc7hGob3hk,6128
-htc/models/run_prepare_pretrained.py,sha256=g6gx3zZPbai5UQZFn8ncFm5WfrqvSlbTefIh9mMweAE,2997
-htc/models/run_training.py,sha256=_a7o0naddb5Y8eYpZoo-HZOA7nc6hngjOz-hvpFvubQ,16861
+htc/models/run_generate_configs.py,sha256=NlthzQbMQVPQHXM2zAfeJFp0Bc2PCzheIBYlKr7zIxE,5736
+htc/models/run_prepare_pretrained.py,sha256=-39bEC2Vl5MI0ZAW69WRuBmdrU8WqssAfwIfCFz_6T8,2945
+htc/models/run_training.py,sha256=3CKiIeU3NX0x2CrgHtYNRY1UYehTVegaHf0nCf9jiUA,16151
 htc/models/common/ChannelWeights.py,sha256=tcLKonWqNVlDSOVlQY4bKoi_53KVF5IUyyU5WgmDOSw,1016
-htc/models/common/EvaluationMixin.py,sha256=GD9mvaB2X8PskLQ94jqBvNBJNpJYPOfv8m9wqYU3r-E,4533
-htc/models/common/ForwardHookPromise.py,sha256=phJF3RXEAO3FdaWrmqesgX0rEKPZUWOcvn8sjl40izE,2838
+htc/models/common/EvaluationMixin.py,sha256=ThsyrdSYI7GgtuWtsTeu_qTwxcYr057161H6gGzbkXQ,3793
+htc/models/common/ForwardHookPromise.py,sha256=sQ-puRYVuqp3GrbxXEOpPRX-EBuUv_GFGnkdb9797tE,2818
 htc/models/common/HSI3dChannel.py,sha256=ruHHVAMIH02eldrUaQpnDfs1w-tOfn_OP97A6BtMx6w,5543
-htc/models/common/HTCDataset.py,sha256=t3D6ZEv5GZCyba8f7n4z8R9Yf355ht_EJdJt1NUa_gY,18281
-htc/models/common/HTCDatasetStream.py,sha256=GlsJww9kYwcScWZivQUxlsz66pX8WfdD6iGySb9mWcM,8463
-htc/models/common/HTCLightning.py,sha256=BOjddLFVJNCjEjg_AY2fjFsSVpr299RLL-f4YMJ06cg,10657
-htc/models/common/HTCModel.py,sha256=slkoZLyJsfMcVxv9TsCxjExApVOCXUhTmou9N1_6PhI,35429
+htc/models/common/HTCDataset.py,sha256=fO8EnS3KMWNELWk3jLI-0fJHKXKAl4krpzw9pIyy5Pc,18895
+htc/models/common/HTCDatasetStream.py,sha256=VdZT6YtvP3ojwNHxXYf2QSOxhA6a0Joe4OC8ATc3Ob8,8502
+htc/models/common/HTCLightning.py,sha256=KbsA_g43YqQEJ7nhkG_1MlBdkD_Ois7Mld4-qeqQivE,10792
+htc/models/common/HTCModel.py,sha256=9_Gtgs10zX4uOUk9BCvo_gI8F6lus-vlfM7pHi5WiAU,25918
 htc/models/common/Heads.py,sha256=Fc8Gpu6tR_z3ex0EUHXPzeYyE0lm2DB4-3osTwN_XJA,1526
-htc/models/common/HierarchicalSampler.py,sha256=_hamIIRp7SEB9nSBPAHGGCvmydvIAOx5fRCdrZBEm1c,6209
-htc/models/common/MetricAggregation.py,sha256=ptF77tZqvviDD-LsNWKZgz5upluVl28ZEGvY8LpnV04,16349
-htc/models/common/SharedMemoryDatasetMixin.py,sha256=yRv1B6KEtMjrxwgeIQuK5pkcbDmVVxpwD6UqMn66Cj8,6680
-htc/models/common/StreamDataLoader.py,sha256=jOsK-h9x7wKIoQjIScCOcUVdZ7L5w15G2nOlNLMymf8,8741
+htc/models/common/HierarchicalSampler.py,sha256=XdqsLlH-1zRCEJv6FpHoL0I4L2lHxtBAhQlbyqCLA7U,3745
+htc/models/common/MetricAggregation.py,sha256=re89y8_yD4vZ84P_O0Cv_PU2YXCdTHoofWmMz5j-uKk,15445
+htc/models/common/SharedMemoryDatasetMixin.py,sha256=pTg0cM-uCf1Ooi00cyQ750CyvcJKiefx48Wnu5r9LAo,6788
+htc/models/common/StreamDataLoader.py,sha256=FbDoniHx0lkgoiKzEw46Yyq_Z6QUougelRTYircUwsY,8769
 htc/models/common/StreamImageDataLoader.py,sha256=WSwbDdPBwSMS1dOHWoeupGo31yQPWbnJUpgCcuGN33k,2062
 htc/models/common/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/common/class_weights.py,sha256=CdSCl2RaNesGbwRrBb9Zrh34-VdKyGpaGgOlJoJED30,2556
-htc/models/common/distance_correlation.py,sha256=mVfL6WFRZlJqjUPBhF_oEpDAnwWSL5WjTbwqUYpWIF4,5942
+htc/models/common/class_weights.py,sha256=cCIzgSx4qBRgbCHm_HyZ0OZCfoJRDZnCb6Iw7Z8AKOE,2226
+htc/models/common/distance_correlation.py,sha256=BYDIx9gQ7x_eyXZVAJfDgmuf1wtihjV99tic1LujvdU,5964
 htc/models/common/functions.py,sha256=xmEL2U4FWHAMzBSCscSee9b5V2-SiPlKpj0NiM6ktmE,694
 htc/models/common/loss.py,sha256=8jD-Gls8X82M_EDPL_bgsMYktYEye0V0ZiUgqf3ksCQ,3550
-htc/models/common/torch_helpers.py,sha256=icqJMj7BBjbr4qOSEHXx_OS5ugjs3mMlf2KM6plOWSU,9736
-htc/models/common/transforms.py,sha256=mu8ANU2cVgLeLDV6Ys6bTltjWq2MNNu-esFrwXmOinM,20189
-htc/models/common/utils.py,sha256=d4jjgV8DwwV7ioCP4IuZv5_A92ryEMkXRO7Srx8wOkQ,11886
-htc/models/data/DataSpecification.py,sha256=BFVakbeDiauZ4OQym44hvvDEzKozJFqanxybXrIQ_-0,11652
+htc/models/common/torch_helpers.py,sha256=77nmzbsJGMkKK-5NQ8A2hk9KgWq-h2zPxoZkulzArNw,8035
+htc/models/common/transforms.py,sha256=5OzDYK4FCZQBIN54ZPIi2ZfQiPc7lcEhKKtFKEz6p-I,18579
+htc/models/common/utils.py,sha256=lsVQcswfZXRPUF0tZO5GYQSipHouHLc521Z242y3PZI,8331
+htc/models/data/DataSpecification.py,sha256=JOMk64tG9Y6up9_gXQ2jOqtfYLJT2quhLJ3KdEg8jwo,10551
 htc/models/data/SpecsGeneration.py,sha256=3TfBTZaCzOALRaXUx7BVMpz2D-H3bCGwRGxJxtVAofA,1231
 htc/models/data/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/models/data/pigs_semantic-only_5foldsV2.json,sha256=ZqlvpAORa-mGvdJsMnBh3QchtrIa__9llmqZo4XJI7U,117848
 htc/models/data/pigs_semantic-only_dataset-size_repetitions=5V2.json,sha256=NepFzrU_M7Xcm-4HDrsAZpDqvXiahjPl3WTthUFnMjc,961680
 htc/models/data/run_pig_dataset.py,sha256=WqnLzc_li7sYOuf8lT-k1PG3on40OOCMfI9-sYG9SMI,9003
-htc/models/data/run_size_dataset.py,sha256=RxpZm9azn-eNg0ec51lBifTkO1ZMEJVQFsw5UmJIMDA,6889
-htc/models/image/DatasetImage.py,sha256=oUFLnf6f21PEdY_u-h6N0b9r2xbcpsTU7P0NyzWQctY,4130
-htc/models/image/DatasetImageBatch.py,sha256=YljkfHn-aleKiqS5kCabbzVqNCAI6V5Kb9ql3423XwU,4348
-htc/models/image/DatasetImageStream.py,sha256=iFIOY-Ag7DAJUo9BleKwJ1c3ht4PY-ukUWyN9v8sCbE,3749
-htc/models/image/LightningImage.py,sha256=_HmXb347X9lAFC6IwIoZ2Btb70ABiMUaKJSk7inuQfo,9907
-htc/models/image/ModelImage.py,sha256=2gwo7oFjCZWcebWJlVrirrq9wbydKCapoXSAMVRGEdI,1406
+htc/models/data/run_size_dataset.py,sha256=TDbXDSzxpHsuInFfDq32YEVVA939INyQ_C1dXn5pqJ4,6907
+htc/models/image/DatasetImage.py,sha256=-OKGT5YPH6uI8JSjnMva5khzckVO27_yw0iz5BttKEo,3890
+htc/models/image/DatasetImageBatch.py,sha256=j-T9z7KjmWNv3BUCgkAf8eFnDFCgBljSc_Qf5m_bkRA,4473
+htc/models/image/DatasetImageStream.py,sha256=GxzZ_B5cZaoAEoteOCvoq_sRI-cHk4Fe4DkB1vkJLxs,3958
+htc/models/image/LightningImage.py,sha256=oiz7EfbHTC-bRU1FSPI7d9vr5phWPxuGkmfNL9qH40c,9816
+htc/models/image/ModelImage.py,sha256=MvdMEeX88DdtErduZg9V06xf89YzU_GRK03Cc3C_F6Y,1287
 htc/models/image/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/image/configs/default.json,sha256=u9-g9LHTEm-M_2XZkFXLRBzcTaVT1xZBlP8IvttVSsk,1779
+htc/models/image/configs/default.json,sha256=_vR2Tc6hMsL0RlwPNv4nyTCthMOyE6GSi6UMsRVFC-A,1771
 htc/models/image/configs/default_parameters.json,sha256=bscNlwoI4PUPtc5EnFKZHm1egHYB-GrVdzYkNOGpdfQ,128
 htc/models/image/configs/default_rgb.json,sha256=_hWUhGznOXmc_zG447cwS4EPibwvetxy4XpbvjwL2TQ,114
-htc/models/image/configs/spxs.json,sha256=cTBNkMKZORrd41CkeH1khtd9a3czbdmC-DyHzzprSP0,304
-htc/models/patch/DatasetPatchImage.py,sha256=90tiSGLANOp1l24kxMV0nFytPdVNVXeiNR5XSooZClU,3771
-htc/models/patch/DatasetPatchStream.py,sha256=9lhHeJdw_DxcgJMPphDfYCraHlmlWz9R6eaRRvbteHU,10815
-htc/models/patch/LightningPatch.py,sha256=8qva157aBuAOuUAPK4Nkhmu6pvwPGR6sUdnSKzcmkW8,2050
+htc/models/image/configs/spxs.json,sha256=t_j9MshKWEhB6iOvefsTCojJMAuGfl_HduFOvAAcqKo,361
+htc/models/patch/DatasetPatchImage.py,sha256=nOexvtykVn_9mZWSC8mHx2myF-4sEAbGFyrEGoHMOWw,3703
+htc/models/patch/DatasetPatchStream.py,sha256=R8p4F5AaAEKqd2WAWAJn7jsKDVCZXVjY-2GJuGoTfVw,11357
+htc/models/patch/LightningPatch.py,sha256=aJtS2LyM6oYP-GRyh97SExc_exNn4Bq0zWqGtPqMusw,1802
 htc/models/patch/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/patch/configs/default.json,sha256=4zv4bjbA3bsU9haJhmRpT52f839voxqqkR-ZG66muH4,1938
-htc/models/patch/configs/default_64.json,sha256=IaSCgBcqXbSRb8PKcS7UeUCN65l0MFSPo2CYrDCcAjs,1937
+htc/models/patch/configs/default.json,sha256=qGFGOSc2RMueq8lbmuWku01jr7uP57ZXnHmj0XkiHUE,1930
+htc/models/patch/configs/default_64.json,sha256=yFml7KiERmYo11ugETnyYlKzIUQFzkZjT3isSO6GqA4,1929
 htc/models/patch/configs/default_64_parameters.json,sha256=g19PiFV4AcruGhhJiHBd3G5GRp7wPoE4ocMecLdzMCQ,131
 htc/models/patch/configs/default_64_rgb.json,sha256=2UHFLElzU95pHFx5Ls_EecqvEU1TAtN8vR83s4prorg,117
 htc/models/patch/configs/default_parameters.json,sha256=bscNlwoI4PUPtc5EnFKZHm1egHYB-GrVdzYkNOGpdfQ,128
 htc/models/patch/configs/default_rgb.json,sha256=_hWUhGznOXmc_zG447cwS4EPibwvetxy4XpbvjwL2TQ,114
-htc/models/pixel/DatasetPixelStream.py,sha256=PS2a-Wi0nGOLnJdYCO-KFva2CHQZhoW1TAWjaJQGq4k,4941
-htc/models/pixel/LightningPixel.py,sha256=5tB1FNblN4PQ5d6PDcwsNx1Nm_710uo_dpbruf0YkXQ,3891
+htc/models/pixel/DatasetPixelStream.py,sha256=VZ7qDeQKXLIEF22iUwY1VpLBqTQ9YkGtMXCPadfi8Qs,5686
+htc/models/pixel/LightningPixel.py,sha256=-8vdXcgodPxuShwEnyF2jx0hQyyoqgdzEGzA7qdJduM,4264
 htc/models/pixel/ModelPixel.py,sha256=nL5aMD9S_t7uJF4aYtwra5kQ-GaoJ5WJN-RY-s-s4Os,4573
 htc/models/pixel/ModelPixelRGB.py,sha256=8t3Ir08YebKFluck_LtF5CCMXcQrF00fW_EhnaInw0A,2100
 htc/models/pixel/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/pixel/configs/default.json,sha256=zd6tQDK6_6tzhvjAR7vLFIhvAfct2L1ITBa1TwCz1sA,1785
+htc/models/pixel/configs/default.json,sha256=iRS_eMQRf6cnEarI1wsSELUIahUVxoRFykleTZNH2uI,1777
 htc/models/pixel/configs/default_parameters.json,sha256=6ppLZgSCl6WulaoxOSKMLujOA5RIotlpWVdQHORdIgw,191
 htc/models/pixel/configs/default_rgb.json,sha256=mU0UJu-y86Wwe9eUIJweFQdmAf7oZAXepFeckfTvPXQ,177
-htc/models/superpixel_classification/DatasetSuperpixelImage.py,sha256=sZeY0gitcAt5BjwYwCb2Lb4u0rIATIfC3qmG1vB_kq4,2615
-htc/models/superpixel_classification/DatasetSuperpixelStream.py,sha256=bgmU6qy4c15wKUdpnUdySKu8LPioZ_dxm3eGxQ_JhaM,4098
-htc/models/superpixel_classification/LightningSuperpixelClassification.py,sha256=2hbFlbH_3AkeXk_vm7_SAkcWOg9fT1ghaMYvnpw61a4,3493
+htc/models/superpixel_classification/DatasetSuperpixelImage.py,sha256=8hvpNSe98Lhhzd4AUf8MGDM7rrUVLXfLL3VYsl399KE,2486
+htc/models/superpixel_classification/DatasetSuperpixelStream.py,sha256=MjG6N3aiWtZf3QOPKbvKwX-1TLyrdZ2qEUTxIQ97JNQ,4060
+htc/models/superpixel_classification/LightningSuperpixelClassification.py,sha256=cKpa8MfRzVddLXzPpu1-Pqb2GoVtQK84zrmsblRu_wY,3583
 htc/models/superpixel_classification/ModelSuperpixelClassification.py,sha256=NTJt9X7jELUbxcqvWzosrIxYMiB3hSvDIgWL_gvEE9c,1886
 htc/models/superpixel_classification/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/models/superpixel_classification/configs/default.json,sha256=oUwlYNszS4BVGvmr-CZxlLm-G_fEmOg2vPv3I6glxQc,1966
+htc/models/superpixel_classification/configs/default.json,sha256=8cVe_OzjGeVWmLip9QAUiljiwTkByvnECpozxp3LfkQ,2015
 htc/models/superpixel_classification/configs/default_parameters.json,sha256=bscNlwoI4PUPtc5EnFKZHm1egHYB-GrVdzYkNOGpdfQ,128
 htc/models/superpixel_classification/configs/default_rgb.json,sha256=_hWUhGznOXmc_zG447cwS4EPibwvetxy4XpbvjwL2TQ,114
 htc/rater_variability/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/rater_variability/rater_evaluation.py,sha256=Iuj2acwwcBkGhR7omHinJ7Du06qmFll9EDJ4wZIverA,4492
 htc/rater_variability/run_nsd_thresholds.py,sha256=Nudg8yhKRclTXJhwy4U3qg1dyasGkczS7uHk4KU5WWM,3331
 htc/tissue_atlas/MetricAggregationClassification.py,sha256=30TI5uwXdUh41pp2TA8iA6PShxhRvRdHVXYhw447bTY,2574
 htc/tissue_atlas/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/tissue_atlas/run_test_table_generation.py,sha256=tLHuGxfj0nS3K6hvk0Am0xodeO9a0EGNNsDApR2jOQs,2487
+htc/tissue_atlas/run_test_table_generation.py,sha256=zVKPvohnCSTTc85uvVp1fwoNUbSdXj3DMM1g36a0UyQ,2474
 htc/tissue_atlas/settings_atlas.py,sha256=LnBLbX0cTcaFeAttAATouMcCdZSkKGOG9lIktMXn6dA,5564
 htc/tissue_atlas/tables.py,sha256=ZtTfUDKB7onp9npxI82slitRqCV_bZCjhN19EqzO1WY,2176
 htc/tissue_atlas/data/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/tissue_atlas/data/run_tissue_atlas_dataset.py,sha256=ILp-gF616ldMmpz9ZjVDZy4snxtnHORjP1YWzz2Yn5Q,3377
 htc/tissue_atlas/data/tissue-atlas_loocv_test-8_seed-0_cam-118.json,sha256=9kX-0Po6sAeTtte4DmDsMSegVwrvkhoEKz50_yusq3E,15656801
 htc/tissue_atlas/median_pixel/DatasetMedianPixel.py,sha256=UEBpp8M4tuIXiRcQSvNuAN50tS04Kkhx4VyUEIjTIkI,2419
-htc/tissue_atlas/median_pixel/LightningMedianPixel.py,sha256=x4S_wy579q0vUyRV5SU_jBH91mlkIaPg4cgN-t_hpf4,6207
+htc/tissue_atlas/median_pixel/LightningMedianPixel.py,sha256=a1uYK8v8Q9HuI54Y4Zh0cgxLAVsGYQbLNu_loqySvHA,5174
 htc/tissue_atlas/median_pixel/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
-htc/tissue_atlas/median_pixel/configs/default.json,sha256=uoDloy834wGo8n67uFIqPJAf49rD5Xy9BKcwRtpTbdw,1252
+htc/tissue_atlas/median_pixel/configs/default.json,sha256=ev_iX1-VngWhReRi9QOAyuw6NNxoYfqCgOsmgSB7Oy8,1244
 htc/tissue_atlas_open/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/tissue_atlas_open/profiles.py,sha256=NAGH9weWwUM3Auc80QwHDNqH3Pw_P28ZbMStoXqXV2A,10646
 htc/tissue_atlas_open/run_label_profiles.py,sha256=hLLJm7uQaR9mwOJQosXbdLaAJICqz53S05z_ih9nZuw,11507
-htc/tissue_atlas_open/run_readme_gif.py,sha256=Ml-ye3pAnGktjMdXI3jA12KVzj6UktlEQ05O_wOHa0E,3290
-htc/tivita/DataPath.py,sha256=2UpwNdSrh5SrWZS7AWEw0Hv0oxRAbR7HMhvXfQpKVgk,55747
-htc/tivita/DataPathMultiorgan.py,sha256=cHR3PN5WVgbt_TY5vbuRGkTB5AYeg9eQ5C6_F_Ik8pM,3867
-htc/tivita/DataPathReference.py,sha256=IgSjQFaxIQVinrgdS6cPxzxplDnIQ2IPiiRq9Cy5-ZM,5372
-htc/tivita/DataPathSepsis.py,sha256=FObMEnDARRw2FA0ttNpjsLGKP-Z_7OSitw2JlVJiE5Q,6036
-htc/tivita/DataPathTivita.py,sha256=qkq24GqyDB-4cPHmC2eFUhOBcfgKfy-eyCcY0bpqhHU,2280
-htc/tivita/DatasetSettings.py,sha256=AnfC2GW67r2Qk7zHIvfB6JMClbUWMq4uortGYUcsZyc,7026
+htc/tissue_atlas_open/run_readme_gif.py,sha256=peOR3MR01yboaoKbPYqCWnHSNro_BYnIOaLNuA2qA5s,4208
+htc/tivita/DataPath.py,sha256=Oyw1rG1QEWQAKfDcJXo6MsJ4PUExDrqpk75FphNts-Q,50469
+htc/tivita/DataPathMultiorgan.py,sha256=V3Y7mp95slLwuApw5krtLcuJxCzcVSD6ttXr-ScR5Ys,3872
+htc/tivita/DataPathReference.py,sha256=XJ12eSRy2xpflMP3J9Va1MTMcSLDzhNsaPi0FCkQTS0,4493
+htc/tivita/DataPathSepsis.py,sha256=MdnU8N0gfY4mNRFb0FkCIlMdrk-zhadRTHene8NoUCA,6043
+htc/tivita/DataPathTivita.py,sha256=qzv1T7gNCD_9YOq0yFYf2trzqiT2_tVRbJo9ov4ChM0,2028
+htc/tivita/DatasetSettings.py,sha256=RmDTsu6HlD8A6u4OBanOVg0czp-IAz8JVk9kC1dNQTc,5874
 htc/tivita/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/tivita/colorscale.py,sha256=g_pA_9jhX5Db68Hwn27h_Twh2Wt0s3nqsjRuXXw0SGU,1288
-htc/tivita/hsi.py,sha256=XDIvEhk3enB-EhW3W2wJe5l_omaF-QPaXKOV5KCcRko,3536
-htc/tivita/metadata.py,sha256=4s1YvTuBxLbHQsZX2A5zdisohn7KKWvJRxqWrgfunLU,5550
-htc/tivita/rgb.py,sha256=q8AB33wiW3zAERNQVQh5qUyfPOi80R3q3cIMPzFcZhc,4033
+htc/tivita/hsi.py,sha256=6ET5jeJrrzlVkpxLHb8rRafqBo91MQ0AC359capjphA,3403
+htc/tivita/metadata.py,sha256=ipVkAMEv4iwCTLRr8t8McGmu-z2XIGyjqzJjP9NKs5I,5529
+htc/tivita/rgb.py,sha256=CSmDiHF_3P8qX3q-emg65ZSe6SqN8PFJ9KnxLUv6rXQ,4057
 htc/utils/AdvancedJSONEncoder.py,sha256=rD-ueCTIHBujlQU6K6oLpb9JBdD0Q01AfFGEo4MjO50,1634
-htc/utils/ColorcheckerReader.py,sha256=k_Fw7KytPy8H06jSp2f8ZdNZvZkQ3PiA0yc87BP2uLA,14564
+htc/utils/ColorcheckerReader.py,sha256=Ua2MEh0Z3IJzZ7Pd2cWZIjN_Cx4oRw9mqEbhGfXqo-M,18217
 htc/utils/ColoredFileLog.py,sha256=Q56nRGvu-wZFjYv1rlkPPRcT-chbd3ifrSDt8XTr0Kk,2357
-htc/utils/Config.py,sha256=RAkNpCKpC0No98jhN5ZayttZ3vX0qW3eRZPQyIIIZYs,19181
-htc/utils/Datasets.py,sha256=VLqSETbChq0an_dVC_EifzxMvVVmZxGNOkuDMdNXOEY,19046
+htc/utils/Config.py,sha256=9T5DFf5mGlHHTSRXrP-B1fBqhplvg8p7DtRF1HOP7f4,17475
+htc/utils/DatasetDir.py,sha256=BuUXfF5Fg8dY6j1CDNsPNMhAnD6xAmxVgphZ0AXCEOY,18628
 htc/utils/DelayedFileHandler.py,sha256=wnE3w3pqPh2BpkXHPe84dbp0iW7MBL2lkqh392oYE-Q,2266
-htc/utils/DomainMapper.py,sha256=Yqp65ItERa2HCacvX1ioWpE9e-KD7IWAP7DCgAf0Y7I,7615
+htc/utils/DomainMapper.py,sha256=ioTRV3RvSeTfdEMosXcK4o2T9o9_DazbnM_EhgIUYcI,7620
 htc/utils/DuplicateFilter.py,sha256=MbSknXpWBRA720QjJxqsAqF5VIxKWPEyCpdy6Qx4Vjs,466
 htc/utils/LDA.py,sha256=M_9aF99BIIpFDlboA6r0QgdAzgTajzxc3_0AniinS_w,3549
-htc/utils/LabelMapping.py,sha256=IrhqarDohj1MS3NkIY-zL3auuiIseF9EPbV-6HrpW48,16950
-htc/utils/MeasureTime.py,sha256=kmmTZkDF4miItF2ton9-zGbvlkPmDTVV7TOUrJq74Bw,1238
-htc/utils/MultiPath.py,sha256=bIWVM-v8TCMdK64BZI44TMyvkO-Ag6uIQNm4sYmCe3g,16469
-htc/utils/SLICWrapper.py,sha256=BfcESFW7cQSp8FYqi-j0x9rJihFfowSKyvsL9M5R0lc,4984
-htc/utils/SpectrometerReader.py,sha256=cCv1CJcQcNr7m67nNZEkLat_ixiGdMRB9qrnSFohmEU,8575
+htc/utils/LabelMapping.py,sha256=GNbC5GYQ6I3rrCnRq8aAwWUwKf9dTkeimRWowKGu66M,15529
+htc/utils/MeasureTime.py,sha256=G6Z_1UsvM5SyqzaS5qDnoIztom1RMGmhAx2C2eqvmMc,1240
+htc/utils/MultiPath.py,sha256=0QYbnkstgpMHUBiSt5WbGjsHtOwFemh8mwQOMOcGxDU,15488
+htc/utils/SpectrometerReader.py,sha256=E8LByLmQi09F9GmE4nWMGkBH0IsrGeCF-UDX4FHAEVc,7161
 htc/utils/__init__.py,sha256=dQz6cE4oHPqwsQWFi4jiJEeyVoOGA_UboU0fJK2F6no,112
 htc/utils/blosc_compression.py,sha256=SH08riMAeJJgyXh7cc6ZVGdvoJrCmf_u52y62gbmRVU,3236
-htc/utils/colors.py,sha256=668az59alp2zCTnZCM153D_F6o_vqBotZjxnVXfk-0s,3623
+htc/utils/colors.py,sha256=jx4T2bxKnot5Xmxcu6oxl4kEy6-KHcEZTzev7tXrApg,3716
 htc/utils/file_transfer.py,sha256=IQUA0ZM22Iaz1179QdPQ3bVL1SC18tYvADR9jN9GVAY,3165
 htc/utils/general.py,sha256=4TBNZIirpd_7EGHgrX4-kfh4XaNg0Wo4mX6e61OOdUA,6338
-htc/utils/helper_functions.py,sha256=w14c0yibYc5TcJJ52bx3ePt8ASEhgI8GIMmRTxQjx2M,34672
-htc/utils/import_extra.py,sha256=SysTRAI0KvD1eQ6LNN3kcIXsl7Yp31rrG6aKVZVcoJ4,1748
-htc/utils/parallel.py,sha256=GBYlKe5rq4GZveVqMNAj4Q7nM3jx-gf9CW7wKYIfcLE,4385
-htc/utils/paths.py,sha256=ohD0hEwfY4Syh7tzzFABKcw6XoWA8vy0wJsdBzcq_BY,5323
+htc/utils/helper_functions.py,sha256=BIzd3KDEnZJcuPwhuB8zLF33CmROgeF_KVNcIHSs5wE,25274
+htc/utils/import_extra.py,sha256=kYTN2ihaHlsL0RxQOvxerF_9zcO57RhdehUFlOBxmxc,1611
+htc/utils/parallel.py,sha256=WBYorRqv1I3f8BySyurCwJ_FKfRlgMGolkmwyCmdGII,3808
+htc/utils/paths.py,sha256=qD5lJpmkNAR8cPNd0YB2_-5TP4o-CqWak_jrze1Ub0c,5328
 htc/utils/renderjson.js,sha256=M3sltRqqBOYx1n0Q2xAcW4LNd29nILAjV-_ASbKp2mQ,9879
 htc/utils/run_system_monitor.py,sha256=HeOqa4HAuzVdkBTHthunZdN_DVLif6AGNHzcUnLFYvo,2644
 htc/utils/sqldf.py,sha256=p6d-hTcEJXn8k466N_p-jN4QMJtEAdl8eZSMJE_KYYs,8301
 htc/utils/type_from_string.py,sha256=GJ6Bsc5oe6P3Z3MX3AmD3uKR2eYC5d7NgssySlm76ZM,2170
-htc/utils/unify_path.py,sha256=-36M-q4PZZN-tj7bgUWd3w9Dmd5pXeLb0WQEFYnSuCQ,1373
-htc/utils/visualization.py,sha256=xm1oAeZiZ5VE8pQHZVVK0HvaZ1dKnmHbvGK2UFgOOco,89202
-imsy_htc-0.0.11.dist-info/LICENSE.md,sha256=xSL9p3BpbT-tLGTCI8cZGq7wN_-8-n2wV_kawQQW1-U,211
-imsy_htc-0.0.11.dist-info/METADATA,sha256=-a7ViSULaaui4l1U4JqdMIKBYsUkrHLGbflWy4B1CJY,32517
-imsy_htc-0.0.11.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-imsy_htc-0.0.11.dist-info/entry_points.txt,sha256=VoW2Vkmsg3ZvFWzIcywo7O8ZuNylWF3SchaKdrFSZEM,37
-imsy_htc-0.0.11.dist-info/top_level.txt,sha256=tTbVDYKGGS8RKZfhYVGMlDomwgky3gtEVg48bLH_o2k,4
-imsy_htc-0.0.11.dist-info/RECORD,,
+htc/utils/unify_path.py,sha256=oLxwvrF0AvKagEaLfB4kgk4jh3mFiT54Sx4YgPjpB-g,960
+htc/utils/visualization.py,sha256=MGJeCqbDdn8tF3-w8YegxE7hiz_mCh7ZaNHrrwLBp8E,78852
+imsy_htc-0.0.9.dist-info/LICENSE.md,sha256=xSL9p3BpbT-tLGTCI8cZGq7wN_-8-n2wV_kawQQW1-U,211
+imsy_htc-0.0.9.dist-info/METADATA,sha256=oadxgwcKIS-yyzQC5_QAkPNSIMxG-WjklSgZ2tGcbcE,30900
+imsy_htc-0.0.9.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
+imsy_htc-0.0.9.dist-info/entry_points.txt,sha256=VoW2Vkmsg3ZvFWzIcywo7O8ZuNylWF3SchaKdrFSZEM,37
+imsy_htc-0.0.9.dist-info/top_level.txt,sha256=tTbVDYKGGS8RKZfhYVGMlDomwgky3gtEVg48bLH_o2k,4
+imsy_htc-0.0.9.dist-info/RECORD,,
```

