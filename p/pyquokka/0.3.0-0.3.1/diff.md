# Comparing `tmp/pyquokka-0.3.0-py3-none-any.whl.zip` & `tmp/pyquokka-0.3.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,50 @@
-Zip file size: 292216 bytes, number of entries: 40
--rw-rw-r--  2.0 unx      309 b- defN 23-May-11 13:24 pyquokka/__init__.py
+Zip file size: 312156 bytes, number of entries: 48
+-rw-rw-r--  2.0 unx      309 b- defN 23-Jun-27 05:10 pyquokka/__init__.py
 -rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-19 05:39 pyquokka/catalog.py
--rw-rw-r--  2.0 unx      372 b- defN 23-Apr-12 19:11 pyquokka/common_startup.sh
+-rw-rw-r--  2.0 unx      372 b- defN 23-Jun-06 17:55 pyquokka/common_startup.sh
 -rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-16 23:17 pyquokka/coordinator.py
--rw-rw-r--  2.0 unx    48407 b- defN 23-May-11 12:30 pyquokka/core.py
--rw-rw-r--  2.0 unx    49168 b- defN 23-May-11 12:22 pyquokka/dataset.py
--rw-rw-r--  2.0 unx    98957 b- defN 23-May-11 12:50 pyquokka/datastream.py
+-rw-rw-r--  2.0 unx    48433 b- defN 23-Jun-22 19:44 pyquokka/core.py
+-rw-rw-r--  2.0 unx    51863 b- defN 23-Jun-14 21:42 pyquokka/dataset.py
+-rw-rw-r--  2.0 unx   101812 b- defN 23-May-16 03:36 pyquokka/datastream.py
 -rw-rw-r--  2.0 unx     1438 b- defN 22-Nov-11 18:24 pyquokka/debugger.py
--rw-rw-r--  2.0 unx    81396 b- defN 23-May-11 13:14 pyquokka/df.py
+-rw-rw-r--  2.0 unx    84360 b- defN 23-Jun-27 15:11 pyquokka/df.py
+-rw-rw-r--  2.0 unx      230 b- defN 23-Jun-01 06:12 pyquokka/disk_setup.sh
 -rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-16 23:19 pyquokka/executors.py
 -rw-rw-r--  2.0 unx    12657 b- defN 23-Mar-28 22:05 pyquokka/expression.py
 -rw-rw-r--  2.0 unx    17483 b- defN 23-Mar-26 21:54 pyquokka/flight.py
 -rw-rw-r--  2.0 unx     3098 b- defN 23-May-11 02:45 pyquokka/hbq.py
 -rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-11 05:32 pyquokka/ldb.so
 -rw-rw-r--  2.0 unx      274 b- defN 23-Mar-25 18:43 pyquokka/leader_start_ray.sh
 -rw-rw-r--  2.0 unx      619 b- defN 23-Mar-25 18:43 pyquokka/leader_startup.sh
--rw-rw-r--  2.0 unx    30992 b- defN 23-May-11 07:12 pyquokka/logical.py
--rw-rw-r--  2.0 unx     9100 b- defN 23-May-11 02:45 pyquokka/orderedstream.py
--rw-rw-r--  2.0 unx      886 b- defN 23-Mar-15 22:44 pyquokka/placement_strategy.py
+-rw-rw-r--  2.0 unx    31410 b- defN 23-Jun-14 20:46 pyquokka/logical.py
+-rw-rw-r--  2.0 unx     8530 b- defN 23-Jun-27 06:14 pyquokka/orderedstream.py
+-rw-rw-r--  2.0 unx     1009 b- defN 23-May-18 17:31 pyquokka/placement_strategy.py
 -rw-rw-r--  2.0 unx     3717 b- defN 23-Mar-29 00:48 pyquokka/quokka_dataset.py
--rw-rw-r--  2.0 unx    19735 b- defN 23-May-11 02:45 pyquokka/quokka_runtime.py
+-rw-rw-r--  2.0 unx    20581 b- defN 23-Jun-05 23:12 pyquokka/quokka_runtime.py
 -rw-rw-r--  2.0 unx    93718 b- defN 22-Oct-04 03:50 pyquokka/redis.conf
--rw-rw-r--  2.0 unx    10625 b- defN 23-May-11 02:45 pyquokka/sql.py
+-rw-rw-r--  2.0 unx    15525 b- defN 23-Jun-02 03:36 pyquokka/sql.py
 -rw-rw-r--  2.0 unx    16471 b- defN 23-Mar-27 22:47 pyquokka/sql_utils.py
 -rw-rw-r--  2.0 unx     2371 b- defN 22-Jul-15 20:59 pyquokka/state.py
 -rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-09 03:05 pyquokka/tables.py
 -rw-rw-r--  2.0 unx     2351 b- defN 23-Jan-28 04:17 pyquokka/target_info.py
 -rw-rw-r--  2.0 unx     5752 b- defN 23-Mar-20 21:30 pyquokka/task.py
--rw-rw-r--  2.0 unx    39151 b- defN 23-May-11 02:45 pyquokka/utils.py
+-rw-rw-r--  2.0 unx    40544 b- defN 23-Jun-27 00:26 pyquokka/utils.py
 -rw-rw-r--  2.0 unx     4081 b- defN 23-Jan-25 17:17 pyquokka/windowtypes.py
--rw-rw-r--  2.0 unx      146 b- defN 23-May-11 02:45 pyquokka/executors/__init__.py
+-rw-rw-r--  2.0 unx     1231 b- defN 23-Jun-22 05:22 pyquokka/dataset/__init__.py
+-rw-rw-r--  2.0 unx      652 b- defN 23-Jun-22 05:25 pyquokka/dataset/base_dataset.py
+-rw-rw-r--  2.0 unx     3553 b- defN 23-Jun-22 18:00 pyquokka/dataset/crypto_dataset.py
+-rw-rw-r--  2.0 unx    11209 b- defN 23-Jun-29 03:12 pyquokka/dataset/ordered_readers.py
+-rw-rw-r--  2.0 unx    36345 b- defN 23-Jun-22 18:00 pyquokka/dataset/unordered_readers.py
+-rw-rw-r--  2.0 unx      192 b- defN 23-Jun-27 05:11 pyquokka/executors/__init__.py
 -rw-rw-r--  2.0 unx      811 b- defN 23-May-11 02:45 pyquokka/executors/base_executor.py
--rw-rw-r--  2.0 unx    17620 b- defN 23-May-11 02:45 pyquokka/executors/sql_executors.py
+-rw-rw-r--  2.0 unx     8198 b- defN 23-Jun-27 06:13 pyquokka/executors/cep_executors.py
+-rw-rw-r--  2.0 unx    20679 b- defN 23-Jun-12 19:32 pyquokka/executors/sql_executors.py
 -rw-rw-r--  2.0 unx    17185 b- defN 23-May-11 02:45 pyquokka/executors/ts_executors.py
--rw-rw-r--  2.0 unx     4503 b- defN 23-May-11 06:14 pyquokka/executors/vector_executors.py
--rw-rw-r--  2.0 unx    11357 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx      999 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx        9 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3188 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/RECORD
-40 files, 1059103 bytes uncompressed, 287244 bytes compressed:  72.9%
+-rw-rw-r--  2.0 unx     5209 b- defN 23-May-19 06:44 pyquokka/executors/vector_executors.py
+-rw-rw-r--  2.0 unx    11357 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx      971 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       53 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3908 b- defN 23-Jun-29 06:48 pyquokka-0.3.1.dist-info/RECORD
+48 files, 1140727 bytes uncompressed, 306048 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -21,14 +21,17 @@
 
 Filename: pyquokka/debugger.py
 Comment: 
 
 Filename: pyquokka/df.py
 Comment: 
 
+Filename: pyquokka/disk_setup.sh
+Comment: 
+
 Filename: pyquokka/executors.py
 Comment: 
 
 Filename: pyquokka/expression.py
 Comment: 
 
 Filename: pyquokka/flight.py
@@ -84,38 +87,59 @@
 
 Filename: pyquokka/utils.py
 Comment: 
 
 Filename: pyquokka/windowtypes.py
 Comment: 
 
+Filename: pyquokka/dataset/__init__.py
+Comment: 
+
+Filename: pyquokka/dataset/base_dataset.py
+Comment: 
+
+Filename: pyquokka/dataset/crypto_dataset.py
+Comment: 
+
+Filename: pyquokka/dataset/ordered_readers.py
+Comment: 
+
+Filename: pyquokka/dataset/unordered_readers.py
+Comment: 
+
 Filename: pyquokka/executors/__init__.py
 Comment: 
 
 Filename: pyquokka/executors/base_executor.py
 Comment: 
 
+Filename: pyquokka/executors/cep_executors.py
+Comment: 
+
 Filename: pyquokka/executors/sql_executors.py
 Comment: 
 
 Filename: pyquokka/executors/ts_executors.py
 Comment: 
 
 Filename: pyquokka/executors/vector_executors.py
 Comment: 
 
-Filename: pyquokka-0.3.0.dist-info/LICENSE
+Filename: pyquokka-0.3.1.dist-info/LICENSE
+Comment: 
+
+Filename: pyquokka-0.3.1.dist-info/METADATA
 Comment: 
 
-Filename: pyquokka-0.3.0.dist-info/METADATA
+Filename: pyquokka-0.3.1.dist-info/WHEEL
 Comment: 
 
-Filename: pyquokka-0.3.0.dist-info/WHEEL
+Filename: pyquokka-0.3.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyquokka-0.3.0.dist-info/top_level.txt
+Filename: pyquokka-0.3.1.dist-info/top_level.txt
 Comment: 
 
-Filename: pyquokka-0.3.0.dist-info/RECORD
+Filename: pyquokka-0.3.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyquokka/common_startup.sh

```diff
@@ -1,11 +1,11 @@
 sudo apt-get update
 sudo apt-get install -y python3-pip
 sudo apt-get install -y curl
 sudo apt-get install -y python3.8-dev
 sudo apt-get install -y unzip
-sudo apt install -y nvme-cli
-curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
-unzip -o  awscliv2.zip
-sudo ./aws/install
-pip3 install --upgrade awscli
+sudo apt-get install -y nvme-cli
+# curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
+# unzip -o  awscliv2.zip
+# sudo ./aws/install
+pip3 install awscli
 sudo apt-get install -y python-cffi
```

## pyquokka/core.py

```diff
@@ -578,15 +578,15 @@
                             # print(name, len(chunk))
                             assert format == "polars"
                             if len(names) == 0 or name != names[-1]:
                                 chunks_list.append([chunk])
                                 names.append(name)
                             else:
                                 chunks_list[-1].append(chunk)
-                                print("creating multi-chunk")
+                                # print("creating multi-chunk")
 
                         except StopIteration:
                             break
                     # print("=============")
 
                     batches = []
                     source_actor_ids = set()
@@ -653,42 +653,42 @@
                     out_seq = self.process_output(actor_id, channel_id, output, transaction, state_seq, out_seq)
                     if out_seq == -1:
                         # this aborts the transaction automatically
                         continue
                     
                     # print("next task reqs", [new_input_reqs] + input_requirements[1:])
                     next_task = ExecutorTask(actor_id, channel_id, state_seq + 1, out_seq, [new_input_reqs] + input_requirements[1:])
+                    lineage = pickle.dumps((source_actor_id, source_channel_seqs))
+                    for source_channel_id in source_channel_seqs:
+                        max_seq = max(source_channel_seqs[source_channel_id])
+                        self.EWT.set(transaction, pickle.dumps((source_actor_id, source_channel_id)), max_seq)
 
                 else:
                     output = self.function_objects[actor_id, channel_id].done(channel_id)
 
                     out_seq = self.process_output(actor_id, channel_id, output, transaction, state_seq, out_seq)
                     if out_seq == -1:
                         continue
                     last_output_seq = out_seq - 1
                     # print("DONE", actor_id, channel_id)
                     self.DST.set(transaction, pickle.dumps((actor_id, channel_id)), last_output_seq)
-                            
                     next_task = None
+                    lineage = None
 
                 if self.configs["checkpoint_interval"] is not None and state_seq % self.configs["checkpoint_interval"] == 0:
                     self.function_objects[actor_id, channel_id].checkpoint(self.checkpoint_bucket, actor_id, channel_id, state_seq)
                     for source_channel_id in source_channel_ids:
                         for seq in source_channel_seqs[source_channel_id]:
                             self.CT.sadd(transaction, pickle.dumps((source_actor_id, source_channel_id, seq)), pickle.dumps((actor_id, channel_id)))
 
                     self.LCT.rpush(transaction, pickle.dumps((actor_id, channel_id)), pickle.dumps((state_seq, out_seq)))
                     self.IRT.set(transaction, pickle.dumps((actor_id, channel_id, state_seq)), pickle.dumps([new_input_reqs] + input_requirements[1:]))
                 # this way of logging the lineage probably use less space than a Polars table actually.                        
 
                 self.EST.set(transaction, pickle.dumps((actor_id, channel_id)), state_seq)                    
-                lineage = pickle.dumps((source_actor_id, source_channel_seqs))
-                for source_channel_id in source_channel_seqs:
-                    max_seq = max(source_channel_seqs[source_channel_id])
-                    self.EWT.set(transaction, pickle.dumps((source_actor_id, source_channel_id)), max_seq)
                 self.state_commit(transaction, actor_id, channel_id, state_seq, lineage)
                 self.task_commit(transaction, candidate_task, next_task)
                 
                 executed = transaction.execute()
                 #if not all(executed):
                 #    raise Exception(executed)
                 
@@ -739,15 +739,15 @@
                             name, format = pickle.loads(metadata)
                             assert format == "polars"
                             if len(names) == 0 or name != names[-1]:
                                 chunks_list.append([chunk])
                                 names.append(name)
                             else:
                                 chunks_list[-1].append(chunk)
-                                print("creating multi-chunk")
+                                # print("creating multi-chunk")
 
                         except StopIteration:
                             break
 
                     # we are going to assume the Flight server gives us results sorted by source_actor_id
                     batches = chunks_list
                     input_names = names
```

## pyquokka/dataset.py

```diff
@@ -155,27 +155,30 @@
         if columns is not None:
             assert type(columns) == list, "columns must be a list of strings"
         self.columns = columns
         if filters is not None:
             assert type(filters) == str, "sql predicate supported"
         self.filters = filters
 
+        self.workers = 4
+        self.executor = None
+
         self.probe_df = None
         self.probe_df_col = None
         self.k = None
 
         assert type(uris) == list
         self.uris = uris
         self.vec_column = vec_column
 
-        for uri in self.uris:
-            dataset = lance.dataset(uri)
-            if not dataset.has_index:
-                print("Warning: dataset {} does not have an index. Expect slow performance and maybe crashes.".format(uri))
-            assert self.vec_column in dataset.schema.names, "vector column not found in schema"
+        # for uri in self.uris:
+        #     dataset = lance.dataset(uri)
+        #     if not dataset.has_index:
+        #         print("Warning: dataset {} does not have an index. Expect slow performance and maybe crashes.".format(uri))
+        #     assert self.vec_column in dataset.schema.names, "vector column not found in schema"
         
     def set_probe_df(self, probe_df, probe_df_col, k):
 
         assert probe_df is not None and probe_df_col is not None and k is not None, "must provide probe_df, probe_df_col, and k"
         
         assert type(probe_df) == polars.DataFrame
         self.probe_df = probe_df
@@ -183,55 +186,96 @@
         self.probe_df_col = probe_df_col
         self.probe_df_vecs = np.stack(self.probe_df[self.probe_df_col].to_numpy())
         assert type(k) == int and k >= 1
         self.k = k
 
     def get_own_state(self, num_channels):
 
-        # split the urls across the channels
-        channel_info = {}
+        self.num_channels = num_channels
+        channel_infos = {}
         for channel in range(num_channels):
-            channel_info[channel] = self.uris[channel::num_channels]
-        print(channel_info)
-        return channel_info
+            my_files = [self.uris[k] for k in range(channel, len(self.uris), self.num_channels)]
+            channel_infos[channel] = []
+            for pos in range(0, len(my_files), self.workers):
+                channel_infos[channel].append( my_files[pos : pos + self.workers])
+        return channel_infos
     
-    def execute(self, mapper_id, uri):
+    def execute(self, mapper_id, uris):
+
+        print("FILTERS", self.filters)
 
         import lance
-        dataset = lance.dataset(uri)
-        if self.k is not None:
-            results = pa.concat_tables( [dataset.to_table(columns = self.columns, filter = self.filters, 
-                                nearest={"column": self.vec_column, "k": self.k, "q": self.probe_df_vecs[i]}) for i in range(len(self.probe_df_vecs))]) 
-        else:
-            results = dataset.to_table(columns = self.columns, filter = self.filters)
+
+        def download(uri):
+            dataset = lance.dataset(uri)
+            if self.k is not None:
+                results = pa.concat_tables( [dataset.to_table(columns = self.columns, filter = self.filters, 
+                                    nearest={"column": self.vec_column, "k": self.k, "q": self.probe_df_vecs[i]}) for i in range(len(self.probe_df_vecs))]) 
+            else:
+                results = dataset.to_table(columns = self.columns, filter = self.filters)
+            return results
+
+        # dirty hack to set env varibles
+
+        if self.executor is None:
+            import os
+            # lines = open("/home/ubuntu/.aws/credentials").readlines()
+            # aws_secret_access_key = lines[1].split("=")[1].strip()
+            # aws_access_key_id = lines[2].split("=")[1].strip()
+
+            # os.environ["AWS_SECRET_ACCESS_KEY"] = aws_secret_access_key
+            # os.environ["AWS_ACCESS_KEY_ID"] = aws_access_key_id
+
+            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
+
+        if len(uris) == 0:
+            return None, None
         
-        return None, results
+        # this will return things out of order, but that's ok!
 
+        future_to_url = {self.executor.submit(download, uri): uri for uri in uris}
+        dfs = []
+        for future in concurrent.futures.as_completed(future_to_url):
+            dfs.append(future.result())
+        
+        result = pa.concat_tables(dfs)
+        if "score" in result.column_names:
+            result = result.drop(["score"])
+        
+        print("LANCE RETURN", [len(df) for df in dfs])
+        
+        return None, result
+        
 class InputEC2ParquetDataset:
 
     # filter pushdown could be profitable in the future, especially when you can skip entire Parquet files
     # but when you can't it seems like you still read in the entire thing anyways
     # might as well do the filtering at the Pandas step. Also you need to map filters to the DNF form of tuples, which could be
     # an interesting project in itself. Time for an intern?
 
-    def __init__(self, files = None, columns=None, filters=None) -> None:
+    def __init__(self, files = None, columns=None, filters=None, workers = 4, name_column = None) -> None:
 
         self.files = files
 
         self.num_channels = None
         self.columns = columns
         self.filters = filters
 
         self.length = 0
-        self.workers = 4
+        self.workers = workers
 
         self.s3 = None
         self.iterator = None
         self.count = 0
 
+        # the uniqueness constraint of name_column is deferred to higher levels of the stack
+        self.name_column = name_column
+        if self.name_column is not None and self.columns is not None and self.name_column in self.columns:
+            self.columns.remove(self.name_column)
+
     def get_own_state(self, num_channels):
         self.num_channels = num_channels
         self.files = [i.replace("s3://", "") for i in self.files]
         channel_infos = {}
         for channel in range(num_channels):
             my_files = [self.files[k] for k in range(channel, len(self.files), self.num_channels)]
             channel_infos[channel] = []
@@ -245,18 +289,18 @@
         if self.s3 is None:
             self.s3 = S3FileSystem()
             self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
 
         def download(file):
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
-                if self.columns is not None:
-                    return pq.read_table( file, columns=self.columns, filters=self.filters, use_threads= True, filesystem = self.s3)
-                else:
-                    return pq.read_table( file, filters=self.filters, use_threads= True, filesystem = self.s3)
+                result = pq.read_table( file, columns=self.columns, filters=self.filters, use_threads= True, filesystem = self.s3)
+                if self.name_column is not None:
+                    result = result.append_column(self.name_column, pa.array([file.split("/")[-1].replace(".parquet","")] * len(result)))
+                return result
 
         assert self.num_channels is not None
 
         if files_to_do is None:
             raise Exception("dynamic lineage for inputs not supported anymore")
 
         if len(files_to_do) == 0:
@@ -270,33 +314,36 @@
             dfs.append(future.result())
         
         return None, pa.concat_tables(dfs)
 
 
 class InputSortedEC2ParquetDataset:
 
-    def __init__(self, files, partitioner, columns=None, filters=None, mode = "stride") -> None:
+    def __init__(self, files, partitioner, columns=None, filters=None, mode = "stride", workers = 4, name_column = None) -> None:
 
         self.files = files
         self.partitioner = partitioner
 
         self.num_channels = None
         self.columns = columns
         self.filters = filters
 
         self.length = 0
-        self.workers = 1
+        self.workers = workers
 
         self.s3 = None
         self.iterator = None
         self.count = 0
         self.bounds = None
 
         assert mode in ["stride", "range"]
         self.mode = mode
+        self.name_column = name_column
+        if self.name_column is not None and self.columns is not None and self.name_column in self.columns:
+            self.columns.remove(self.name_column)
 
     def _get_bounds(self, num_channels):
         
         channel_infos = {}
         fragments = []
         self.num_channels = num_channels
         s3fs = S3FileSystem()
@@ -366,15 +413,18 @@
         if self.s3 is None:
             self.s3 = S3FileSystem()
             self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
 
         def download(file):
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
-                return pq.read_table(file, columns=self.columns, filters=self.filters, use_threads= False, filesystem = self.s3)
+                result = pq.read_table( file, columns=self.columns, filters=self.filters, use_threads= True, filesystem = self.s3)
+                if self.name_column is not None:
+                    result = result.append_column(self.name_column, pa.array([file.split("/")[-1].replace(".parquet","")] * len(result)))
+                return result
 
         assert self.num_channels is not None
 
         if files_to_do is None:
             raise Exception("dynamic lineage for inputs not supported anymore")
 
         if len(files_to_do) == 0:
@@ -389,15 +439,15 @@
         
         sorted_dfs = sorted(dfs, key = lambda x: x[1])
         
         return None, pa.concat_tables([k[0] for k in sorted_dfs])
 
 class InputEC2CoPartitionedSortedParquetDataset:
 
-    def __init__(self, bucket, prefix, partitioner, columns=None, filters=None) -> None:
+    def __init__(self, bucket, prefix, partitioner, columns=None, filters=None, name_column = None) -> None:
 
         self.bucket = bucket
         self.prefix = prefix
         self.partitioner = partitioner
         assert self.prefix is not None
 
         self.num_channels = None
@@ -407,14 +457,17 @@
         self.length = 0
         self.workers = 4
 
         self.s3 = None
         self.iterator = None
         self.count = 0
         self.bounds = None
+        self.name_column = name_column
+        if self.name_column is not None and self.columns is not None and self.name_column in self.columns:
+            self.columns.remove(self.name_column)
 
     def get_bounds(self, num_channels, channel_bounds):
 
         def overlap(a, b):
             return max(-1, min(a[1], b[1]) - max(a[0], b[0]))
         
         channel_infos = {channel: [] for channel in channel_bounds}
@@ -471,16 +524,20 @@
         if self.s3 is None:
             self.s3 = S3FileSystem()
             self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
 
         def download(file):
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
-                return pq.read_table(file, columns=self.columns, filters=self.filters, use_threads= False, use_legacy_dataset = True, filesystem = self.s3)
+                result = pq.read_table( file, columns=self.columns, filters=self.filters, use_threads= True, filesystem = self.s3)
+                if self.name_column is not None:
+                    result = result.append_column(self.name_column, pa.array([file.split("/")[-1].replace(".parquet","")] * len(result)))
 
+                return result
+            
         assert self.num_channels is not None
 
         if files_to_do is None:
             raise Exception("dynamic lineage for inputs not supported anymore")
 
         if len(files_to_do) == 0:
             return None, None
```

## pyquokka/datastream.py

```diff
@@ -389,19 +389,31 @@
             transformed = self.transform(f, new_schema = self.schema, required_columns=self.schema)
             return transformed
         else:
             return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=FilterNode(self.schema, predicate),
                                               schema=self.schema, sorted = self.sorted)
 
 
-    def nn_probe(self, probe_df, vec_column = None, vec_column_left = None, vec_column_right = None, k = 1, suffix = "_probe"):
+    def vector_nn_join(self, probe_df, vec_column = None, vec_column_left = None, vec_column_right = None, k = 1, suffix = "_probe", probe_side = "right", tmp_directory = None):
 
         """
-        This will perform a nearest neighbor join between two Quokka
-        
+        Probe a Quokka DataStream with a vector column by a Polars DataFrame with a vector column.
+        For each vector in the Polars DataFrame it will find the k nearest neighbors in the vector column of the Quokka DataStream.
+        This is a blocking operation. The result length will be len(probe_df) * k.
+
+        Args:
+            probe_df (polars.DataFrame): The Polars DataFrame to probe with.
+            vec_column (str): The name of the vector column in both the Quokka DataStream and the Polars DataFrame.
+            vec_column_left (str): The name of the vector column in the Quokka DataStream.
+            vec_column_right (str): The name of the vector column in the Polars DataFrame.
+            k (int): The number of nearest neighbors to find.
+            suffix (str): The suffix to append to the column names of the result.
+
+        Return:
+            A DataStream consisting of the nearest neighbors of each vector in the Polars DataFrame.
         """
 
         if vec_column is not None:
             assert vec_column_left is None and vec_column_right is None, "cannot specify both vec_column and vec_column_left/vec_column_right"
             vec_column_left = vec_column
             vec_column_right = vec_column
         else:
@@ -409,14 +421,18 @@
         
         assert k >= 1, "k must be at least 1"
         assert type(probe_df) == polars.DataFrame, "probe_df must be a polars DataFrame"
 
         assert vec_column_left in self.schema, "Vector column not in schema"
         assert vec_column_right in probe_df.schema, "Probe column not in schema"
 
+        assert probe_side in ["left", "right"], "probe_side must be either 'left' or 'right'"
+        if probe_side == "left": 
+            assert tmp_directory is not None, "tmp_directory must be specified if probe_side is 'left'"
+
         # rename any column in probe_df that is also in self.schema
 
         probe_vec_col = vec_column_right
         new_schema = self.schema
         schema_mapping = {col: {0: col} for col in self.schema}
         for col_name in probe_df.columns:                
             if col_name in self.schema:
@@ -426,20 +442,46 @@
                     probe_vec_col = col_name + suffix
                 new_schema.append(col_name + suffix)
                 schema_mapping[col_name + suffix] = {-1: col_name}
             else:
                 new_schema.append(col_name)
                 schema_mapping[col_name] = {-1: col_name}
 
-        node = NearestNeighborFilterNode(new_schema, schema_mapping, vec_column_left, probe_df, probe_vec_col,  k)
+        if probe_side == "right":
+            node = NearestNeighborFilterNode(new_schema, schema_mapping, vec_column_left, probe_df, probe_vec_col,  k)
+
+            return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=node,
+                                                schema=new_schema, sorted = self.sorted)
+        else:
+            # you are probing a small polars dataframe with a datastream, this will typically make the datastream bigger. 
+            # so don't push down, can't push down into lance anyway.
+            import lance
+            class ProbeExecutor(Executor):
+                def __init__(self) -> None:
+                    try:
+                        self.dataset = lance.write_dataset(probe_df.to_arrow(), tmp_directory)
+                        self.dataset.create_index(vec_column_right, index_type = "IVF_PQ", num_partitions=256, num_sub_vectors=16)
+                    except:
+                        raise Exception("Failed to write dataset to tmp directory, is it writeable?")
+                def execute(self,batches,stream_id, executor_id):
+                    batch = pa.concat_tables(batches)
+                    probe_vecs = batch.column(probe_vec_col).to_numpy()
+                    results = pa.concat_tables([self.dataset.to_table(nearest={"column": vec_column_right, "k": k, "q": probe_vecs[i]})
+                                                 for i in range(len(probe_vecs))]) 
+                    return pa.Table.from_arrays(batch.columns + results.columns, names=batch.column_names + results.column_names)
+                def done(self,executor_id):
+                    return
+                
+            executor = ProbeExecutor()
+            return self.stateful_transform(executor, new_schema = new_schema, required_columns = set(vec_column_left), 
+                                           partitioner = PassThroughPartitioner(), placement_strategy= CustomChannelsStrategy(1))
 
-        return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=node,
-                                              schema=new_schema, sorted = self.sorted)
+            
 
-    def _ann_join(self, vec_column = None, vec_column_left = None, vec_column_right = None, probe_side = "left", k = 1):
+    def vector_range_join(self, vec_column = None, vec_column_left = None, vec_column_right = None, probe_side = "left", k = 1):
 
         """
         This will perform a nearest neighbor join between two Quokka DataStreams.
         The plan is to convert this to a NearestNeighborFilterNode after you propagate cardinality and figure out which side is smaller.
         
         """
```

## pyquokka/df.py

```diff
@@ -58,15 +58,15 @@
             >>> manager = QuokkaClusterManager(key_name = "my_key", key_location = "/home/ubuntu/.ssh/my_key.pem", security_group = "my_security_group")
             >>> cluster = manager.from_json("my_cluster.json")
 
         """
 
         self.sql_config= {"optimize_joins" : True, "s3_csv_materialize_threshold" : 10 * 1048576, "disk_csv_materialize_threshold" : 1048576,
                       "s3_parquet_materialize_threshold" : 10 * 1048576, "disk_parquet_materialize_threshold" : 1048576}
-        self.exec_config = {"hbq_path": "/data/", "fault_tolerance": False, "memory_limit": 0.25, "max_pipeline_batches": 30, 
+        self.exec_config = {"hbq_path": "/data/", "fault_tolerance": False, "memory_limit": 0.1, "max_pipeline_batches": 30, 
                         "checkpoint_interval": None, "checkpoint_bucket": "quokka-checkpoint", "batch_attempt": 20, "max_pipeline": 3}
 
         self.latest_node_id = 0
         self.nodes = {}
         self.cluster = LocalCluster() if cluster is None else cluster
         if type(self.cluster) == LocalCluster:
             if self.exec_config["fault_tolerance"]:
@@ -82,45 +82,53 @@
         self.task_managers = {}
         self.node_locs= {}
         self.io_nodes = set()
         self.compute_nodes = set()
         self.replay_nodes = set()
         count = 0
 
+        self.tag_io_nodes = {k: set() for k in self.cluster.tags}
+        self.tag_compute_nodes = {k: set() for k in self.cluster.tags}
+
         self.leader_compute_nodes = []
         self.leader_io_nodes = []
 
         # default strategy launches two IO nodes and one compute node per machine
         private_ips = list(self.cluster.private_ips.values())
         for ip in private_ips:
             
             for k in range(1):
-                self.task_managers[count] = ReplayTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_private_ip, list(self.cluster.private_ips.values()), self.exec_config)
+                self.task_managers[count] = ReplayTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_public_ip, list(self.cluster.private_ips.values()), self.exec_config)
                 self.replay_nodes.add(count)
                 self.node_locs[count] = ip
                 count += 1
             for k in range(io_per_node):
-                self.task_managers[count] = IOTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_private_ip, list(self.cluster.private_ips.values()), self.exec_config)
+                self.task_managers[count] = IOTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_public_ip, list(self.cluster.private_ips.values()), self.exec_config)
                 self.io_nodes.add(count)
-                self.node_locs[count] = ip
-                count += 1
-
                 if ip == self.cluster.leader_private_ip:
                     self.leader_io_nodes.append(count)
+                for tag in self.cluster.tags:
+                    if ip in self.cluster.tags[tag]:
+                        self.tag_io_nodes[tag].add(count)
+                self.node_locs[count] = ip
+                count += 1
 
             for k in range(exec_per_node):
                 if type(self.cluster) == LocalCluster:
-                    self.task_managers[count] = ExecTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_private_ip, list(self.cluster.private_ips.values()), self.exec_config)
+                    self.task_managers[count] = ExecTaskManager.options(num_cpus = 0.001,  max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_public_ip, list(self.cluster.private_ips.values()), self.exec_config)
                 elif type(self.cluster) == EC2Cluster:
-                    self.task_managers[count] = ExecTaskManager.options(num_cpus = 0.001, max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_private_ip, list(self.cluster.private_ips.values()), self.exec_config) 
+                    self.task_managers[count] = ExecTaskManager.options(num_cpus = 0.001,  max_concurrency = 2, resources={"node:" + ip : 0.001}).remote(count, self.cluster.leader_public_ip, list(self.cluster.private_ips.values()), self.exec_config) 
                 else:
                     raise Exception
 
                 if ip == self.cluster.leader_private_ip:
                     self.leader_compute_nodes.append(count)
+                for tag in self.cluster.tags:
+                    if ip in self.cluster.tags[tag]:
+                        self.tag_compute_nodes[tag].add(count)
                 
                 self.compute_nodes.add(count)
                 self.node_locs[count] = ip
                 count += 1        
 
         ray.get(self.coordinator.register_nodes.remote(replay_nodes = {k: self.task_managers[k] for k in self.replay_nodes}, io_nodes = {k: self.task_managers[k] for k in self.io_nodes}, compute_nodes = {k: self.task_managers[k] for k in self.compute_nodes}))
         ray.get(self.coordinator.register_node_ips.remote( self.node_locs ))
@@ -398,22 +406,25 @@
                 token = ray.get(self.catalog.register_disk_csv_source.remote(table_location, schema, sep))
                 self.nodes[self.latest_node_id] = InputDiskCSVNode(table_location, schema, sep, has_header)
                 self.nodes[self.latest_node_id].set_catalog_id(token)
             
         self.latest_node_id += 1
         return DataStream(self, schema, self.latest_node_id - 1)
 
-    def read_parquet(self, table_location: str):
+    def read_parquet(self, table_location: str, nthreads = 4, name_column = None):
 
         """
         Read Parquet. It can be a single Parquet or a list of Parquets. It can be Parquet(s) on disk
         or Parquet(s) on S3. Currently other clouds are not supported. 
 
         Args:
             table_location (str): where the Parquet(s) are. This mostly mimics Spark behavior. Look at the examples.
+            nthreads (int): number of threads to use when reading Parquet(s). Default is 4.
+            name_column (str): this adds the option to add a column to the DataStream that represents the filename the row came from.
+                this is useful if each file is a category and you didn't also store the category name in the file.
 
         Return:
             DataStream.
         
         Examples:
             
             Read a single Parquet. It's better always to specify the absolute path.
@@ -458,48 +469,64 @@
                     files.extend([bucket + "/" + i['Key'] for i in z['Contents']
                                     if i['Key'].endswith(".parquet")])
                     sizes.extend([i['Size'] for i in z['Contents'] if i['Key'].endswith('.parquet')])
 
                 assert len(files) > 0, "could not find any parquet files. make sure they end with .parquet"
                 if sum(sizes) < self.sql_config["s3_parquet_materialize_threshold"] and len(files) == 1:
                     df = polars.from_arrow(pq.read_table(files[0], filesystem = S3FileSystem()))
-                    return return_materialized_stream(df)
+                    if name_column is not None:
+                        assert name_column not in df.columns, "name_column already exists in Parquet columns"
+                        return return_materialized_stream(df.with_columns(polars.lit(files[0].split("/")[-1].replace(".parquet",""))).alias(name_column))
+                    else:
+                        return return_materialized_stream(df)
                 
                 try:
                     f = pq.ParquetFile(S3FileSystem().open_input_file(files[0]))
                     schema = [k.name for k in f.schema_arrow]
                 except:
                     raise Exception("schema discovery failed for Parquet dataset at location {}. Please raise Github issue.".format(table_location))
                 
+                if name_column is not None:
+                    assert name_column not in schema, "name_column already exists in Parquet columns"
+                    schema.append(name_column)
                 token = ray.get(self.catalog.register_s3_parquet_source.remote(files[0], len(sizes)))
-                self.nodes[self.latest_node_id] = InputS3ParquetNode(files, schema)
+                self.nodes[self.latest_node_id] = InputS3ParquetNode(files, schema, nthreads = nthreads, name_column = name_column)
                 self.nodes[self.latest_node_id].set_catalog_id(token)
             else:
                 try:
                     f = pq.ParquetFile(S3FileSystem().open_input_file(table_location))
                     schema = [k.name for k in f.schema_arrow]
                 except:
                     raise Exception("""schema discovery failed for Parquet dataset at location {}. 
                                     Note if you are specifying a prefix to many parquet files, must use asterix. E.g.
                                     qc.read_parquet("s3://rottnest/happy.parquet/*")""".format(table_location))
                 key = "/".join(table_location.split("/")[1:])
                 response = s3.head_object(Bucket= bucket, Key=key)
                 size = response['ContentLength']
                 if size < self.sql_config["s3_parquet_materialize_threshold"]:
                     df = polars.from_arrow(pq.read_table(table_location, filesystem = S3FileSystem()))
-                    return return_materialized_stream(df)
+                    if name_column is not None:
+                        assert name_column not in df.columns, "name_column already exists in Parquet columns"
+                        return return_materialized_stream(df.with_columns(polars.lit(key.split("/")[-1].replace(".parquet",""))).alias(name_column))
+                    else:
+                        return return_materialized_stream(df)
                 
                 token = ray.get(self.catalog.register_s3_parquet_source.remote(bucket + "/" + key, 1))
-                self.nodes[self.latest_node_id] = InputS3ParquetNode([table_location], schema)
+                if name_column is not None:
+                    assert name_column not in schema, "name_column already exists in Parquet columns"
+                    schema.append(name_column)
+                self.nodes[self.latest_node_id] = InputS3ParquetNode([table_location], schema, nthreads = nthreads, name_column = name_column)
                 self.nodes[self.latest_node_id].set_catalog_id(token)
 
             # self.nodes[self.latest_node_id].set_placement_strategy(CustomChannelsStrategy(2))
         else:
             if type(self.cluster) == EC2Cluster:
                 raise NotImplementedError("Does not support reading local dataset with S3 cluster. Must use S3 bucket.")
+            if name_column is not None:
+                raise NotImplementedError("Does not support name_column for local dataset yet.")
             
             if "*" in table_location:
                 table_location = table_location[:-1]
                 assert table_location[-1] == "/", "must specify * with entire directory, doesn't support prefixes yet"
                 try:
                     files = [i for i in os.listdir(table_location) if i.endswith(".parquet")]
                 except:
@@ -554,25 +581,22 @@
             table_location = table_location[5:]
             bucket = table_location.split("/")[0]
             if "*" in table_location:
                 assert "*" not in table_location[:-1], "wildcard can only be the last character in address string"
                 table_location = table_location[:-1]
                 prefix = "/".join(table_location[:-1].split("/")[1:])
 
-                z = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
-                if 'Contents' not in z:
-                    raise Exception("Wrong S3 path")
-                files = [bucket + "/" + i['Key'] for i in z['Contents'] if i['Key'].endswith(".lance")]
+                z = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter = "/")
+                files = ["s3://" + bucket + "/" + i['Prefix'] for i in z['CommonPrefixes'] if i['Prefix'].endswith(".lance/")]
                 while 'NextContinuationToken' in z.keys():
                     z = s3.list_objects_v2(
-                        Bucket=bucket, Prefix=prefix, ContinuationToken=z['NextContinuationToken'])
-                    files.extend([bucket + "/" + i['Key'] for i in z['Contents']
-                                    if i['Key'].endswith(".parquet")])
+                        Bucket=bucket, Prefix=prefix, Delimiter = "/", ContinuationToken=z['NextContinuationToken'])
+                    files.extend(["s3://" + bucket + "/" + i['Prefix'] for i in z['CommonPrefixes'] if i['Prefix'].endswith(".lance/")])
 
-                assert len(files) > 0, "could not find any parquet files. make sure they end with .parquet"
+                assert len(files) > 0, "could not find any lance files. make sure they end with .lance"
                 
                 try:
                     schema = lance.dataset(files[0]).schema.names
                     assert vec_column in schema, "vector column not found in schema"
                 except:
                     raise Exception("""schema discovery failed for Parquet dataset at location {}. Please raise Github issue.
                                     Lance dataset with S3 does not work with creds set with AWS configure, must use env variables.""".format(table_location))
@@ -759,19 +783,20 @@
         
         """
 
         self.nodes[self.latest_node_id] = InputPolarsNode(polars.from_arrow(df))
         self.latest_node_id += 1
         return DataStream(self, df.columns, self.latest_node_id - 1, materialized=True)
 
-    def read_sorted_parquet(self, table_location: str, sorted_by: str, schema = None):
+    def read_sorted_parquet(self, table_location: str, sorted_by: str,  nthreads = 4, sort_order = "stride", name_col = None):
         assert type(sorted_by) == str
-        stream = self.read_parquet(table_location)
+        assert sort_order in ["stride", "range"]
+        stream = self.read_parquet(table_location, nthreads = nthreads, name_column=name_col)
         assert sorted_by in stream.schema, f"sorted_by column {sorted_by} not in schema {stream.schema}"
-        return OrderedStream(stream, {sorted_by : "stride"})
+        return OrderedStream(stream, {sorted_by : sort_order})
     
     def read_sorted_csv(self, table_location: str,sorted_by: str, schema = None, has_header = False, sep=","):
         assert type(sorted_by) == str
         stream = self.read_csv(table_location, schema, has_header, sep)
         return OrderedStream(stream, {sorted_by : "stride"})
 
     def read_iceberg(self, table, snapshot = None):
@@ -842,16 +867,18 @@
             source_datastream = sources[source]
             node.parents[source] = source_datastream.source_node_id
             parent = self.nodes[source_datastream.source_node_id]
             parent.targets[self.latest_node_id] = TargetInfo(
                 partitioners[source], sqlglot.exp.TRUE, None, [])
 
         self.latest_node_id += 1
-
-        return DataStream(self, schema, self.latest_node_id - 1, sorted, materialized)
+        if sorted is None:
+            return DataStream(self, schema, self.latest_node_id - 1, sorted, materialized)
+        else:
+            return OrderedStream(DataStream(self, schema, self.latest_node_id - 1, sorted, materialized), sorted)
 
     '''
     This defines a dataset object which is used by the optimizer. 
     '''
     def new_dataset(self, source, schema: list):
         stream = self.new_stream(sources={0: source}, partitioners={
                                  0: PassThroughPartitioner()}, node=DataSetNode(schema), schema=schema)
@@ -1248,15 +1275,15 @@
             assert len(node.parents) == 1 and len(targets) != 0
             # figure out which source node is the parent by walking the graph. currently the vector must come from a source node. It cannot be generated by something.
             
             curr_node_id = node.parents[0]
             curr_col_name = node.vec_column
             curr_target_id = node_id
             while True:
-                if issubclass(type(self.execution_nodes[curr_node_id]), SourceNode):
+                if issubclass(type(self.execution_nodes[curr_node_id]), SourceNode) or issubclass(type(self.execution_nodes[curr_node_id]), FilterNode):
                     break
                 else:
                     schema_mapping = self.execution_nodes[curr_node_id].schema_mapping
                     interested = schema_mapping[curr_col_name]
                     assert len(interested) == 1, "vector column must only originate from one source currently, i.e. unions don't work. Please raise Github issue."
                     my_parent = list(interested.keys())[0]
                     if my_parent == -1:
@@ -1276,14 +1303,24 @@
                 # delete yourself
                 parent = self.execution_nodes[node.parents[0]]
                 for target_id in targets:
                     parent.targets[target_id] = targets[target_id]
                 del parent.targets[node_id]
                 del self.execution_nodes[node_id]
 
+                # find yourself in your targets' parents and replace yourself with your parent
+                for target_id in targets:
+                    success = False
+                    for key in self.execution_nodes[target_id].parents:
+                        if self.execution_nodes[target_id].parents[key] == node_id:
+                            self.execution_nodes[target_id].parents[key] = node.parents[0]
+                            success = True
+                            break
+                    assert success
+
             # otherwise you need to maintain yourself
             else:
 
                 # first remove yourself from the current location in the graph, i.e. wire up your parent to your targets.
                 # parent = self.execution_nodes[node.parents[0]]
                 # for target_id in targets:
                 #     parent.targets[target_id] = targets[target_id]
```

## pyquokka/logical.py

```diff
@@ -288,15 +288,15 @@
 
         assert probe_df is not None and probe_df_col is not None and k is not None, "must provide probe_df, probe_df_col, and k"
         
         assert type(probe_df) == polars.DataFrame
         self.probe_df = probe_df
         assert type(probe_df_col) == str and probe_df_col in probe_df.columns, "probe_df_col must be a string and in probe_df"
         self.probe_df_col = probe_df_col
-        self.probe_df_vecs = np.stack(self.probe_df[self.probe_df_col].to_numpy())
+        # self.probe_df_vecs = np.stack(self.probe_df[self.probe_df_col].to_numpy())
         assert type(k) == int and k >= 1
         self.k = k
 
     def set_cardinality(self, catalog):
         
         # assert self.catalog_id is not None
         for target in self.targets:
@@ -304,30 +304,32 @@
     
     def lower(self, task_graph):
         lance_reader = InputLanceDataset(self.uris, self.vec_column, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate)
         if self.probe_df is not None:
             lance_reader.set_probe_df(self.probe_df, self.probe_df_col, self.k)
         node = task_graph.new_input_reader_node(lance_reader, self.stage, self.placement_strategy)
         if self.probe_df is not None:
-            operator = DFProbeDataStreamNNExecutor2(self.vec_col, self.probe_df_vecs, self.probe_df_col, self.k)
+            operator = DFProbeDataStreamNNExecutor2(self.vec_column, self.probe_df, self.probe_df_col, self.k)
             target_info = TargetInfo(BroadcastPartitioner(), sqlglot.exp.TRUE, None, [])
             node = task_graph.new_non_blocking_node({0: node}, 
                         operator, self.stage, SingleChannelStrategy(), 
                         source_target_info={0: target_info})
 
         return node
         
 
 class InputS3ParquetNode(SourceNode):
-    def __init__(self, files, schema, predicate = None, projection = None) -> None:
+    def __init__(self, files, schema, predicate = None, projection = None, nthreads = 4, name_column = None) -> None:
         super().__init__(schema)
         
         self.files = files
         self.predicate = predicate
         self.projection = projection
+        self.nthreads = nthreads
+        self.name_column = name_column
 
     def set_cardinality(self, catalog):
 
         assert self.catalog_id is not None
         for target in self.targets:
             predicate = sql_utils.label_sample_table_names(self.targets[target].predicate)
             self.cardinality[target] = ray.get(catalog.estimate_cardinality.remote(self.catalog_id, predicate, self.predicate))
@@ -335,17 +337,17 @@
     
     def lower(self, task_graph):
 
         if self.output_sorted_reqs is not None:
             assert len(self.output_sorted_reqs) == 1
             key = list(self.output_sorted_reqs.keys())[0]
             val = self.output_sorted_reqs[key]
-            parquet_reader = InputSortedEC2ParquetDataset(self.files, key, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate, mode=val)
+            parquet_reader = InputSortedEC2ParquetDataset(self.files, key, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate, mode=val, workers = self.nthreads, name_column=self.name_column)
         else:
-            parquet_reader = InputEC2ParquetDataset(self.files, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate)
+            parquet_reader = InputEC2ParquetDataset(self.files, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate, workers = self.nthreads, name_column=self.name_column)
         node = task_graph.new_input_reader_node(parquet_reader, self.stage, self.placement_strategy)
         return node
     
     def __str__(self):
         result = str(type(self)) + '\nPredicate: ' + str(self.predicate) + '\nProjection: ' + str(self.projection) + '\nTargets:' 
         for target in self.targets:
             result += "\n\t" + str(target) + " " + str(self.targets[target])
@@ -456,14 +458,15 @@
 
         join_type, join_info = self.join_specs[0]
         assert len(join_info) == 2
         [(left_parent, left_key), (right_parent, right_key)] = join_info
 
         key_to_keep = "left" if left_key in self.schema else "right"
         operator = BuildProbeJoinExecutor(None, left_key, right_key, join_type, key_to_keep)
+        # operator = DiskBuildProbeJoinExecutor(None, left_key, right_key, join_type, key_to_keep)
 
         left_parent_target_info = parent_source_info[left_parent]
         right_parent_target_info = parent_source_info[right_parent]
         left_parent_target_info.partitioner = HashPartitioner(left_key)
         right_parent_target_info.partitioner = HashPartitioner(right_key)
 
         if len(self.join_specs) == 1 and self.blocking:
@@ -479,14 +482,15 @@
             join_type, join_info = self.join_specs[i]
             assert len(join_info) == 2
             [(left_parent, left_key), (right_parent, right_key)] = join_info
 
             # left parent is always the probe. right parent is always the build
             key_to_keep = "left" if left_key in self.schema else "right"
             operator = BuildProbeJoinExecutor(None, left_key, right_key, join_type, key_to_keep)
+            # operator = DiskBuildProbeJoinExecutor(None, left_key, right_key, join_type, key_to_keep)
 
             assert  parent_nodes[left_parent] in joined_parents
             intermediate_target_info = TargetInfo(HashPartitioner(left_key), sqlglot.exp.TRUE, None, [])
             parent_target_info = parent_source_info[right_parent]
             parent_target_info.partitioner = HashPartitioner(right_key)
             # print("adding node", intermediate_node, parent_nodes[right], {0: str(intermediate_target_info), 1: str(parent_target_info)})
             if i == len(self.join_specs) - 1 and self.blocking:
```

## pyquokka/orderedstream.py

```diff
@@ -1,18 +1,18 @@
 from pyquokka.datastream import * 
 
 class OrderedStream(DataStream):
     def __init__(self, datastream, sorted_reqs) -> None:
         super().__init__(datastream.quokka_context, datastream.schema, datastream.source_node_id, sorted_reqs, datastream.materialized)
     
     def __str__(self):
-        return "OrderedStream[" + ",".join(self.schema) + "] order by {}".format(self.sorted_reqs.keys())
+        return "OrderedStream[" + ",".join(self.schema) + "] order by {}".format(self.sorted)
 
     def __repr__(self):
-        return "OrderedStream[" + ",".join(self.schema) + "] order by {}".format(self.sorted_reqs.keys())
+        return "OrderedStream[" + ",".join(self.schema) + "] order by {}".format(self.sorted)
 
     def shift(self, n, by = None, fill_value=None):
         """
         Shifts the elements of this stream by `n` positions, inserting `fill_value`
         for the new elements created at the beginning or end of the stream.
 
         :param n: the number of positions to shift the elements (positive or negative) positive means shift forward
@@ -30,118 +30,90 @@
                 required_columns={0: set(self.schema)},
                 operator=ShiftOperator(n, fill_value),
                 assume_sorted={0:True},
             ),
             schema=self.schema,
         )
     
-    def pattern_recognize(self, anchors, duration_limits, partition_by = None):
+    def pattern_recognize(self, time_col, events, maxspan, by):
+        
+        # we are going to use the cer method from the cep_utils module
+        if by is None:
+            raise Exception("You must specify a key by column for pattern_recognize currently.")
+        
+        assert time_col in self.sorted
+        assert by in self.schema
+
+        executor = CEPExecutor(events, maxspan, time_col, by)
+        if executor.prefilter != sqlglot.exp.FALSE:
+            filtered_stream = self.filter_sql(executor.prefilter)
+        else:
+            filtered_stream = self
+        
+        return filtered_stream.stateful_transform(executor, ["event_number", "first_event_timestamp", "last_event_timestamp"],
+                                                        required_columns = executor.touched_columns.union({time_col, by}), by = by, placement_strategy = CustomChannelsStrategy(1))
+
+    def stateful_transform(self, executor: Executor, new_schema: list, required_columns: set,
+                           by = None, placement_strategy = CustomChannelsStrategy(1)):
 
-        """
-        experimental API.
         """
 
-        query = "select "
-        curr_alias = 0
-        for anchor in anchors:
-            assert type(anchor) == Expression or type(anchor) == str
-            if type(anchor) == Expression:
-                anchor = anchor.sql()
-            columns = set(i.name for i in anchor.find_all(
-            sqlglot.expressions.Column))
-            for column in columns:
-                assert column in self.schema, "Tried to define an anchor using a column not in the schema {}".format(column)
-            query += anchor + " as __anchor_{}, ".format(curr_alias)
-            curr_alias += 1
-            assert "__anchor_{}".format(curr_alias) not in self.schema, "Column called __anchor_{} already exists in the schema".format(curr_alias)
-        query = query[:-2] + " from batch_arrow"
-
-        for duration_limit in duration_limits:
-            assert(type(duration_limit) == int or type(duration_limit) == float)
-
-        duration_buffer = sum(duration_limits)
-        assert len(duration_limits) == len(anchors) - 1
-
-        class CEPExecutor(Executor):
-            def __init__(self) -> None:
-                import ldbpy
-                self.state = None
-                self.cep = ldbpy.CEP(duration_limits)
-                self.con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
-                self.num_anchors = len(anchors)
-
-            def execute(self,batches,stream_id, executor_id):
-                from pyarrow.cffi import ffi
-                os.environ["OMP_NUM_THREADS"] = "8"
-         
-                arrow_batch = pa.concat_tables(batches)
-                # you can only process up to the duration_buffer, the rest needs to be cached
-                if self.state is not None:
-                    arrow_batch = pa.concat_tables([self.state, arrow_batch])
-                
-                if len(arrow_batch) > duration_buffer:
-                    self.state = arrow_batch[-duration_buffer:]
-                    arrow_batch = arrow_batch[: -duration_buffer]
-                else:
-                    self.state = arrow_batch
-                    return
-            
-                result = polars.from_arrow(self.con.execute(query).arrow())
-                array_ptrs = []
-                schema_ptrs = []
-                c_schemas = []
-                c_arrays = []
-                list_of_arrs = []
-                for anchor in range(self.num_anchors):
-                    index = result.select(polars.arg_where(polars.col("__anchor_{}".format(anchor))))
-                    list_of_arrs.append(index.to_arrow()["__anchor_{}".format(anchor)].combine_chunks())
-                    c_schema = ffi.new("struct ArrowSchema*")
-                    c_array = ffi.new("struct ArrowArray*")
-                    c_schemas.append(c_schema)
-                    c_arrays.append(c_array)
-                    schema_ptr = int(ffi.cast("uintptr_t", c_schema))
-                    array_ptr = int(ffi.cast("uintptr_t", c_array))
-                    list_of_arrs[-1]._export_to_c(array_ptr, schema_ptr)
-                    array_ptrs.append(array_ptr)
-                    schema_ptrs.append(schema_ptr)
-
-                result = self.cep.do_arrow_batch(array_ptrs, schema_ptrs)
-                del c_schemas
-                del c_arrays
-                # print("TIME", time.time() - start)
-                
-            def done(self,executor_id):
-                from pyarrow.cffi import ffi
-                if self.state is None:
-                    return 
-
-                arrow_batch = self.state
-                self.state = None
-                result = polars.from_arrow(self.con.execute(query).arrow())
-                array_ptrs = []
-                schema_ptrs = []
-                c_schemas = []
-                c_arrays = []
-                list_of_arrs = []
-                for anchor in range(self.num_anchors):
-                    index = result.select(polars.arg_where(polars.col("__anchor_{}".format(anchor))))
-                    list_of_arrs.append(index.to_arrow()["__anchor_{}".format(anchor)].combine_chunks())
-                    c_schema = ffi.new("struct ArrowSchema*")
-                    c_array = ffi.new("struct ArrowArray*")
-                    c_schemas.append(c_schema)
-                    c_arrays.append(c_array)
-                    schema_ptr = int(ffi.cast("uintptr_t", c_schema))
-                    array_ptr = int(ffi.cast("uintptr_t", c_array))
-                    list_of_arrs[-1]._export_to_c(array_ptr, schema_ptr)
-                    array_ptrs.append(array_ptr)
-                    schema_ptrs.append(schema_ptr)
+        **EXPERIMENTAL API** 
 
-                result = self.cep.do_arrow_batch(array_ptrs, schema_ptrs)
+        This is like `transform`, except you can use a stateful object as your transformation function. This is useful for example, if you want to run
+        a heavy Pytorch model on each batch coming in, and you don't want to reload this model for each function call. Remember the `transform` API only
+        supports stateless transformations. You could also implement much more complicated stateful transformations, like implementing your own aggregation
+        function if you are not satisfied with Quokka's default operator's performance.
+
+        This API is still being finalized. A version of it that takes multiple input streams is also going to be added. This is the part of the DataStream level 
+        api that is closest to the underlying execution engine. Quokka's underlying execution engine basically executes a series of stateful transformations
+        on batches of data. The difficulty here is how much of that underlying API to expose here so it's still useful without the user having to understand 
+        how the Quokka runtime works. To that end, we have to come up with suitable partitioner and placement strategy abstraction classes and interfaces.
+
+        If you are interested in helping us hammer out this API, please talke to me: zihengw@stanford.edu.
+
+        Args:
+            executor (pyquokka.executors.Executor): The stateful executor. It must be a subclass of `pyquokka.executors.Executor`, and expose the `execute` 
+                and `done` functions. More details forthcoming.
+            new_schema (list): The names of the columns of the Polars DataFrame that the transformation function produces. 
+            required_columns (list or set): The names of the columns that are required for this transformation. This argument is made mandatory
+                because it's often trivial to supply and can often greatly speed things up.
 
+        Return:
+            A transformed DataStream.
         
+        Examples:
+            Check the code for the `gramian` function.
+        """
+        if type(required_columns) == list:
+            required_columns = set(required_columns)
+        assert type(required_columns) == set
+        assert issubclass(type(executor), Executor), "user defined executor must be an instance of a \
+            child class of the Executor class defined in pyquokka.executors. You must override the execute and done methods."
+
+        select_stream = self.select(required_columns)
+
+        custom_node = StatefulNode(
+            schema=new_schema,
+            # cannot push through any predicates or projections!
+            schema_mapping={col: {-1: col} for col in new_schema},
+            required_columns={0: required_columns},
+            operator=executor,
+            assume_sorted={0:True}
+        )
+
+        custom_node.set_placement_strategy(placement_strategy)
+
+        return self.quokka_context.new_stream(
+            sources={0: select_stream},
+            partitioners={0: PassThroughPartitioner() if by is None else HashPartitioner(by)},
+            node=custom_node,
+            schema=new_schema,
+            
+        )
 
     def join_asof(self, right, on=None, left_on=None, right_on=None, by=None, left_by = None, right_by = None, suffix="_2"):
 
         assert type(right) == OrderedStream
         if on is not None:
             assert left_on is None and right_on is None
             left_on = on
```

## pyquokka/placement_strategy.py

```diff
@@ -23,13 +23,14 @@
 '''
 class DatasetStrategy(PlacementStrategy):
     def __init__(self, total_channels) -> None:
         super().__init__()
         self.total_channels = total_channels
 
 '''
-Launch only on GPU instances.
+Lance a custom umber of channels per node within a tag. 
 '''
-class GPUStrategy(PlacementStrategy):
-    def __init__(self) -> None:
+class TaggedCustomChannelsStrategy(PlacementStrategy):
+    def __init__(self, channels, tag) -> None:
         super().__init__()
-
+        self.channels_per_node = channels
+        self.tag = tag
```

## pyquokka/quokka_runtime.py

```diff
@@ -53,14 +53,16 @@
 
     def get_total_channels_from_placement_strategy(self, placement_strategy, node_type):
 
         if type(placement_strategy) == SingleChannelStrategy:
             return 1
         elif type(placement_strategy) == CustomChannelsStrategy:
             return self.context.cluster.num_node * placement_strategy.channels_per_node * (self.context.io_per_node if node_type == 'input' else self.context.exec_per_node)
+        elif type(placement_strategy) == TaggedCustomChannelsStrategy:
+            return len(self.context.cluster.tags[placement_strategy.tag]) * placement_strategy.channels_per_node * (self.context.io_per_node if node_type == 'input' else self.context.exec_per_node)
         elif type(placement_strategy) == DatasetStrategy:
             return placement_strategy.total_channels
         else:
             print(placement_strategy)
             raise Exception("strategy not supported")
 
     def epilogue(self, stage, placement_strategy):
@@ -116,15 +118,15 @@
     def new_input_reader_node(self, reader, stage = 0, placement_strategy = None):
 
         start = time.time()
         self.actor_types[self.current_actor] = 'input'
         if placement_strategy is None:
             placement_strategy = CustomChannelsStrategy(1)
         
-        assert type(placement_strategy) in [SingleChannelStrategy, CustomChannelsStrategy]
+        assert type(placement_strategy) in [SingleChannelStrategy, CustomChannelsStrategy, TaggedCustomChannelsStrategy]
         
         channel_info = reader.get_own_state(self.get_total_channels_from_placement_strategy(placement_strategy, 'input'))
         # print(channel_info)
         self.input_partitions[self.current_actor] = sum([len(channel_info[k]) for k in channel_info])
 
         pipe = self.r.pipeline()
         start = time.time()
@@ -140,17 +142,19 @@
             vals = {pickle.dumps((self.current_actor, count, seq)) : pickle.dumps(lineages[seq]) for seq in range(len(lineages))}
             input_task = TapedInputTask(self.current_actor, count, [i for i in range(len(lineages))])
             self.LIT.set(pipe, pickle.dumps((self.current_actor,count)), len(lineages) - 1)
             self.LT.mset(pipe, vals)
             self.NTT.rpush(pipe, node, input_task.reduce())
             channel_locs[count] = node
         
-        elif type(placement_strategy) == CustomChannelsStrategy:
+        elif type(placement_strategy) == CustomChannelsStrategy or type(placement_strategy) == TaggedCustomChannelsStrategy:
         
-            for node in sorted(self.context.io_nodes):
+            nodes = sorted(self.context.io_nodes) if type(placement_strategy) == CustomChannelsStrategy else sorted(self.context.tag_io_nodes[placement_strategy.tag])
+
+            for node in nodes:
                 for channel in range(placement_strategy.channels_per_node):
 
                     if count in channel_info:
                         lineages = channel_info[count]
                     else:
                         lineages = []
 
@@ -182,24 +186,24 @@
             # return entirety of data to a random channel between source_channel * ratio and source_channel * ratio + ratio
             target_channel = int(random.random() * ratio) + source_channel * ratio
             return {target_channel: data}
 
         source_placement_strategy = self.actor_placement_strategy[source_node_id]
         
 
-        if type(source_placement_strategy) == CustomChannelsStrategy:
+        if type(source_placement_strategy) == CustomChannelsStrategy or type(source_placement_strategy) == TaggedCustomChannelsStrategy:
             source_total_channels = self.get_total_channels_from_placement_strategy(source_placement_strategy, self.actor_types[source_node_id])
         elif type(source_placement_strategy) == DatasetStrategy:
             source_total_channels = source_placement_strategy.total_channels
         elif type(source_placement_strategy) == SingleChannelStrategy:
             source_total_channels = 1
         else:
             raise Exception("source strategy not supported")
 
-        if type(target_placement_strategy) == CustomChannelsStrategy:
+        if type(source_placement_strategy) == CustomChannelsStrategy or type(source_placement_strategy) == TaggedCustomChannelsStrategy:
             target_total_channels = self.get_total_channels_from_placement_strategy(target_placement_strategy, "exec")
         elif type(target_placement_strategy) == SingleChannelStrategy:
             target_total_channels = 1
         else:
             raise Exception("target strategy not supported")
         
         if source_total_channels >= target_total_channels:
@@ -338,25 +342,27 @@
             node = self.context.leader_compute_nodes[0]
             exec_task = ExecutorTask(self.current_actor, 0, 0, 0, input_reqs)
             channel_locs[0] = node
             self.NTT.rpush(pipe, node, exec_task.reduce())
             self.CLT.set(pipe, pickle.dumps((self.current_actor, 0)), self.context.node_locs[node])
             self.IRT.set(pipe, pickle.dumps((self.current_actor, 0, -1)), pickle.dumps(input_reqs))
 
-        elif type(placement_strategy) == CustomChannelsStrategy:
+        elif type(placement_strategy) == CustomChannelsStrategy or type(placement_strategy) == TaggedCustomChannelsStrategy:
 
+            nodes = sorted(self.context.compute_nodes) if type(placement_strategy) == CustomChannelsStrategy else sorted(self.context.tag_compute_nodes[placement_strategy.tag])
             count = 0
-            for node in sorted(self.context.compute_nodes):
+            for node in nodes:
                 for channel in range(placement_strategy.channels_per_node):
                     exec_task = ExecutorTask(self.current_actor, count, 0, 0, input_reqs)
                     channel_locs[count] = node
                     self.NTT.rpush(pipe, node, exec_task.reduce())
                     self.CLT.set(pipe, pickle.dumps((self.current_actor, count)), self.context.node_locs[node])
                     self.IRT.set(pipe, pickle.dumps((self.current_actor, count, -1)), pickle.dumps(input_reqs))
                     count += 1
+
         else:
             raise Exception("placement strategy not supported")
         
         pipe.execute()
         ray.get(self.context.coordinator.register_actor_location.remote(self.current_actor, channel_locs))
 
         return self.epilogue(stage, placement_strategy)
```

## pyquokka/sql.py

```diff
@@ -2,14 +2,16 @@
 import duckdb
 import pyquokka
 import json
 from collections import deque
 
 from pyquokka.df import * 
 
+supported_aggs = ['count', 'sum', 'avg', 'min', 'max'] 
+
 def get_new_var_name(var_names):
     return 'v_'+str(len(var_names)+1)
 
 def get_new_col_name(schema):
     i = 0
     while 'col_'+str(i) in schema:
         i += 1
@@ -39,33 +41,52 @@
         aliases[cols[i]] = new_col_name
         col_aliases.append(new_col_name)
         if indices:
             indexed_name = '#' + str(indices[i])
             aliases[indexed_name] = new_col_name
     return cols, col_aliases
 
-def emit_code(node, var_names, var_values, aliases, no_exec=True):
+def emit_code(node, var_names, var_values, aliases, tables, table_prefixes, qc, print_code=True):
     """
     Args:
-        var_names: id number -> name
+        var_names: node id number -> name of variable
         var_values: name -> value
         aliases: 
             #int -> name ('#3' -> 'n_nationkey'),
             formula -> name ('l_partkey + 1' -> 'col1')
             'count_star()' -> 'count(*)'
+        tables (dict): mapping of table name (string) to Quokka Datastream
+        table_prefixes (dict): prefix of columns that identifies which table they belong to, e.g.
+       {
+        'l': 'lineitem',
+        'o': 'orders',
+        'c': 'customer',
+        'p': 'part',
+        'ps': 'partsupp',
+        's': 'supplier',
+        'r': 'region',
+        'n': 'nation'
+        }
+        This is needed because DuckDB plans currently don't include the table name in the READ_PARQUET nodes. Clearly this fix may not extend to other schemas besides TPC-H, so we will find a better solution in the future. 
+        qc: QuokkaContext
+        print_code: print generated code if this flag is set, otherwise don't print anything. 
     Returns:
         values
     """
     var_name = get_new_var_name(var_values.keys())
     code = ""
     result = None
     
-    print('#' + node['name'])
-    if node['name'] == 'PARQUET_SCAN' or node['name'] == 'PROJECTION':
-        if node['name'] == 'PARQUET_SCAN':
+    if print_code: 
+        # Label each code block with the node they correspond to
+        print('#' + node['name'])
+        
+    # Parquet scan nodes have different names in the json depending on DuckDB version
+    if node['name'] in ['PARQUET_SCAN', 'READ_PARQUET '] or node['name'] == 'PROJECTION':
+        if node['name'] in ['PARQUET_SCAN', 'READ_PARQUET ']:
             extra_info = node['extra_info'].split('[INFOSEPARATOR]')
             cols = rewrite_aliases(extra_info[0].strip(), aliases).split('\n')
 
             ####### TEMPORARY FOR TPC-H #######
             table = table_prefixes[cols[0].split('_')[0]]
             ###################################
             child = tables[table]
@@ -93,79 +114,82 @@
             else: 
                 with_column_indices.append(index)
                 with_columns.append(c)
             index += 1
         with_columns, col_aliases = name_aliases(with_columns, aliases, child.schema, with_column_indices)
         
         # Filters may depend on columns not in the select, so filter first
-        if node['name'] == 'PARQUET_SCAN':
+        if node['name'] in ['PARQUET_SCAN', 'READ_PARQUET ']:
             if len(extra_info) >= 2 and 'Filters' in extra_info[1]:
                 filters = extra_info[1].strip()
                 if 'EC' in extra_info[1]: # if there's no infoseparator
                     filters = filters.split('EC')[0].strip()
                 filters = filters.split('\n')
                 filters[0] = filters[0].split('Filters: ')[1]
                 for f in filters:
-                    if no_exec:
+                    if print_code:
                         filter_stmt = var_name + ' = ' + var_name + '.filter_sql("' + f + '")'
                         code += filter_stmt + '\n'
                     # there are some formatting issues for filter where 'r_name = ASIA', etc. won't work, so we'll comment this out for now as filtering doesn't change the schema
-                    if not no_exec: result = result.filter_sql(f)
-        if no_exec and len(selected_columns) > 0:
+                    if not print_code: result = result.filter_sql(f)
+        if print_code and len(selected_columns) > 0:
             select_stmt = var_name + ' = ' + var_name + '.select(' + str(selected_columns) + ')'
             code += select_stmt + '\n'
         result = result.select(selected_columns)
         
         # Parquet scan node won't introduce new columns
         if node['name'] == 'PROJECTION':
             if len(with_columns) > 0:
-                if no_exec:
+                if print_code:
                     with_col_stmt = var_name + ' = ' + var_name + '.with_columns_sql("' + ','.join(with_columns) + '")'
                     code += with_col_stmt + '\n'
                 result = result.with_columns_sql(','.join(with_columns))
                 
                 # If we are only creating new columns, need to make sure we throw away other ones
-                if no_exec and len(selected_columns) == 0:
+                if print_code and len(selected_columns) == 0:
                     select_stmt = var_name + ' = ' + var_name + '.select(' + str(col_aliases) + ')'
                     code += select_stmt
                 result = result.select(col_aliases)
                 
     elif node['name'] == 'FILTER':
-        filters = node['extra_info'].split('\n')
+        filters = node['extra_info'].split('[INFOSEPARATOR]')[0].strip().split('\n')
         child_name = var_names[node['children'][0]['id']]
         child = var_values[child_name]
         result = child
         
         first_filter = True  # for whether to use child_name or var_name in assignment
         for f in filters:
-            if no_exec:
+            if print_code:
                 if first_filter:
                     filter_stmt = var_name + ' = ' + child_name + '.filter_sql("' + f + '")'
                 else:
                     filter_stmt = var_name + ' = ' + var_name + '.filter_sql("' + f + '")'
                 code += filter_stmt + '\n'
             result = result.filter_sql(f)
             first_filter = False
         
     elif node['name'] == 'HASH_JOIN':
         extra_info = node['extra_info'].split('\n')
         how = extra_info[0].lower()
+        # Only allow inner joins for now, there's a bug with semi joins in the plan 
+        assert how in ['inner'], how+ ' joins not supported'
+        
         left_key = extra_info[1].split(' = ')[0]
         right_key = extra_info[1].split(' = ')[1]
         
         left_table_name = var_names[node['children'][0]['id']]
         right_table_name = var_names[node['children'][1]['id']]
         
         left_table = var_values[left_table_name]
         right_table = var_values[right_table_name]
         
         suffix = '_' + str(node['id'])
         # Need to add back left key to table in case it is referenced later in query
         alias_left_key = left_key + ' as ' + right_key
-        if no_exec:
+        if print_code:
             join_stmt = var_name + ' = ' + left_table_name + '.join(' + right_table_name \
                     + ', left_on = "' + left_key + '", right_on = "'  + right_key \
                     + ', suffix = ' + '"' + suffix + '"' + '", how = "' + how + '")'
             code += join_stmt + '\n'
             with_column_stmt = var_name + ' = ' + var_name + '.with_columns_sql("' + alias_left_key + '")'
             code += with_column_stmt
         result = left_table.join(right_table, left_on = left_key, right_on = right_key, suffix = suffix, how = how)
@@ -188,64 +212,54 @@
                     group.append(c)
                 elif is_agg(c):
                     aggregates.append(c)
                 else:
                     with_columns.append(c)
             if len(with_columns) > 0:
                 with_columns, _ = name_aliases(with_columns, aliases, result.schema)
-                if no_exec:
+                if print_code:
                     with_col_stmt = var_name + ' = ' + var_name + '.with_columns_sql("' + ','.join(with_columns) + '")'
                     code += with_col_stmt + '\n'
                 result = result.with_columns_sql(','.join(with_columns))
                 
             # Quokka GroupedDatastream does not have schema attribute, so do this before calling the groupby
             aggregates, _ = name_aliases(aggregates, aliases, result.schema)
             if len(group) > 0:
-                if no_exec:
+                if print_code:
                     groupby_stmt = var_name + ' = ' + var_name + '.groupby(' + str(group) + ')'
                     code += groupby_stmt + '\n'
                 result = result.groupby(group)
         else:
             # All columns should be aggregates in an ungrouped aggregate
             aggregates = rewrite_aliases(node['extra_info'].strip(), aliases).split('\n')
             aggregates, _ = name_aliases(aggregates, aliases, result.schema)
 
-        if no_exec:
+        if print_code:
             agg_stmt = var_name + ' = ' + var_name + '.agg_sql("' + ','.join(aggregates) + '")'
             code += agg_stmt
     else:
         code += '# not supported yet'
 
-    if no_exec: print(code + '\n')
+    if print_code: print(code + '\n')
     var_names[node['id']] = var_name
     var_values[var_name] = result
-
-def generate_code(json_plan, tables, qc):
-    """
-        json_plan (string): name of json file with DuckDB plan
-        tables (dict): looks like
-           {
-                "lineitem": lineitem,
-                "orders": orders,
-                "customer": customer,
-                "part": part,
-                "supplier": supplier,
-                "partsupp": partsupp,
-                "nation": nation,
-                "region": region
-            }
-          where table name is mapped to Quokka datastream
-       qc (QuokkaContext)
-    """
     
-    with open(json_plan) as f:
-        obj = json.load(f)
-    plan = obj['children'][0]['children']
-
+def generate_code_from_plan(plan, tables, qc, table_prefixes = {
+        'l': 'lineitem',
+        'o': 'orders',
+        'c': 'customer',
+        'p': 'part',
+        'ps': 'partsupp',
+        's': 'supplier',
+        'r': 'region',
+        'n': 'nation'
+        }):
+    
     #Reverse topologically sort
+    node = plan[0]
     nodes = deque([node])
     i = 0
     node['id'] = i
     reverse_sorted_nodes = [node]
     while len(nodes) > 0:
         new_node = nodes.popleft()
         for child in new_node['children']:
@@ -255,8 +269,102 @@
             nodes.append(child)
     reverse_sorted_nodes = reverse_sorted_nodes[::-1]
     var_names = {}
     var_values = {}
     aliases = {'count_star()': 'count(*)'}
     
     for node in reverse_sorted_nodes:
-        emit_code(node, var_names, var_values, aliases)
+        emit_code(node, var_names, var_values, aliases, tables, table_prefixes, qc)
+                            
+def generate_code(query, data_path, table_prefixes = {
+        'l': 'lineitem',
+        'o': 'orders',
+        'c': 'customer',
+        'p': 'part',
+        'ps': 'partsupp',
+        's': 'supplier',
+        'r': 'region',
+        'n': 'nation'
+        }, mode='DISK', format_="parquet"):
+    """
+    Args:
+       query (string): String representing SQL query
+       data_path (string): path to the disk storage location, e.g. "/home/ziheng/tpc-h/"
+       table_prefixes (dict): prefix of columns that identifies which table they belong to, e.g.
+       {
+        'l': 'lineitem',
+        'o': 'orders',
+        'c': 'customer',
+        'p': 'part',
+        'ps': 'partsupp',
+        's': 'supplier',
+        'r': 'region',
+        'n': 'nation'
+        }
+        This is needed because DuckDB plans currently don't include the table name in the READ_PARQUET nodes. Clearly this fix may not extend to other schemas besides TPC-H, so we will find a better solution in the future.
+        mode (string): Only 'DISK' is allowed for now. Support for 'S3' will be added soon.
+        format_ (string): The format of the data, 'csv' or 'parquet'. The default value is 'parquet'.
+    """
+    if mode == "DISK":
+        cluster = LocalCluster()
+    else:
+        raise Exception
+
+    qc = QuokkaContext(cluster,2,2)
+    qc.set_config("fault_tolerance", True)
+
+    if mode == "DISK":
+        disk_path = data_path
+        if format_ == "csv":
+            lineitem = qc.read_csv(disk_path + "lineitem.tbl", sep="|", has_header=True)
+            orders = qc.read_csv(disk_path + "orders.tbl", sep="|", has_header=True)
+            customer = qc.read_csv(disk_path + "customer.tbl",sep = "|", has_header=True)
+            part = qc.read_csv(disk_path + "part.tbl", sep = "|", has_header=True)
+            supplier = qc.read_csv(disk_path + "supplier.tbl", sep = "|", has_header=True)
+            partsupp = qc.read_csv(disk_path + "partsupp.tbl", sep = "|", has_header=True)
+            nation = qc.read_csv(disk_path + "nation.tbl", sep = "|", has_header=True)
+            region = qc.read_csv(disk_path + "region.tbl", sep = "|", has_header=True)
+        elif format_ == "parquet":
+            lineitem = qc.read_parquet(disk_path + "lineitem.parquet")
+            orders = qc.read_parquet(disk_path + "orders.parquet")
+            customer = qc.read_parquet(disk_path + "customer.parquet")
+            part = qc.read_parquet(disk_path + "part.parquet")
+            supplier = qc.read_parquet(disk_path + "supplier.parquet")
+            partsupp = qc.read_parquet(disk_path + "partsupp.parquet")
+            nation = qc.read_parquet(disk_path + "nation.parquet")
+            region = qc.read_parquet(disk_path + "region.parquet")
+        else:
+            raise Exception
+    else:
+        raise Exception
+        
+    tables = {
+        "lineitem": lineitem,
+        "orders": orders,
+        "customer": customer,
+        "part": part,
+        "supplier": supplier,
+        "partsupp": partsupp,
+        "nation": nation,
+        "region": region
+    }
+    
+    # Need this argument to know which parquet files to read
+    TABLES = tables.keys()
+    # Table files should be in the form [DATA_PATH][table name].parquet
+    DATA_PATH = data_path
+
+    print("Start plan generation")
+    con = duckdb.connect()
+    con.execute("SET explain_output='ALL';")
+
+    CREATE_SCRIPT = '\n'.join(['CREATE VIEW ' + table + " AS SELECT * FROM read_"+format_+"('" + DATA_PATH + table + "."+format_+"');" for table in TABLES])
+    con.execute(CREATE_SCRIPT)
+    con.execute("PRAGMA explain_output = 'OPTIMIZED_ONLY'; PRAGMA enable_profiling = json; ")
+    result = con.execute('explain analyze ' + query).fetchall()[0][1]
+    obj = json.loads(result)
+
+    plan = obj['children'][0]['children']
+    
+    print("Finished plan generation, beginning code generation")
+    
+    generate_code_from_plan(plan, tables, qc, table_prefixes)
```

## pyquokka/utils.py

```diff
@@ -8,32 +8,37 @@
 import json
 import signal
 import polars
 import multiprocessing
 import concurrent.futures
 import yaml
 import subprocess
+import argparse
+
+from redis import Redis
+
 
 def preexec_function():
     # Ignore the SIGINT signal by setting the handler to the standard
     # signal handler SIG_IGN.
     signal.signal(signal.SIGINT, signal.SIG_IGN)
 
 class EC2Cluster:
-    def __init__(self, public_ips, private_ips, instance_ids, cpu_count_per_instance, spill_dir, docker_head_private_ip = None) -> None:
+    def __init__(self, public_ips, private_ips, instance_ids, cpu_count_per_instance, spill_dir, docker_head_private_ip = None, tags = {}) -> None:
 
         """
         Not meant to be called directly. Use QuokkaClusterManager to create a cluster.
         """
         
         self.num_node = len(public_ips)
         self.public_ips = {}
         self.private_ips = {}
         self.instance_ids = {}
         self.spill_dir = spill_dir
+        self.tags = tags
 
         for node in range(self.num_node):
             self.public_ips[node] = public_ips[node]
             self.private_ips[node] = private_ips[node]
             self.instance_ids[node] = instance_ids[node]
         
         self.state = "running"
@@ -48,14 +53,21 @@
         # connect to that ray cluster
         if docker_head_private_ip is not None:
             return 
         else:
             ray.init(address='ray://' + str(self.leader_public_ip) + ':10001', 
                  runtime_env={"py_modules":[pyquokka_loc]})
     
+    def tag_instance(self, private_ip, tag):
+        assert private_ip in self.private_ips.values()
+        if tag in self.tags:
+            self.tags[tag].append(private_ip)
+        else:
+            self.tags[tag] = [private_ip]
+
     def to_json(self, output = "cluster.json"):
 
         """
         Creates JSON representation of this cluster that can be used to connect to the cluster again.
 
         Args:
             output (str, optional): Path to output file. Defaults to "cluster.json".
@@ -74,19 +86,19 @@
 
             >>> from pyquokka.utils import *
             >>> manager = QuokkaClusterManager(key_name = "my_key", key_location = "/home/ubuntu/.ssh/my_key.pem", security_group = "my_security_group")
             >>> cluster = manager.from_json("my_cluster.json")
         
         """
 
-        json.dump({"instance_ids":self.instance_ids,"cpu_count_per_instance":self.cpu_count, "spill_dir": self.spill_dir},open(output,"w"))
+        json.dump({"instance_ids":self.instance_ids,"cpu_count_per_instance":self.cpu_count, "spill_dir": self.spill_dir, "tags": self.tags},open(output,"w"))
 
 
 class LocalCluster:
-    def __init__(self) -> None:
+    def __init__(self, **kwargs) -> None:
 
         """
         Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because `QuokkaContext` will automatically
         make one for you.
 
         Return:
             LocalCluster: A LocalCluster object.
@@ -103,31 +115,42 @@
             >>> from pyquokka.df import QuokkaContext
             >>> qc = QuokkaContext()
             
         """
 
         print("Initializing local Quokka cluster.")
         self.num_node = 1
+        self.tags = {}
         self.cpu_count = multiprocessing.cpu_count()
         pyquokka_loc = pyquokka.__file__.replace("__init__.py","")
         # we assume you have pyquokka installed, and we are going to spin up a ray cluster locally
         ray.init(ignore_reinit_error=True)
         flight_file = pyquokka_loc + "/flight.py"
         self.flight_process = None
         self.redis_process = None
         os.system("export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288")
         port5005 = os.popen("lsof -i:5005").read()
         if "python" in port5005:
             raise Exception("Port 5005 is already in use. Kill the process that is using it first.")
-            
         try:
             self.flight_process = subprocess.Popen(["python3", flight_file], preexec_fn = preexec_function)
         except:
             raise Exception("Could not start flight server properly. Check if there is already something using port 5005, kill it if necessary. Use lsof -i:5005")
-        self.redis_process = subprocess.Popen(["redis-server" , pyquokka_loc + "redis.conf", "--port 6800", "--protected-mode no"], preexec_fn=preexec_function)
+
+        if 'docker_redis_enabled' in kwargs :
+            redis_host = "127.0.0.1"
+            redis_port = 6800
+            redis_instance = Redis(redis_host, port=redis_port, socket_connect_timeout=1)
+            try:
+                redis_instance.ping()
+            except:
+                raise Exception(f"Could not connect to local redis server at: {redis_host}:{redis_port}")
+        else:
+            self.redis_process = subprocess.Popen(["redis-server" , pyquokka_loc + "redis.conf", "--port 6800", "--protected-mode no"], preexec_fn=preexec_function)
+
         self.leader_public_ip = "localhost"
         self.leader_private_ip = ray.get_runtime_context().gcs_address.split(":")[0]
         self.public_ips = {0:"localhost"}
         self.private_ips = {0: ray.get_runtime_context().gcs_address.split(":")[0]}
         print("Finished setting up local Quokka cluster.")
     
     def __del__(self):
@@ -137,24 +160,38 @@
         if self.redis_process is not None:
             self.redis_process.kill()
 
 
 def execute_script(key_location, x):
     return os.system("ssh -oStrictHostKeyChecking=no -i {} ubuntu@{} 'bash -s' < {}".format(key_location, x, pyquokka.__file__.replace("__init__.py", "common_startup.sh")))
 
+def execute_script1(key_location, x, spill_dir):
+    return os.system("ssh -oStrictHostKeyChecking=no -i {} ubuntu@{} 'bash -s' < {} {}".format(key_location, x, pyquokka.__file__.replace("__init__.py", "disk_setup.sh"), spill_dir))
+
 def get_cluster_from_docker_head(spill_dir = "/data"):
     import socket
     self_ip = socket.gethostbyname(socket.gethostname())
     ray.init("ray://" + self_ip + ":10001")
     private_ips = [name.split(":")[1] for name in ray.cluster_resources().keys() if "node" in name]
     # rotate private_ips so that self_ip is the first element
     private_ips = private_ips[private_ips.index(self_ip):] + private_ips[:private_ips.index(self_ip)]
     cpu_count = ray.cluster_resources()["CPU"] // len(private_ips)
     return EC2Cluster([None] * len(private_ips), private_ips, [None] * len(private_ips), cpu_count, spill_dir, docker_head_private_ip=self_ip)
 
+def get_cluster_from_ip(ip, spill_dir = "/data"):
+    pyquokka_loc = pyquokka.__file__.replace("__init__.py","")
+    ray.init("ray://" + ip + ":10001",runtime_env={"py_modules":[pyquokka_loc]})
+    private_ips = [name.split(":")[1] for name in ray.cluster_resources().keys() if "node" in name]
+    head_node_private_ip = ray.nodes()[0]['NodeManagerAddress']
+    # rotate private_ips so that head_node_private_ip is the first element
+    private_ips = private_ips[private_ips.index(head_node_private_ip):] + private_ips[:private_ips.index(head_node_private_ip)]
+    cpu_count = ray.cluster_resources()["CPU"] // len(private_ips)
+    return EC2Cluster([None] * len(private_ips), private_ips, [None] * len(private_ips), cpu_count, spill_dir, docker_head_private_ip=ip)
+
+
 class QuokkaClusterManager:
 
     def __init__(self, key_name = None, key_location = None, security_group= None) -> None:
         
         """
         Create a QuokkaClusterManager object. This object is used to create Ray clusters on AWS EC2 configured with Quokka or connecting to existing Ray clusters.
         This requires you to have an AWS key pair for logging into instances. 
@@ -190,15 +227,15 @@
         return {int(i):d[i] for i in d}
     
     def install_python_package(self, cluster, req):
         assert type(cluster) == EC2Cluster
         self.launch_all("pip3 install " + req, list(cluster.public_ips.values()), "Failed to install " + req)
 
     def launch_ssh_command(self, command, ip, ignore_error=False):
-        launch_command = "ssh -oStrictHostKeyChecking=no -oConnectTimeout=5 -i " + self.key_location + " ubuntu@" + ip + " " + command
+        launch_command = "ssh -oStrictHostKeyChecking=no -oConnectTimeout=5 -i " + self.key_location + " ubuntu@" + ip + " '" + command.replace("'", "'\"'\"'") + "' "
         try:
             result = subprocess.run(launch_command, shell=True, capture_output=True, check=True)
             return result.stdout.decode().strip()
         except subprocess.CalledProcessError as e:
             raise Exception(f"launch_ssh_command failed with exit code: {e.returncode}")
         
     def launch_all(self, command, ips, error = "Error", ignore_error = False):
@@ -239,15 +276,16 @@
         private_ips = [k['PrivateIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
         
         leader_public_ip = public_ips[0]
         leader_private_ip = private_ips[0]
 
         self.check_instance_alive(public_ips)
 
-        self.set_up_spill_dir(public_ips, spill_dir)
+        pool = multiprocessing.Pool(multiprocessing.cpu_count())        
+        pool.starmap(execute_script1, [(self.key_location, public_ip, spill_dir) for public_ip in public_ips])
         z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + leader_public_ip + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
         print(z)
         z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + leader_public_ip + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_start_ray.sh"))
         print(z)
 
         command ="/home/ubuntu/.local/bin/ray start --address='" + str(leader_private_ip) + ":6380' --redis-password='5241590000000000'"
         self.launch_all(command, public_ips, "ray workers failed to connect to ray head node")
@@ -255,16 +293,16 @@
         self.copy_and_launch_flight(public_ips)
 
     def set_up_envs(self, public_ips, requirements, aws_access_key, aws_access_id):
             
         pool = multiprocessing.Pool(multiprocessing.cpu_count())        
         pool.starmap(execute_script, [(self.key_location, public_ip) for public_ip in public_ips])
 
-        self.launch_all("aws configure set aws_secret_access_key " + str(aws_access_key), public_ips, "Failed to set AWS access key")
-        self.launch_all("aws configure set aws_access_key_id " + str(aws_access_id), public_ips, "Failed to set AWS access id")
+        self.launch_all("/home/ubuntu/.local/bin/aws configure set aws_secret_access_key " + str(aws_access_key), public_ips, "Failed to set AWS access key")
+        self.launch_all("/home/ubuntu/.local/bin/aws configure set aws_access_key_id " + str(aws_access_id), public_ips, "Failed to set AWS access id")
 
         print("----Completed aws access key setting----")
         # cluster must have same ray version as client.
         requirements = ["ray==" + ray.__version__, "polars==" + polars.__version__,  "pyquokka"] + requirements
         for req in requirements:
             assert type(req) == str
             try:
@@ -276,39 +314,15 @@
 
     def copy_and_launch_flight(self, public_ips):
         
         self.copy_all(pyquokka.__file__.replace("__init__.py","flight.py"), public_ips, "Failed to copy flight server file.")
         self.launch_all("export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288", public_ips, "Failed to set malloc limit")
         self.launch_all("nohup python3 -u flight.py > foo.out 2> foo.err < /dev/null &", public_ips, "Failed to start flight servers on workers.")
 
-    def set_up_spill_dir(self, public_ips, spill_dir):
-
-        print("Trying to set up spill dir.")   
-        result = self.launch_all("sudo nvme list", public_ips, "failed to list nvme devices")
-        devices = []
-
-        for sentence in result:
-            if "Amazon EC2 NVMe Instance Storage" in sentence:
-                split_by_newline = sentence.split("\n")
-                device = split_by_newline[2].split(" ")[0]
-                devices.append(device)
-
-        if len(devices) == 0:
-            print("No nvme devices found. Skipping.")
-            return
-
-        assert all([device == devices[0] for device in devices]), "All instances must have same nvme device location. Raise Github issue if you see this."
-        device = devices[0]
-        print("Found nvme device: ", device)
-        self.launch_all("sudo mkfs.ext4 -F -E nodiscard {};".format(device), public_ips, "failed to format nvme ssd")
-        self.launch_all("sudo mount {} {};".format(device, spill_dir), public_ips, "failed to mount nvme ssd")
-        self.launch_all("sudo chmod -R a+rw {}".format(spill_dir), public_ips, "failed to give spill dir permissions")
-
-
-    def create_cluster(self, aws_access_key, aws_access_id, num_instances, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = [], spill_dir = "/data"):
+    def create_cluster(self, aws_access_key, aws_access_id, instance_types = None, amis = None, requirements = [], spill_dir = "/data", volume_size = 8):
 
         """
         Create a Ray cluster configured to run Quokka applications.
 
         Args:
             aws_access_key (str): AWS access key.
             aws_access_id (str): AWS access id.
@@ -331,101 +345,72 @@
             >>> qc = QuokkaContext(cluster)
             >>> df = qc.read_csv("s3://my_bucket/my_file.csv")
                     
         """
 
         start_time = time.time()
         ec2 = boto3.client("ec2")
-        vcpu_per_node = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
-        waiter = ec2.get_waiter('instance_running')
-        res = ec2.run_instances(ImageId=ami, InstanceType = instance_type, SecurityGroupIds = [self.security_group], KeyName=self.key_name ,MaxCount=num_instances, MinCount=num_instances)
-        instance_ids = [res['Instances'][i]['InstanceId'] for i in range(num_instances)] 
-        waiter.wait(InstanceIds=instance_ids)
-        a = ec2.describe_instances(InstanceIds = instance_ids)
-        public_ips = [a['Reservations'][0]['Instances'][i]['PublicIpAddress'] for i in range(num_instances)]
-        private_ips = [a['Reservations'][0]['Instances'][i]['PrivateIpAddress'] for i in range(num_instances)]
-
-        self.check_instance_alive(public_ips)
-
-        self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
-        self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory")
-        self._initialize_instances(instance_ids, spill_dir)
-
-        print("Launching of Quokka cluster used: ", time.time() - start_time)
-
-        return EC2Cluster(public_ips, private_ips, instance_ids, vcpu_per_node, spill_dir)  
-        
 
-    def stop_cluster(self, quokka_cluster):
+        assert instance_types is not None, "Please specify instance types."
+        assert type(instance_types) == dict, "instance_types must be a dictionary, e.g. {'i3.2xlarge':4} or {'i3.2xlarge':4, 'c6i.4xlarge':2}"
+        assert amis is not None, "Please specify AMIs."
+        assert type(amis) == dict, "amis must be a dictionary, e.g. {'i3.2xlarge':'ami-0530ca8899fac469f'} or {'i3.2xlarge':'ami-0530ca8899fac469f', 'c6i.4xlarge':'ami-0530ca8899fac469f'}"
 
-        """
-        Stops a cluster, does not terminate it. If the cluster had been saved to json, can use `get_cluster_from_json` to restart the cluster.
-
-        Args:
-            quokka_cluster (EC2Cluster): Cluster to stop.
-
-        Return:
-            None
-
-        Examples:
-
-            >>> from pyquokka.utils import *
-            >>> manager = QuokkaClusterManager()
-            >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = ["numpy", "pandas"])
-            >>> cluster.to_json("my_cluster.json")
-            >>> manager.stop_cluster(cluster)
-        
-        """
+        public_ips = []
+        private_ips = []
+        instance_ids = []
+        tags = {}
 
-        ec2 = boto3.client("ec2")
-        instance_ids = list(quokka_cluster.instance_ids.values())
-        ec2.stop_instances(InstanceIds = instance_ids)
-        while True:
-            time.sleep(0.1)
-            a = ec2.describe_instances(InstanceIds = instance_ids)
-            states = [a['Reservations'][0]['Instances'][i]['State']['Name'] for i in range(len(instance_ids))]
-            if "running" in states:
-                continue
-            else:
-                break
-        quokka_cluster.state = "stopped"
-        
-        
-    def terminate_cluster(self, quokka_cluster):
+        for instance_type in instance_types:
 
-        """
-        Terminate a cluster.
+            num_instances = instance_types[instance_type]
+            ami = amis[instance_type]
 
-        Args:
-            quokka_cluster (EC2Cluster): Cluster to terminate.
+            vcpu_per_node = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
+            waiter = ec2.get_waiter('instance_running')
+            res = ec2.run_instances(
+                BlockDeviceMappings=[
+                    {
+                        'DeviceName': ec2.describe_images(ImageIds=[ami])['Images'][0]['RootDeviceName'],
+                        'Ebs': {
+                            'DeleteOnTermination': True,
+                            'VolumeSize': volume_size,
+                            'VolumeType': 'gp3'
+                        }
+                    }
+                ],
+                ImageId=ami, InstanceType = instance_type, SecurityGroupIds = [self.security_group], KeyName=self.key_name ,MaxCount=num_instances, MinCount=num_instances)
+            instance_instance_ids = [res['Instances'][i]['InstanceId'] for i in range(num_instances)] 
+            waiter.wait(InstanceIds=instance_instance_ids)
+            a = ec2.describe_instances(InstanceIds = instance_instance_ids)
+            instance_public_ips = [a['Reservations'][0]['Instances'][i]['PublicIpAddress'] for i in range(num_instances)]
+            instance_private_ips = [a['Reservations'][0]['Instances'][i]['PrivateIpAddress'] for i in range(num_instances)]
+            tags[instance_type] = instance_private_ips
+            public_ips.extend(instance_public_ips)
+            private_ips.extend(instance_private_ips)
+            instance_ids.extend(instance_instance_ids)
 
-        Return:
-            None
+        self.check_instance_alive(public_ips)
 
-        Examples:
+        self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
+        self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory")
+        self._initialize_instances(instance_ids, spill_dir)
 
-            >>> from pyquokka.utils import *
-            >>> manager = QuokkaClusterManager()
-            >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = ["numpy", "pandas"])
-            >>> manager.terminate_cluster(cluster)
+        print("Launching of Quokka cluster used: ", time.time() - start_time)
 
-        """
+        return EC2Cluster(public_ips, private_ips, instance_ids, vcpu_per_node, spill_dir, tags = tags)  
+    
+    def start_cluster(self, cluster_json):
 
         ec2 = boto3.client("ec2")
-        instance_ids = list(quokka_cluster.instance_ids.values())
-        ec2.terminate_instances(InstanceIds = instance_ids)
-        while True:
-            time.sleep(0.1)
-            a = ec2.describe_instances(InstanceIds = instance_ids)
-            states = [a['Reservations'][0]['Instances'][i]['State']['Name'] for i in range(len(instance_ids))]
-            if "running" in states:
-                continue
-            else:
-                break
-        del quokka_cluster
+        with open(cluster_json, "r") as f:
+            cluster_info = json.load(f)
+        instance_ids = list(cluster_info['instance_ids'].values())
+        ec2.start_instances(InstanceIds = instance_ids)
+        self._initialize_instances(instance_ids, cluster_info["spill_dir"])
 
     
     def get_cluster_from_json(self, json_file):
 
         """
         Get an EC2Cluster object from a json file. The json file must have been created by `EC2Cluster.to_json`.
         This will restart the cluster if all the instances have been stopped and set up the Quokka runtime. 
@@ -457,125 +442,30 @@
         instance_ids = self.str_key_to_int(stuff["instance_ids"])
         instance_ids = [instance_ids[i] for i in range(len(instance_ids))]
         a = ec2.describe_instances(InstanceIds = instance_ids)
         
         states = [k['State']['Name'] for reservation in a['Reservations'] for k in reservation['Instances']] 
 
         if sum([i=="stopped" for i in states]) == len(states):
-            ec2.start_instances(InstanceIds = instance_ids)
-            self._initialize_instances(instance_ids, spill_dir)
-            a = ec2.describe_instances(InstanceIds = instance_ids)
-
-            public_ips = [k['PublicIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
-            private_ips = [k['PrivateIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
-
-            return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+            print("Cluster is stopped. Run quokkactl start_cluster to start the cluster.")
         if sum([i=="running" for i in states]) == len(states):
             request_instance_ids = [k['InstanceId'] for reservation in a['Reservations'] for k in reservation['Instances']]
             public_ips = [k['PublicIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
             private_ips = [k['PrivateIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
 
             # figure out where in request_instance_ids is instance_ids[0]
             leader_index = request_instance_ids.index(instance_ids[0])
             assert leader_index != -1, "Leader instance not found in request_instance_ids"
             public_ips = public_ips[leader_index:] + public_ips[:leader_index]
             private_ips = private_ips[leader_index:] + private_ips[:leader_index]
 
-            return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+            return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir, tags = stuff['tags'] if 'tags' in stuff else {})
         else:
             print("Cluster in an inconsistent state. Either only some machines are running or some machines have been terminated.")
             return False
-    
-    def get_cluster_from_ray(self, path_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data"):
-
-        """
-        Connect to a Ray cluster. This will set up the Quokka runtime on the cluster. The Ray cluster must be in a running state and created by 
-        the `ray up` command. The `ray up` command creates a yaml file that is used to connect to the cluster. This function will read the yaml file
-        and connect to the cluster.
-
-        Make sure all the instances are running before calling this function! Best wait for a few minutes after calling `ray up` before calling this function.
-
-        Args:
-            path_to_yaml (str): Path to the yaml file used by `ray up`.
-            aws_access_key (str): AWS access key.
-            aws_access_id (str): AWS access id.
-            requirements (list): List of python packages to install on the cluster.
-            spill_dir (str): Directory to use for spill files. This is the directory where the Quokka runtime will write spill files.
-                Quokka will detect if your instance have NVME SSD and mount it to this directory.
-        
-        Return:
-            EC2Cluster: Cluster object.
-
-        Examples:
-
-            You have `us_west_2.yaml`. You call `ray up us-west-2.yaml`. You can then connect to the cluster **after all the instances are running** by doing:
-
-            >>> from pyquokka.utils import *
-            >>> manager = QuokkaClusterManager()
-            >>> cluster = manager.get_cluster_from_ray("my_cluster.yaml", aws_access_key, aws_access_id, requirements = ["numpy", "pandas"], spill_dir = "/data")
-            >>> from pyquokka.df import QuokkaContext
-            >>> qc = QuokkaContext(cluster)
-
-            It is recommended to do this only once and save the cluster object to a json file using `EC2Cluster.to_json` and then use `QuokkaClusterManager.get_cluster_from_json` to connect to the cluster.
-
-            >>> cluster.to_json("my_cluster.json")
-            >>> cluster = manager.get_cluster_from_json("my_cluster.json")
-        
-        """
-
-        import yaml
-        with open(path_to_yaml, 'r') as f:
-            config = yaml.safe_load(f)
-        
-        region = config["provider"]["region"]
-        ec2 = boto3.client("ec2", region_name=region)
-    
-        tag_key = "ray-cluster-name"
-        cluster_name = config['cluster_name']
-        instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
-        cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
-
-        filters = [{'Name': 'instance-state-name', 'Values': ['running']},
-                   {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
-        response = ec2.describe_instances(Filters=filters)
-        instance_ids = []
-        public_ips = []
-        private_ips = []
-
-        instance_names = [[k for k in instance['Tags'] if k['Key'] == 'ray-user-node-type'][0]['Value'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        instance_ids = [instance['InstanceId'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        public_ips = [instance['PublicIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        private_ips = [instance['PrivateIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-
-        try:
-            head_index = instance_names.index("ray.head.default")
-        except:
-            print("No head node found. Please make sure that the cluster is running.")
-            return False
-    
-        # rotate instance_ids, public_ips, private_ips so that head is first
-        instance_ids = instance_ids[head_index:] + instance_ids[:head_index]
-        public_ips = public_ips[head_index:] + public_ips[:head_index]
-        private_ips = private_ips[head_index:] + private_ips[:head_index]
-
-        assert len(instance_ids) == len(public_ips) == len(private_ips)
-        print("Detected {} instances in running ray cluster {}".format(len(instance_ids), cluster_name))
-
-        print(public_ips)
-        self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
-        self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory", ignore_error = True)
-        self.set_up_spill_dir(public_ips, spill_dir)
-
-        z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + public_ips[0] + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
-        print(z)
-
-        print(f"-----Before launching redis-----")
-        self.copy_and_launch_flight(public_ips)
-        print(f"-----Returning EC2 cluster------")
-        return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
 
     
     def get_multiple_clusters_from_yaml(self, paths_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data"):
         
         """
         Connect to a multiregional Ray cluster. This will set up the Quokka runtime on the cluster. The Ray clusters must be in a running state and created by 
         the `ray up` command. The `ray up` command creates a yaml file that is used to connect to the cluster. This function will read the provided files
@@ -663,15 +553,16 @@
             return (instance_ids, public_ips, private_ips, cpu_count, region)
         
 
         def get_cluster_from_ips(instance_ids, public_ips, private_ips, cpu_count, aws_access_id, aws_access_key, requirements, spill_dir):
             
             self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
             self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory", ignore_error = True)
-            self.set_up_spill_dir(public_ips, spill_dir)
+            pool = multiprocessing.Pool(multiprocessing.cpu_count())        
+            pool.starmap(execute_script1, [(self.key_location, public_ip, spill_dir) for public_ip in public_ips])
 
             print(f"----Launching ray cluster----")
 
             leader_public_ip = public_ips[0]
             leader_private_ip = private_ips[0]
 
             z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + str(leader_public_ip) + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
@@ -746,49 +637,223 @@
         print("----Stopping ray on all instances----")
         self.launch_all(stop_command, all_public_ips)
 
         # Created cluster across all instances
         cluster = get_cluster_from_ips(all_instance_ids, all_public_ips, all_private_ips, cpu_count, aws_access_id, aws_access_key, requirements, spill_dir)
 
         return (cluster, region_info)
-      
-    def get_cluster_from_docker_cluster(self, path_to_yaml, spill_dir = "/data", cluster_name = None):
+    
+def stop_cluster(cluster_json):
 
-        """
-        """
+    """
+    Stops a cluster, does not terminate it. If the cluster had been saved to json, can use `get_cluster_from_json` to restart the cluster.
 
-        import yaml
-        ec2 = boto3.client("ec2")
-        with open(path_to_yaml, 'r') as f:
-            config = yaml.safe_load(f)
-    
-        tag_key = "ray-cluster-name"
-        if cluster_name is None:
-            cluster_name = config['cluster_name']
-        instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
-        cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
+    Args:
+        quokka_cluster (EC2Cluster): Cluster to stop.
+
+    Return:
+        None
+
+    Examples:
+
+        >>> from pyquokka.utils import *
+        >>> manager = QuokkaClusterManager()
+        >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = ["numpy", "pandas"])
+        >>> cluster.to_json("my_cluster.json")
+        >>> manager.stop_cluster(cluster)
+    
+    """
+
+    ec2 = boto3.client("ec2")
+    with open(cluster_json, "r") as f:
+        cluster_info = json.load(f)
+    instance_ids = list(cluster_info['instance_ids'].values())
+    ec2.stop_instances(InstanceIds = instance_ids)
+    while True:
+        time.sleep(0.1)
+        a = ec2.describe_instances(InstanceIds = instance_ids)
+        states = [a['Reservations'][0]['Instances'][i]['State']['Name'] for i in range(len(instance_ids))]
+        if "running" in states:
+            continue
+        else:
+            break
+        
+        
+def terminate_cluster(cluster_json):
+
+    """
+    Terminate a cluster.
+
+    Args:
+        quokka_cluster (EC2Cluster): Cluster to terminate.
+
+    Return:
+        None
+
+    Examples:
+
+        >>> from pyquokka.utils import *
+        >>> manager = QuokkaClusterManager()
+        >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = ["numpy", "pandas"])
+        >>> manager.terminate_cluster(cluster)
+
+    """
+
+    ec2 = boto3.client("ec2")
+    with open(cluster_json, "r") as f:
+        cluster_info = json.load(f)
+    instance_ids = list(cluster_info['instance_ids'].values())
+    ec2.terminate_instances(InstanceIds = instance_ids)
+    ec2.terminate_instances(InstanceIds = instance_ids)
+    while True:
+        time.sleep(0.1)
+        a = ec2.describe_instances(InstanceIds = instance_ids)
+        states = [a['Reservations'][0]['Instances'][i]['State']['Name'] for i in range(len(instance_ids))]
+        if "running" in states:
+            continue
+        else:
+            break
+    os.remove(cluster_json)
 
-        filters = [{'Name': 'instance-state-name', 'Values': ['running']},
-                   {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
-        response = ec2.describe_instances(Filters=filters)
-
-        instance_names = [[k for k in instance['Tags'] if k['Key'] == 'ray-user-node-type'][0]['Value'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        instance_ids = [instance['InstanceId'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        public_ips = [instance['PublicIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
-        private_ips = [instance['PrivateIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+def make_new_sg():
 
+    # Create an EC2 resource object
+    ec2 = boto3.resource('ec2')
+
+    # Create a new security group
+    try:
+        security_group = ec2.create_security_group(
+            GroupName='quokka-security-group',
+            Description='Quokka Security Group'
+        )
+    except:
+        raise Exception("quokka-security-group already exists, just use that one!")
+
+    # Authorize all inbound and outbound traffic within the security group
+    security_group.authorize_ingress(
+        IpPermissions=[
+            {
+                'IpProtocol': '-1',  # All protocols
+                'UserIdGroupPairs': [
+                    {
+                        'GroupId': security_group.id  # Allow traffic from the same security group
+                    }
+                ]
+            }
+        ]
+    )
+
+    security_group.authorize_egress(
+        IpPermissions=[
+            {
+                'IpProtocol': '-1',  # All protocols
+                'UserIdGroupPairs': [
+                    {
+                        'GroupId': security_group.id  # Allow traffic to the same security group
+                    }
+                ]
+            }
+        ]
+        )
+
+    return security_group.id
+
+
+def cli_main():
+    parser = argparse.ArgumentParser(description='Utility to launch, stop or terminate a Quokka cluster. API inspired by AWS EMR.')
+
+    parser.add_argument('action', choices = ['create-cluster','start-cluster', 'pip-install', 'stop-cluster', 'terminate-cluster'], help = 'Action to perform.')
+
+    parser.add_argument('--name', help = 'Name of the cluster to launch. Your cluster will be saved to name.json.')
+    parser.add_argument('--instance-type', help = 'Instance type to use for the cluster. Default is r6id.2xlarge.', default = 'r6id.2xlarge')
+    parser.add_argument('--instance-count', help = 'Number of instances to launch. Default is 1.', default = 1, type = int)
+    parser.add_argument('--security-group', help = 'Security group to use for the cluster. Default is to make a new random one.', default = None)
+    parser.add_argument('--pem-key', help = 'Path to the key to use for the cluster.', default = None)
+
+    args = parser.parse_args()
+
+    if args.action == 'create-cluster':
+
+        # make sure args.name is not already there
+        if os.path.isfile(args.name + '.json'):
+            raise ValueError('specified cluster name already exists! Please use a different name or log into the existing cluster.')
+        print("Creating Quokka cluster, cluster config will be written to {}.json".format(args.name))
+
+        # make sure the pem-key is a real file
+        assert args.pem_key is not None, "Must specify a pem key!"
+        if not os.path.isfile(args.pem_key):
+            raise ValueError('specified pem key does not exist!')
+        key_name = os.path.basename(args.pem_key).split('.')[0]
+        # check the security group exists if not None, otherwise make a new one
+        if args.security_group is None:
+            security_group = make_new_sg()
+        else:
+            # check security group is real using boto3 api
+            ec2 = boto3.resource('ec2')
+            try:
+                security_group = ec2.SecurityGroup(args.security_group)
+            except:
+                raise ValueError('specified security group does not exist!')
+
+        manager = QuokkaClusterManager(key_name, args.pem_key, security_group)
+        # try to get your AWS credentials from the environment
         try:
-            head_index = instance_names.index("ray.head.default")
+            aws_access_id = os.environ['AWS_ACCESS_KEY_ID']
+            aws_access_key = os.environ['AWS_SECRET_ACCESS_KEY']
         except:
-            print("No head node found. Please make sure that the cluster is running.")
-            return False
+            # not in environemnt variables, maybe in ~/.aws/credentials
+            try:
+                session = boto3.Session()
+                credentials = session.get_credentials()
+                aws_access_id = credentials.access_key
+                aws_access_key = credentials.secret_key
+            except:
+                raise ValueError('AWS credentials not found! Please run aws configure to set them up.')
+        
+        cluster = manager.create_cluster( aws_access_key, aws_access_id,  args.instance_count, args.instance_type)
+        cluster.to_json(args.name + '.json')
+    
+    elif args.action == 'start-cluster':
+
+        # make sure args.name is not already there
+        if not os.path.isfile(args.name + '.json'):
+            raise ValueError('make sure {} is present in current folder'.format(args.name + '.json'))
+        
+        assert args.pem_key is not None, "Must specify a pem key!"
+        if not os.path.isfile(args.pem_key):
+            raise ValueError('specified pem key does not exist!')
+        key_name = os.path.basename(args.pem_key).split('.')[0]
+
+        manager = QuokkaClusterManager(key_name, args.pem_key, None)
+        manager.start_cluster(args.name + '.json')
+        print("Creating Quokka cluster, cluster config will be written to {}.json".format(args.name))
     
-        # rotate instance_ids, public_ips, private_ips so that head is first
-        instance_ids = instance_ids[head_index:] + instance_ids[:head_index]
-        public_ips = public_ips[head_index:] + public_ips[:head_index]
-        private_ips = private_ips[head_index:] + private_ips[:head_index]
+    elif args.action == 'pip-install':
+
+        # make sure args.name is not already there
+        if not os.path.isfile(args.name + '.json'):
+            raise ValueError('make sure {} is present in current folder'.format(args.name + '.json'))
+        
+        assert args.pem_key is not None, "Must specify a pem key!"
+        if not os.path.isfile(args.pem_key):
+            raise ValueError('specified pem key does not exist!')
+        key_name = os.path.basename(args.pem_key).split('.')[0]
 
-        assert len(instance_ids) == len(public_ips) == len(private_ips)
-        print("Detected {} instances in running ray cluster {}".format(len(instance_ids), cluster_name))
+        manager = QuokkaClusterManager(key_name, args.pem_key, None)
+        manager.install_python_package()
+        print("Creating Quokka cluster, cluster config will be written to {}.json".format(args.name))
 
-        print(public_ips)
-        return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+    elif args.action == 'stop-cluster':
+        
+        # make sure args.name is not already there
+        if not os.path.isfile(args.name + '.json'):
+            raise ValueError('make sure {} is present in current folder'.format(args.name + '.json'))
+        
+        stop_cluster(args.name + '.json')
+    
+    elif args.action == 'terminate-cluster':
+
+        # make sure args.name is not already there
+        if not os.path.isfile(args.name + '.json'):
+            raise ValueError('make sure {} is present in current folder'.format(args.name + '.json'))
+        
+        terminate_cluster(args.name + '.json')
```

## pyquokka/executors/__init__.py

```diff
@@ -1,3 +1,4 @@
 from pyquokka.executors.sql_executors import * 
 from pyquokka.executors.vector_executors import * 
 from pyquokka.executors.ts_executors import * 
+from pyquokka.executors.cep_executors import *
```

## pyquokka/executors/sql_executors.py

```diff
@@ -9,15 +9,15 @@
     
     def deserialize(self, s):
         pass
 
     def execute(self,batches,stream_id, executor_id):
         batches = [i for i in batches if i is not None]
         if len(batches) > 0:
-            return self.udf(polars.concat(batches, rechunk=False))
+            return self.udf(polars.from_arrow(pa.concat_tables(batches)))
         else:
             return None
 
     def done(self,executor_id):
         return
 
 # this is not fault tolerant. If the storage is lost you just re-read
@@ -183,15 +183,15 @@
                     cached_batches[source] = cached_batches[source][desired_length:]
             
             result = polars.concat(desired_batches).sort(self.key)
             print("yield one took", time.time() - start)
             yield result
 
 class OutputExecutor(Executor):
-    def __init__(self, filepath, format, prefix = "part", region = "local", row_group_size = 5500000) -> None:
+    def __init__(self, filepath, format, prefix = "part", region = "local", row_group_size = 5000000) -> None:
         self.num = 0
         assert format == "csv" or format == "parquet"
         self.format = format
         self.filepath = filepath
         self.prefix = prefix
         self.row_group_size = row_group_size
         self.my_batches = []
@@ -313,14 +313,15 @@
         if len(batches) == 0:
             return
         batch = polars.concat(batches)
         return batch.join(self.state, left_on = self.big_on, right_on = self.small_on, how = self.how, suffix = self.suffix)
         
     def done(self,executor_id):
         return
+        
 
 # this is an inner join executor that must return outputs in a sorted order based on sorted_col
 # the operator will maintain the sortedness of the probe side
 # 0/left is probe, 1/right is build.
 class BuildProbeJoinExecutor(Executor):
 
     def __init__(self, on = None, left_on = None, right_on = None, how = "inner", key_to_keep = "left"):
@@ -370,14 +371,78 @@
             if self.key_to_keep == "right":
                 result = result.rename({self.left_on: self.right_on})
             return result
     
     def done(self,executor_id):
         pass
 
+# this is an inner join executor that must return outputs in a sorted order based on sorted_col
+# the operator will maintain the sortedness of the probe side
+# 0/left is probe, 1/right is build.
+class DiskBuildProbeJoinExecutor(Executor):
+
+    def __init__(self, on = None, left_on = None, right_on = None, how = "inner", key_to_keep = "left", spill_dir = "/data"):
+        import secrets
+        import string
+        if on is not None:
+            assert left_on is None and right_on is None
+            self.left_on = on
+            self.right_on = on
+        else:
+            assert left_on is not None and right_on is not None
+            self.left_on = left_on
+            self.right_on = right_on
+        
+        self.phase = "build"
+        assert how in {"inner", "left", "semi", "anti"}
+        self.how = how
+        self.key_to_keep = key_to_keep
+        self.things_seen = []
+        self.count = 0
+        self.spill_dir = spill_dir
+        self.prefix = ''.join(secrets.choice(string.ascii_uppercase + string.ascii_lowercase) for i in range(7))
+
+    def execute(self,batches, stream_id, executor_id):
+        # state compaction
+        batches = [polars.from_arrow(i) for i in batches if i is not None and len(i) > 0]
+        if len(batches) == 0:
+            return
+        batch = polars.concat(batches)
+        self.things_seen.append((stream_id, len(batches)))
+
+        # build
+        if stream_id == 1:
+            assert self.phase == "build", (self.left_on, self.right_on, self.things_seen)
+            
+            # batch.write_ipc(f"{self.spill_dir}/build_{self.prefix}_{executor_id}_{self.count}.arrow")
+            batch.write_parquet(f"{self.spill_dir}/build_{self.prefix}_{executor_id}_{self.count}.parquet")
+            self.count += 1
+                           
+        # probe
+        elif stream_id == 0:
+            if self.count == 0:
+                if self.how == "anti":
+                    return batch
+                else:
+                    return
+            
+            self.phase = "probe"
+            results = []
+            for i in range(self.count):
+                # results.append(batch.lazy().join(polars.scan_ipc(f"{self.spill_dir}/build_{self.prefix}_{executor_id}_{i}.arrow"),left_on = self.left_on, right_on = self.right_on ,how= self.how))
+                results.append(batch.lazy().join(polars.scan_parquet(f"{self.spill_dir}/build_{self.prefix}_{executor_id}_{i}.parquet"),left_on = self.left_on, right_on = self.right_on ,how= self.how))
+            result = polars.concat(results).collect()
+
+            if self.key_to_keep == "right":
+                result = result.rename({self.left_on: self.right_on})
+            return result
+    
+    def done(self,executor_id):
+        pass
+
 class DistinctExecutor(Executor):
     def __init__(self, keys) -> None:
 
         self.keys = keys
         self.state = None
     
     def checkpoint(self, conn, actor_id, channel_id, seq):
@@ -411,14 +476,19 @@
         self.seen = s[0][0]
     
     def done(self, executor_id):
         return
 
 class SQLAggExecutor(Executor):
     def __init__(self, groupby_keys, orderby_keys, sql_statement) -> None:
+        """
+        groupby_keys (list): Keys to perform the group by operation on.
+        orderby_keys (list of tuples): Keys to order by. Add "desc" as second element of tuple to order in descending order.
+        sql_statement (string): aggregation statement in SQL
+        """
         assert type(groupby_keys) == list
         if orderby_keys is not None:
             assert type(orderby_keys) == list
         if len(groupby_keys) > 0:
             self.agg_clause = "select " + ",".join(groupby_keys) + ", " + sql_statement + " from batch_arrow"
         else:
             self.agg_clause = "select " + sql_statement + " from batch_arrow"
@@ -436,14 +506,15 @@
                 else:
                     self.agg_clause += key + ","
             self.agg_clause = self.agg_clause[:-1]
         
         self.state = None
     
     def execute(self, batches, stream_id, executor_id):
+        
         batch = pa.concat_tables(batches)
         self.state = batch if self.state is None else pa.concat_tables([self.state, batch])
 
     def done(self, executor_id):
         if self.state is None:
             return None
         con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
```

## pyquokka/executors/vector_executors.py

```diff
@@ -14,22 +14,40 @@
         self.vec_col = vec_col
         query_vecs = np.stack(query_df[query_vec_col].to_numpy())
         self.query_df = query_df
         self.query_vecs = query_vecs / np.linalg.norm(query_vecs, axis = 1, keepdims = True)
         self.k = k
 
     def execute(self, batches, stream_id, executor_id):
+
+        start = time.time()
         batch = pa.concat_tables(batches)
         vectors = np.stack(batch[self.vec_col].to_numpy())
         normalized_vectors = vectors / np.linalg.norm(vectors, axis = 1, keepdims = True)
+
         with threadpool_limits(limits=8, user_api='blas'):
             distances = np.dot(normalized_vectors, self.query_vecs.T)
         indices = np.argsort(distances, axis = 0)[-self.k:].T.flatten()
-        print(indices)
-        # you could be smarter here and keep track of separate candidate sets for each probe, but let's leave this for an intern.
+
+        # import torch
+        # torch.set_num_threads(8)
+        # B = len(batch)
+        # all_indices = []
+
+        # for i in range(0, len(batch), B):
+        #     vectors = torch.from_numpy(np.stack(batch[self.vec_col].to_numpy()[i: i + B])).cuda().half()
+        #     normalized_vectors = vectors / vectors.norm(dim=1).unsqueeze(1)
+        #     query_vecs = torch.from_numpy(self.query_vecs).cuda().half().T
+        #     distances = torch.matmul(normalized_vectors, query_vecs)
+        #     indices = torch.topk(distances, self.k, dim = 0).indices.T.flatten().cpu().numpy()
+
+        #     all_indices.append(indices)
+            # you could be smarter here and keep track of separate candidate sets for each probe, but let's leave this for an intern.
+        # return batch.take(np.concatenate(all_indices))
+        
         return batch.take(indices)
     
     def done(self, executor_id):
         return
 
 
 class DFProbeDataStreamNNExecutor2(Executor):
```

## Comparing `pyquokka-0.3.0.dist-info/LICENSE` & `pyquokka-0.3.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyquokka-0.3.0.dist-info/METADATA` & `pyquokka-0.3.1.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyquokka
-Version: 0.3.0
+Version: 0.3.1
 Summary: Quokka
 Author: Tony Wang
 Author-email: zihengw@stanford.edu
 License: http://www.apache.org/licenses/LICENSE-2.0
 Keywords: python
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
@@ -25,15 +25,14 @@
 Requires-Dist: polars (>=0.17.0)
 Requires-Dist: sqlglot (>=11.4.2)
 Requires-Dist: graphviz
 Requires-Dist: tqdm
 Requires-Dist: aiohttp
 Requires-Dist: botocore
 Requires-Dist: threadpoolctl
-Requires-Dist: parallel-ssh
 
 
 Dope way to do cloud analytics
 
 Check out https://github.com/marsupialtail/quokka
 
 or https://marsupialtail.github.io/quokka/
```

## Comparing `pyquokka-0.3.0.dist-info/RECORD` & `pyquokka-0.3.1.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,40 +1,48 @@
 pyquokka/__init__.py,sha256=pBsjKUKy3Fdy47TuKyvWUGKAppifFP4F64l_ZuSwV80,309
 pyquokka/catalog.py,sha256=XczH9nRAjYE0eWvPdoI_sQE1N85vflPouzjpIe0lqOA,4291
-pyquokka/common_startup.sh,sha256=Rst9lyiO_Fx34swl6_-Z8w7lRHQM-8GDhsUFCbS_kP8,372
+pyquokka/common_startup.sh,sha256=km_6KjJKJtmTovnLdKjUMeg3uuvuLmSuPQhlpYpw0xs,372
 pyquokka/coordinator.py,sha256=s7yjNB_F6bEoWoko_5cTfN0BAyH9AcYm2PKIbqJexhI,28239
-pyquokka/core.py,sha256=lvMrsMc11UTuDE29eri3m4FDHq1wxxVj5n2WQRTPR3k,48407
-pyquokka/dataset.py,sha256=C9khUFsw2uvqDrw066Sc9nXS_JiRcY22JbJRcocpjyI,49168
-pyquokka/datastream.py,sha256=3nR-fmvqIYB0-9Iy1Zp9RjZzjP7VmKFOSEDdsEpVSxI,98957
+pyquokka/core.py,sha256=mJ7LW5GRsnccv_Zy7DrVyo3zUj4XozKG-yUn2bZQ9u8,48433
+pyquokka/dataset.py,sha256=8oE4Z0uHmnnbFlqxvg2buvQtlp--j_QiHkgGEhsgTH8,51863
+pyquokka/datastream.py,sha256=qeqpWiI9tVcJbfec41uE6f-KhzAWu31B-Z3zB3EjO0s,101812
 pyquokka/debugger.py,sha256=Yi1CqGHbV2y2bszUhKuxcQ561vugc6hs6xfnpJ8HIjU,1438
-pyquokka/df.py,sha256=DJXVI-kFwxs7WswQlJHRPWTHVL3JLZFO6GXvWRC57S8,81396
+pyquokka/df.py,sha256=q_LzMlihNADynd5ORFUjJxIjLGyahY7qQvHPguZUCnM,84360
+pyquokka/disk_setup.sh,sha256=4ZKHTrju2mQo-v8509xhGaEkacP_lBIzX8QYZkTLEhQ,230
 pyquokka/executors.py,sha256=dVzy3_WbsMtbAWay2UlEgE1GmxEVbT6Fz1X9i2elboY,37360
 pyquokka/expression.py,sha256=SmJxvGrSgW3ra8EU5SLGbQ8AxbJ2bPxKHLvbXpQ-8WY,12657
 pyquokka/flight.py,sha256=ri1aEXtn4s2_ACD8PfU5EIy_fA7FlNrM61WuUjQ9Pl8,17483
 pyquokka/hbq.py,sha256=V3nE2IcIIclxxdDyXRdzuV7_y_DRa90B_aVaa6aU9tg,3098
 pyquokka/ldb.so,sha256=SO70uhXN219Ns5RNIEePv1fs0qRSksc6yq2eN0tucw8,368480
 pyquokka/leader_start_ray.sh,sha256=vyZi-Utmj7r8Qr43YmOQq3uIuAN5Nn4F61MIszzdit4,274
 pyquokka/leader_startup.sh,sha256=yOP5vjuLS9D2WtcozcFewXQB84p8jm8IcyltYcuWNss,619
-pyquokka/logical.py,sha256=ydgvSN800xiuuy41p9pqpBe4m3DbvVtiP-k_h8LTjvg,30992
-pyquokka/orderedstream.py,sha256=ciirQMgndCbWOurDGtaZBD0TRF8txdnNB0cIad_wxrc,9100
-pyquokka/placement_strategy.py,sha256=KX1hEDHCTBUfUoQ64Zv5e7iVjTKRh4oAFFs03HIxDqQ,886
+pyquokka/logical.py,sha256=1qv8igMqQT_GWk9eUQdMzAO2LervMD7d3D_XbHARSeI,31410
+pyquokka/orderedstream.py,sha256=Kg7h8yimNlqQX_KX8c-fbtrYPD9qoUyoSpwwcGB1lN4,8530
+pyquokka/placement_strategy.py,sha256=0ISmKzf5d4Hxs8RvNXWyvPSCuybIsNxG8nbJgBV6NRk,1009
 pyquokka/quokka_dataset.py,sha256=_b7L8zWCimtiR3kMbCZWLH4g-w3SZ-J8gw8bjqhv-lA,3717
-pyquokka/quokka_runtime.py,sha256=LG8kBwWlR20AkVZBHyRcP280qwKr_NAB-TFLyIvinv0,19735
+pyquokka/quokka_runtime.py,sha256=D8FdK_4504DWO_YDsaa8rFn5FbKM_5d_07s4Ichrmaw,20581
 pyquokka/redis.conf,sha256=Hk0GU-BnDDpMZ6Gmit1-Ct_iyO7ttCvzyfz5PLFVkJY,93718
-pyquokka/sql.py,sha256=npTL6_m57gP9NRzLX9SYGP74_Vy0jhy5zR4zo0LINN0,10625
+pyquokka/sql.py,sha256=T5YcwnIElhQa3s9xPqR6D9P3B4o8Fz7ctJlKWNQh9i4,15525
 pyquokka/sql_utils.py,sha256=1vWMrQft37so5KtOLlsqLk8MpaFB0XO9Ib9XwQEuw90,16471
 pyquokka/state.py,sha256=wGt5uh_ZS-xV-HqVgWmdTZUnQoabpvSOHiQXOejg7L4,2371
 pyquokka/tables.py,sha256=58vsFPBKOEKZ-7Ei7GbhXKsAgw6JguwdfQhayRJrjEI,11695
 pyquokka/target_info.py,sha256=RVldOYWGgxv9YXMSN3J3S-UbSEJxeUMoYY1LcHT3edg,2351
 pyquokka/task.py,sha256=_O_itxDxzXnjojys8___CKSaApTnEpUxVrAvWaU-AVI,5752
-pyquokka/utils.py,sha256=sn_Rs0BIGfkmtN8rNfWGOSKJ_1btLTDk8-SW9SazJQ0,39151
+pyquokka/utils.py,sha256=RI7h-D1R2qP8B1zfGqpm346cRiEvo9b0i8bqfIuV9oU,40544
 pyquokka/windowtypes.py,sha256=zPh9QgwSQ3E00eNQM8jk2iQZNMuzAQ3eoaFDm1H5NGk,4081
-pyquokka/executors/__init__.py,sha256=48lCF53HMa4Od5yyQfV8T2X0aLL5q9LZ0XVO_0-_MtA,146
+pyquokka/dataset/__init__.py,sha256=n2qlwH9kIaV9FXeK9yrFGTomb_XXdJEGcRMdBCq8Unw,1231
+pyquokka/dataset/base_dataset.py,sha256=lx8vlk6VI0jXpofm69iq-aOgooc8MQwXC5pRE78s1lg,652
+pyquokka/dataset/crypto_dataset.py,sha256=gn58ljIXLJs7pt2NIuNnAcDzRDgjotntcek3w0IGSU8,3553
+pyquokka/dataset/ordered_readers.py,sha256=vkOsw0HGdJJLyt4vWevQ8cLNawcxCL0obRPMhSy2TH4,11209
+pyquokka/dataset/unordered_readers.py,sha256=w_aFkXgTmTJnpg6P9oL7-s1EtnXiVt30hN0Kp-Uupxk,36345
+pyquokka/executors/__init__.py,sha256=jGnveyusBOz39mkcih7C0eN1WlOudDZ2sTSEiuMvs_s,192
 pyquokka/executors/base_executor.py,sha256=clVkP0wf03OMG7x9vMdzRPTviT5DE6rQv3n6E81bZM4,811
-pyquokka/executors/sql_executors.py,sha256=L9y3_mXtWXON1IsSHzuhY_kyyf3cit0CSisG8cz5NzU,17620
+pyquokka/executors/cep_executors.py,sha256=9eqdDZbvvcu5WO2Nu_WlcK4wjXJkscfRr2pNw-fxNZM,8198
+pyquokka/executors/sql_executors.py,sha256=DG9AXg1Zl3XsBzKeUfXQxu9Q3zLk9Cb1RMwoj-FsLck,20679
 pyquokka/executors/ts_executors.py,sha256=KPRjeMap_PogESYfqYpCBrLyVgRKVrGxWQYICQRehEo,17185
-pyquokka/executors/vector_executors.py,sha256=M_yRDAJkQvVC-IeDDWLTczabcFAvMqWKCPiCLn_W9dc,4503
-pyquokka-0.3.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-pyquokka-0.3.0.dist-info/METADATA,sha256=IygPZwrh1a1nuy1PXMGzIBtxaPtbBKYGrYCwGG6Z9Is,999
-pyquokka-0.3.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-pyquokka-0.3.0.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
-pyquokka-0.3.0.dist-info/RECORD,,
+pyquokka/executors/vector_executors.py,sha256=etRjJ93u0l5h4hwUeIUdUcLKgmWJhtITLzBJQZMm_Qc,5209
+pyquokka-0.3.1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+pyquokka-0.3.1.dist-info/METADATA,sha256=WkPIEDmg5ffMQ6Alasq-Xid5GMEyAiqgcJ06l9T4oJ4,971
+pyquokka-0.3.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+pyquokka-0.3.1.dist-info/entry_points.txt,sha256=EuS_cN6o5rCqK6wFXMnnaQBY-2jSlNWpZxP41mHMbAU,53
+pyquokka-0.3.1.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
+pyquokka-0.3.1.dist-info/RECORD,,
```

