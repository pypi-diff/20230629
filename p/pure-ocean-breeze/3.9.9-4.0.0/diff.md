# Comparing `tmp/pure_ocean_breeze-3.9.9.tar.gz` & `tmp/pure_ocean_breeze-4.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pure_ocean_breeze-3.9.9.tar", last modified: Mon Jun 26 01:28:01 2023, max compression
+gzip compressed data, was "pure_ocean_breeze-4.0.0.tar", last modified: Thu Jun 29 13:51:40 2023, max compression
```

## Comparing `pure_ocean_breeze-3.9.9.tar` & `pure_ocean_breeze-4.0.0.tar`

### file list

```diff
@@ -1,119 +1,119 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.567464 pure_ocean_breeze-3.9.9/
--rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     2491 2023-06-26 01:28:01.567464 pure_ocean_breeze-3.9.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     2045 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/
--rw-r--r--   0 runner    (1001) docker     (123)     7858 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    31742 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/dicts.py
--rw-r--r--   0 runner    (1001) docker     (123)    31577 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/read_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    68253 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    50076 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/write_data.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/
--rw-r--r--   0 runner    (1001) docker     (123)      188 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    23637 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/half_way.py
--rw-r--r--   0 runner    (1001) docker     (123)     9307 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/in_thoughts.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/initialize/
--rw-r--r--   0 runner    (1001) docker     (123)      114 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/initialize/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3334 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/initialize/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/
--rw-r--r--   0 runner    (1001) docker     (123)      162 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13461 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/comment.py
--rw-r--r--   0 runner    (1001) docker     (123)   228559 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/process.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/
--rw-r--r--   0 runner    (1001) docker     (123)       79 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      250 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2524 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/initialize.py
--rw-r--r--   0 runner    (1001) docker     (123)   192825 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/pure_ocean_breeze.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/
--rw-r--r--   0 runner    (1001) docker     (123)      300 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2524 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/initialize.py
--rw-r--r--   0 runner    (1001) docker     (123)    85655 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/pure_ocean_breeze.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4443 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/initialize.py
--rw-r--r--   0 runner    (1001) docker     (123)   314704 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/pure_ocean_breeze.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/
--rw-r--r--   0 runner    (1001) docker     (123)     7373 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    29395 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/dicts.py
--rw-r--r--   0 runner    (1001) docker     (123)    18816 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/read_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     8320 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    43178 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/write_data.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/
--rw-r--r--   0 runner    (1001) docker     (123)      188 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7883 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/half_way.py
--rw-r--r--   0 runner    (1001) docker     (123)     9310 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/in_thoughts.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/initialize/
--rw-r--r--   0 runner    (1001) docker     (123)      114 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/initialize/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3923 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/initialize/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/
--rw-r--r--   0 runner    (1001) docker     (123)      162 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11017 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/comment.py
--rw-r--r--   0 runner    (1001) docker     (123)   138094 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/process.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.559464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/mail/
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/mail/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/mail/email.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/
--rw-r--r--   0 runner    (1001) docker     (123)      283 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/homeplace.py
--rw-r--r--   0 runner    (1001) docker     (123)      298 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/states.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/withs/
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/withs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1360 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/withs/requires.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/
--rw-r--r--   0 runner    (1001) docker     (123)     7288 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/
--rw-r--r--   0 runner    (1001) docker     (123)      528 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    29805 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/dicts.py
--rw-r--r--   0 runner    (1001) docker     (123)    23214 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/read_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    29153 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    45478 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/write_data.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/
--rw-r--r--   0 runner    (1001) docker     (123)      228 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8366 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/half_way.py
--rw-r--r--   0 runner    (1001) docker     (123)     9347 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/in_thoughts.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/initialize/
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/initialize/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3334 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/initialize/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/
--rw-r--r--   0 runner    (1001) docker     (123)      242 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10739 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/comment.py
--rw-r--r--   0 runner    (1001) docker     (123)   193541 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/process.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/mail/
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/mail/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/mail/email.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/homeplace.py
--rw-r--r--   0 runner    (1001) docker     (123)      777 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/states.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/withs/
--rw-r--r--   0 runner    (1001) docker     (123)      128 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/withs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1635 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/withs/requires.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.563464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/mail/
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/mail/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/mail/email.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.567464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/
--rw-r--r--   0 runner    (1001) docker     (123)      283 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3225 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)      904 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/homeplace.py
--rw-r--r--   0 runner    (1001) docker     (123)      777 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/states.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.567464 pure_ocean_breeze-3.9.9/pure_ocean_breeze/withs/
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/withs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze/withs/requires.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 01:28:01.555464 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     2491 2023-06-26 01:28:01.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     4216 2023-06-26 01:28:01.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-26 01:28:01.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      347 2023-06-26 01:28:01.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       18 2023-06-26 01:28:01.000000 pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-26 01:28:01.567464 pure_ocean_breeze-3.9.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-06-26 01:27:52.000000 pure_ocean_breeze-3.9.9/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/
+-rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     2522 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     2076 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.793793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/
+-rw-r--r--   0 runner    (1001) docker     (123)     7858 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.793793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31919 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/dicts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31577 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/read_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    66656 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53679 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/write_data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.793793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23637 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/half_way.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9307 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/in_thoughts.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.793793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/initialize/
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/initialize/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3907 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/initialize/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13461 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/comment.py
+-rw-r--r--   0 runner    (1001) docker     (123)   263427 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/process.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/
+-rw-r--r--   0 runner    (1001) docker     (123)       79 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      250 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2524 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/initialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)   192825 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/pure_ocean_breeze.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2524 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/initialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)    85655 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/pure_ocean_breeze.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4443 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/initialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)   314704 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/pure_ocean_breeze.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.797793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/
+-rw-r--r--   0 runner    (1001) docker     (123)     7373 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29395 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/dicts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18816 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/read_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8320 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43178 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/write_data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7883 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/half_way.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9310 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/in_thoughts.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/initialize/
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/initialize/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3923 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/initialize/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11017 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/comment.py
+-rw-r--r--   0 runner    (1001) docker     (123)   138094 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/process.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/mail/
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/mail/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/mail/email.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/homeplace.py
+-rw-r--r--   0 runner    (1001) docker     (123)      298 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/states.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/withs/
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/withs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1360 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/withs/requires.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/
+-rw-r--r--   0 runner    (1001) docker     (123)     7288 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.801793 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      528 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29805 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/dicts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23214 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/read_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29153 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45478 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/write_data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/
+-rw-r--r--   0 runner    (1001) docker     (123)      228 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8366 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/half_way.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9347 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/in_thoughts.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/initialize/
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/initialize/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3334 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/initialize/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10739 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/comment.py
+-rw-r--r--   0 runner    (1001) docker     (123)   193541 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/process.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/mail/
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/mail/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/mail/email.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/homeplace.py
+-rw-r--r--   0 runner    (1001) docker     (123)      777 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/states.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/withs/
+-rw-r--r--   0 runner    (1001) docker     (123)      128 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/withs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1635 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/withs/requires.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/mail/
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/mail/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/mail/email.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3225 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)      990 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/homeplace.py
+-rw-r--r--   0 runner    (1001) docker     (123)      777 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/states.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.805794 pure_ocean_breeze-4.0.0/pure_ocean_breeze/withs/
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/withs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze/withs/requires.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-29 13:51:40.793793 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     2522 2023-06-29 13:51:40.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     4216 2023-06-29 13:51:40.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-29 13:51:40.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      354 2023-06-29 13:51:40.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       18 2023-06-29 13:51:40.000000 pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-29 13:51:40.809794 pure_ocean_breeze-4.0.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     1837 2023-06-29 13:51:29.000000 pure_ocean_breeze-4.0.0/setup.py
```

### Comparing `pure_ocean_breeze-3.9.9/LICENSE` & `pure_ocean_breeze-4.0.0/LICENSE`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/PKG-INFO` & `pure_ocean_breeze-4.0.0/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,47 @@
 Metadata-Version: 2.1
 Name: pure_ocean_breeze
-Version: 3.9.9
+Version: 4.0.0
 Summary: stock factor test
 Home-page: https://github.com/chen-001/pure_ocean_breeze.git
 Author: chenzongwei
 Author-email: winterwinter999@163.com
 License: MIT
 Project-URL: Documentation, https://chen-001.github.io/pure_ocean_breeze/
 Requires-Python: >=3
 Description-Content-Type: text/markdown
 Provides-Extra: windows
 Provides-Extra: macos
 Provides-Extra: linux
 License-File: LICENSE
 
 # pure_ocean_breeze 
-#### **ä¼—äººçš„å› å­å›æµ‹æ¡†æ¶**
+#### **ä¼—äººçš„å› å­æ¡†æ¶**
 ##### æˆ‘ä»¬çš„å£å·æ˜¯ï¼šé‡ä»·å› å­æ‰æ˜¯æœ€ç‰›çš„ï¼
 ***
 
 ### å…¨æ–°å¤§ç‰ˆæœ¬ğŸ“¢
+* v4.0.0 â€” 2023.06.28
+> å› å­æ¡†æ¶4.0ç‰ˆæœ¬æ¥å•¦ï¼é€ç¬”æ•°æ®&ä»»æ„ç§’çº§æ•°æ®æ¥å•¦ï¼
+
 * v3.0.0 â€” 2022.08.16
+
 >å›æµ‹æ¡†æ¶3.0ç‰ˆæœ¬æ¥å•¦ï¼ æ¨¡å—æ‹†åˆ†&è¯´æ˜æ–‡æ¡£æ¥å•¦ï¼[pure_ocean_breezeè¯´æ˜æ–‡æ¡£](https://chen-001.github.io/pure_ocean_breeze/)
 * v2.0.0 â€” 2022.07.12
 >å›æµ‹æ¡†æ¶2.0ç‰ˆæœ¬æ¥å•¦ï¼æ•°æ®åº“&è‡ªåŠ¨æ›´æ–°&æœ€ç»ˆå› å­åº“åŠŸèƒ½ä¸Šçº¿å•¦ï¼
 
 ### å®‰è£…&ä½¿ç”¨æŒ‡å—ğŸ¯
 1. å®‰è£…
 > ä½¿ç”¨`pip install pure_ocean_breeze`å‘½ä»¤è¿›è¡Œå®‰è£…
 2. åˆå§‹åŒ–
 >* åœ¨åˆæ¬¡å®‰è£…æ¡†æ¶æ—¶ï¼Œè¯·è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥å°†è·¯å¾„è®¾ç½®åˆ°è‡ªå·±çš„æ–‡ä»¶é‡Œ
 >* ä½¿ç”¨å¦‚ä¸‹è¯­å¥è¿›è¡Œåˆå§‹åŒ–
 >>```python
->>import pure_ocean_breeze.initialize.initialize
->>pure_ocean_breeze.initialize.initialize.initialize()
+>>import pure_ocean_breeze as p
+>>p.ini()
 >>```
 >* ç„¶åæ ¹æ®æç¤ºè¿›è¡Œæ“ä½œå³å¯
 >* è¯·æ³¨æ„è·¯å¾„ä¸è¦å†™åæ–œæ \ï¼Œè€Œè¦å†™æˆ/
 >* ç»è¿‡åˆå§‹åŒ–åï¼Œä»¥åå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸è®ºé‡å¯ç”µè„‘æˆ–è€…ç‰ˆæœ¬å‡çº§ï¼Œéƒ½ä¸ç”¨å†åˆå§‹åŒ–
 >* å¦‚æœæ›´æ¢äº†æ•°æ®åº“è·¯å¾„ï¼Œè¯·é‡æ–°åˆå§‹åŒ–
 3. æ—¥å¸¸è°ƒç”¨
 >* **å¯¼å…¥æ¡†æ¶**
```

### Comparing `pure_ocean_breeze-3.9.9/README.md` & `pure_ocean_breeze-4.0.0/README.md`

 * *Files 11% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 # pure_ocean_breeze 
-#### **ä¼—äººçš„å› å­å›æµ‹æ¡†æ¶**
+#### **ä¼—äººçš„å› å­æ¡†æ¶**
 ##### æˆ‘ä»¬çš„å£å·æ˜¯ï¼šé‡ä»·å› å­æ‰æ˜¯æœ€ç‰›çš„ï¼
 ***
 
 ### å…¨æ–°å¤§ç‰ˆæœ¬ğŸ“¢
+* v4.0.0 â€” 2023.06.28
+> å› å­æ¡†æ¶4.0ç‰ˆæœ¬æ¥å•¦ï¼é€ç¬”æ•°æ®&ä»»æ„ç§’çº§æ•°æ®æ¥å•¦ï¼
+
 * v3.0.0 â€” 2022.08.16
+
 >å›æµ‹æ¡†æ¶3.0ç‰ˆæœ¬æ¥å•¦ï¼ æ¨¡å—æ‹†åˆ†&è¯´æ˜æ–‡æ¡£æ¥å•¦ï¼[pure_ocean_breezeè¯´æ˜æ–‡æ¡£](https://chen-001.github.io/pure_ocean_breeze/)
 * v2.0.0 â€” 2022.07.12
 >å›æµ‹æ¡†æ¶2.0ç‰ˆæœ¬æ¥å•¦ï¼æ•°æ®åº“&è‡ªåŠ¨æ›´æ–°&æœ€ç»ˆå› å­åº“åŠŸèƒ½ä¸Šçº¿å•¦ï¼
 
 ### å®‰è£…&ä½¿ç”¨æŒ‡å—ğŸ¯
 1. å®‰è£…
 > ä½¿ç”¨`pip install pure_ocean_breeze`å‘½ä»¤è¿›è¡Œå®‰è£…
 2. åˆå§‹åŒ–
 >* åœ¨åˆæ¬¡å®‰è£…æ¡†æ¶æ—¶ï¼Œè¯·è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥å°†è·¯å¾„è®¾ç½®åˆ°è‡ªå·±çš„æ–‡ä»¶é‡Œ
 >* ä½¿ç”¨å¦‚ä¸‹è¯­å¥è¿›è¡Œåˆå§‹åŒ–
 >>```python
->>import pure_ocean_breeze.initialize.initialize
->>pure_ocean_breeze.initialize.initialize.initialize()
+>>import pure_ocean_breeze as p
+>>p.ini()
 >>```
 >* ç„¶åæ ¹æ®æç¤ºè¿›è¡Œæ“ä½œå³å¯
 >* è¯·æ³¨æ„è·¯å¾„ä¸è¦å†™åæ–œæ \ï¼Œè€Œè¦å†™æˆ/
 >* ç»è¿‡åˆå§‹åŒ–åï¼Œä»¥åå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸è®ºé‡å¯ç”µè„‘æˆ–è€…ç‰ˆæœ¬å‡çº§ï¼Œéƒ½ä¸ç”¨å†åˆå§‹åŒ–
 >* å¦‚æœæ›´æ¢äº†æ•°æ®åº“è·¯å¾„ï¼Œè¯·é‡æ–°åˆå§‹åŒ–
 3. æ—¥å¸¸è°ƒç”¨
 >* **å¯¼å…¥æ¡†æ¶**
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/__init__.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 ä¸€ä¸ªé‡åŒ–å¤šå› å­ç ”ç©¶çš„æ¡†æ¶ï¼ŒåŒ…å«æ•°æ®ã€å›æµ‹ã€å› å­åŠ å·¥ç­‰æ–¹é¢çš„åŠŸèƒ½
 """
 
-__updated__ = "2023-06-21 14:51:14"
-__version__ = "3.9.9"
+__updated__ = "2023-06-29 21:50:43"
+__version__ = "4.0.0"
 __author__ = "chenzongwei"
 __author_email__ = "winterwinter999@163.com"
 __url__ = "https://github.com/chen-001/pure_ocean_breeze"
 __all__ = [
     "data",
     "labor",
     "state",
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/database.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/database.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-__updated__ = "2023-05-16 11:40:07"
+__updated__ = "2023-06-28 12:29:06"
 
 import pandas as pd
 import pymysql
 from sqlalchemy import create_engine
 from sqlalchemy import FLOAT, INT, VARCHAR, BIGINT
 from loguru import logger
 import datetime
@@ -728,17 +728,22 @@
             è¿”å›çš„æ—¥æœŸæ˜¯å¦æˆä»¥100, by default False
 
         Returns
         -------
         list
             è¡¨ä¸­æ‰€æœ‰çš„æ—¥æœŸ
         """
-        df = self.get_data(
-            f"select distinct(date) from {self.database_name}.{table_name}"
-        ).sort_values("date")
+        if 'second' in table_name:
+            df = self.get_data(f"select distinct(toYYYYMMDD(date)) from {table_name}").sort_values(
+                "date"
+            )
+        else:
+            df = self.get_data(f"select distinct(date) from {table_name}").sort_values(
+                "date"
+            )
         if mul_100:
             return [i for i in list(df.date) if i != 0]
         else:
             return [int(i / 100) for i in list(df.date) if i != 0]
 
 
 class PostgreSQL(DriverOfPostgre):
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/dicts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/dicts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/read_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/read_data.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/tools.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/tools.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """
 é’ˆå¯¹ä¸€äº›ä¸å¸¸è§çš„æ–‡ä»¶æ ¼å¼ï¼Œè¯»å–æ•°æ®æ–‡ä»¶çš„ä¸€äº›å·¥å…·å‡½æ•°ï¼Œä»¥åŠå…¶ä»–æ•°æ®å·¥å…·
 """
 
-__updated__ = "2023-06-20 22:25:36"
+__updated__ = "2023-06-28 16:09:30"
 
 import os
 import pandas as pd
 import tqdm.auto
 import datetime
 import scipy.io as scio
 import numpy as np
@@ -44,81 +44,14 @@
         else:
             return False  # Other type (?)
     except NameError:
         return False
 
 
 @do_on_dfs
-@deprecation.deprecated(
-    deprecated_in="3.0",
-    removed_in="4.0",
-    current_version=__version__,
-    details="è€ƒè™‘åˆ°h5æ–‡ä»¶çš„å¤šæ ·æ€§ï¼Œ4.0ç‰ˆæœ¬å¼€å§‹å°†ä¸å†æ”¯æŒä¸€é”®è¯»å…¥h5æ–‡ä»¶",
-)
-def read_h5(path: str) -> Dict:
-    """
-    Reads a HDF5 file into a dictionary of pandas DataFrames.
-
-    Parameters
-    ----------
-    path : str
-        The path to the HDF5 file.
-
-    Returns
-    -------
-    `Dict`
-        A dictionary of pandas DataFrames.
-    """
-    res = {}
-    import h5py
-
-    a = h5py.File(path)
-    for k, v in tqdm.tqdm(list(a.items()), desc="æ•°æ®åŠ è½½ä¸­â€¦â€¦"):
-        value = list(v.values())[-1]
-        try:
-            col = [i.decode("utf-8") for i in list(list(v.values())[0])]
-        except Exception:
-            col=list(list(v.values())[0])
-        try:
-            ind = [i.decode("utf-8") for i in list(list(v.values())[1])]
-        except Exception:
-            ind=list(list(v.values())[1])
-        res[k] = pd.DataFrame(value, columns=col, index=ind)
-    return res
-
-
-@do_on_dfs
-@deprecation.deprecated(
-    deprecated_in="3.0",
-    removed_in="4.0",
-    current_version=__version__,
-    details="è€ƒè™‘åˆ°h5æ–‡ä»¶çš„å¤šæ ·æ€§ï¼Œ4.0ç‰ˆæœ¬å¼€å§‹å°†ä¸å†æ”¯æŒä¸€é”®è¯»å…¥h5æ–‡ä»¶",
-)
-def read_h5_new(path: str) -> pd.DataFrame:
-    """è¯»å–h5æ–‡ä»¶
-
-    Parameters
-    ----------
-    path : str
-        h5æ–‡ä»¶è·¯å¾„
-
-    Returns
-    -------
-    `pd.DataFrame`
-        è¯»å–å­—å…¸çš„ç¬¬ä¸€ä¸ªvalue
-    """
-    import h5py
-
-    a = h5py.File(path)
-    v = list(a.values())[0]
-    v = a[v.name][:]
-    return pd.DataFrame(v)
-
-
-@do_on_dfs
 def read_mat(path: str) -> pd.DataFrame:
     """è¯»å–matæ–‡ä»¶
 
     Parameters
     ----------
     path : str
         matæ–‡ä»¶è·¯å¾„
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/data/write_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/data/write_data.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-__updated__ = "2023-06-21 14:50:52"
+__updated__ = "2023-06-27 20:38:39"
 
 import time
 
 try:
     import rqdatac
 
     rqdatac.init()
@@ -49,14 +49,20 @@
 from sqlalchemy import FLOAT, INT, VARCHAR, BIGINT
 from tenacity import retry
 import pickledb
 import tqdm.auto
 from functools import reduce
 from typing import Union, List
 import dcube as dc
+import py7zr
+import unrar
+import zipfile
+import rarfile
+import shutil
+import chardet
 from tenacity import retry, stop_after_attempt
 import questdb.ingress as qdbing
 from pure_ocean_breeze.state.homeplace import HomePlace
 
 try:
     homeplace = HomePlace()
 except Exception:
@@ -85,14 +91,100 @@
     convert_code,
     drop_duplicates_index,
     select_max,
 )
 from pure_ocean_breeze.labor.process import pure_fama
 
 
+
+# å¾…è¡¥å……
+def database_update_second_data_to_clickhouse():
+    ...
+    
+    
+    
+def convert_tick_by_tick_data_to_parquet(file_name:str,PATH:str,delete_7z:bool=False):
+    try:
+        files = sorted(os.listdir(file_name))
+        files=[i for i in files if i[0]!='.']
+        files = [file_name + "/" + i for i in files]
+        dfs = []
+        for i in files:
+            with open(i,'rb') as f:
+                tmp = chardet.detect(f.read())
+            df = pd.read_csv(i,encoding=tmp['encoding'])
+            df.Time = file_name.split('/')[-1] + " " + df.Time
+            df.Time = pd.to_datetime(df.Time)
+            df = df.rename(
+                columns={
+                    "TranID": "tranid",
+                    "Time": "date",
+                    "Price": "price",
+                    "Volume": "money",
+                    "SaleOrderVolume": "salemoney",
+                    "BuyOrderVolume": "buymoney",
+                    "Type": "action",
+                    "SaleOrderID": "saleid",
+                    "SaleOrderPrice": "saleprice",
+                    "BuyOrderID": "buyid",
+                    "BuyOrderPrice": "buyprice",
+                }
+            )
+            df = df.assign(code=add_suffix(i.split("/")[-1].split(".")[0]))
+            dfs.append(df)
+        dfs = pd.concat(dfs)
+        dfs.to_parquet(f"{'/'.join(PATH.split('/')[:-2])}/data/{file_name.split('/')[-1]}.parquet")
+        # logger.success(f"{file_name.split('/')[-1]}çš„é€ç¬”æ•°æ®å·²ç»å†™å…¥å®Œæˆï¼")
+        shutil.rmtree(file_name + "/",True)
+        if delete_7z:
+            os.remove(file_name + ".7z")
+            
+        # logger.warning(f"{file_name.split('/')[-1]}çš„é€ç¬”æ•°æ®csvç‰ˆå·²ç»åˆ é™¤")
+    except Exception:
+        file_name=file_name+'/'+file_name.split('/')[-1]
+        convert_tick_by_tick_data_to_parquet(file_name,PATH)
+        
+        
+def convert_tick_by_tick_data_daily(day_path:str,PATH:str):
+    try:
+        olds=os.listdir('/Volumes/My Passport/data/')
+        theday=day_path.split('/')[-1].split('.')[0]
+        olds_ok=[i for i in olds if theday in i]
+        if len(olds_ok)==0:
+            if os.path.exists(day_path.split('.')[0]):
+                ...
+                # print(f"{day_path.split('.')[0]}å·²å­˜åœ¨")
+            elif day_path.endswith('.7z'):
+                archive = py7zr.SevenZipFile(day_path, mode='r')
+                archive.extractall(path='/'.join(day_path.split('/')[:-1]))
+                archive.close()
+            elif day_path.endswith('.zip'):
+                f = zipfile.ZipFile(day_path,'r') # å‹ç¼©æ–‡ä»¶ä½ç½®
+                f.extractall('/'.join(day_path.split('/')[:-1]))             # è§£å‹ä½ç½®
+                f.close()
+            elif day_path.endswith('.rar'):
+                f=rarfile.RarFile(day_path,'r')
+                f.extractall('/'.join(day_path.split('/')[:-1]))
+                f.close()
+            convert_tick_by_tick_data_to_parquet(day_path.split('.')[0],PATH)
+        else:
+            print(f'{theday}å·²ç»æœ‰äº†ï¼Œè·³è¿‡')
+    except Exception:
+        logger.error(f'{day_path}å‡ºé”™äº†ï¼Œè¯·å½“å¿ƒï¼ï¼ï¼')
+        
+
+def convert_tick_by_tick_data_monthly(month_path:str,PATH:str):
+    files=os.listdir(month_path)
+    files=[i for i in files if i.startswith('20')]
+    date=month_path.split('/')[-1]
+    files=[month_path+'/'+i for i in files] # æ¯ä¸ªå½¢å¦‚2018-01-02.7z
+    for i in tqdm.auto.tqdm(files,f'{date}çš„è¿›åº¦'):
+        convert_tick_by_tick_data_daily(i,PATH)
+        
+        
 def database_update_minute_data_to_clickhouse_and_questdb(
     kind: str, web_port: str = "9001"
 ) -> None:
     """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³clickhouseå’Œquestdbä¸­
 
     Parameters
     ----------
@@ -552,15 +644,15 @@
         partpe_new = partpe_new[sorted(list(partpe_new.columns))]
         partpe_new = drop_duplicates_index(partpe_new)
         partpe_new.to_parquet(homeplace.daily_data_file + "pe.parquet")
         logger.success("å¸‚ç›ˆç‡æ›´æ–°å®Œæˆ")
         
         # pettm
         partpe = df2s[["date", "code", "pe_ttm"]].pivot(
-            index="date", columns="code", values="pe"
+            index="date", columns="code", values="pe_ttm"
         )
         partpe_old = pd.read_parquet(homeplace.daily_data_file + "pettm.parquet")
         partpe_new = pd.concat([partpe_old, partpe])
         partpe_new = partpe_new.drop_duplicates()
         partpe_new = partpe_new[closes.columns]
         partpe_new = partpe_new[sorted(list(partpe_new.columns))]
         partpe_new = drop_duplicates_index(partpe_new)
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/half_way.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/half_way.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/future_version/in_thoughts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/future_version/in_thoughts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/initialize/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/initialize/initialize.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/comment.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/comment.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/labor/process.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/process.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,102 +1,93 @@
-__updated__ = "2023-06-26 09:24:10"
+__updated__ = "2022-11-05 00:16:56"
 
 import warnings
 
 warnings.filterwarnings("ignore")
 import numpy as np
 import pandas as pd
 import knockknock as kk
 import os
-import tqdm.auto
+import tqdm
 import scipy.stats as ss
-from scipy.optimize import linprog
 import statsmodels.formula.api as smf
 import matplotlib as mpl
 
 mpl.rcParams.update(mpl.rcParamsDefault)
 import matplotlib.pyplot as plt
 
+plt.style.use(["science", "no-latex", "notebook"])
 plt.rcParams["axes.unicode_minus"] = False
 
 from functools import reduce, lru_cache, partial
 from dateutil.relativedelta import relativedelta
 from loguru import logger
 import datetime
 from collections.abc import Iterable
 import plotly.express as pe
 import plotly.io as pio
 from plotly.tools import FigureFactory as FF
+import plotly.graph_objects as go
+import plotly.tools as plyoo
 import pyfinance.ols as po
 from texttable import Texttable
 from xpinyin import Pinyin
-import tradetime as tt
 import cufflinks as cf
-import deprecation
-from mpire import WorkerPool
-from scipy.optimize import minimize
-from pure_ocean_breeze import __version__
 
 cf.set_config_file(offline=True)
-from typing import Callable, Union, Dict, List, Tuple
-from pure_ocean_breeze.data.read_data import (
+from typing import Callable, Union
+from pure_ocean_breeze.legacy_version.v3p4.data.read_data import (
     read_daily,
     read_market,
     get_industry_dummies,
     read_swindustry_prices,
     read_zxindustry_prices,
     database_read_final_factors,
-    read_index_single,
 )
-from pure_ocean_breeze.state.homeplace import HomePlace
+from pure_ocean_breeze.legacy_version.v3p4.state.homeplace import HomePlace
 
-try:
-    homeplace = HomePlace()
-except Exception:
-    print("æ‚¨æš‚æœªåˆå§‹åŒ–ï¼ŒåŠŸèƒ½å°†å—é™")
-from pure_ocean_breeze.state.states import STATES
-from pure_ocean_breeze.state.decorators import do_on_dfs
-from pure_ocean_breeze.data.database import *
-from pure_ocean_breeze.data.dicts import INDUS_DICT
-from pure_ocean_breeze.data.tools import (
+homeplace = HomePlace()
+from pure_ocean_breeze.legacy_version.v3p4.state.states import STATES, is_notebook
+from pure_ocean_breeze.legacy_version.v3p4.data.database import *
+from pure_ocean_breeze.legacy_version.v3p4.data.dicts import INDUS_DICT
+from pure_ocean_breeze.legacy_version.v3p4.data.tools import (
     indus_name,
     drop_duplicates_index,
     to_percent,
     to_group,
-    standardlize,
-    select_max,
-    select_min,
 )
-from pure_ocean_breeze.labor.comment import (
+from pure_ocean_breeze.legacy_version.v3p4.labor.comment import (
     comments_on_twins,
     make_relative_comments,
     make_relative_comments_plot,
 )
 
 
-@do_on_dfs
 def daily_factor_on300500(
     fac: pd.DataFrame,
     hs300: bool = 0,
     zz500: bool = 0,
+    zz800: bool = 0,
     zz1000: bool = 0,
     gz2000: bool = 0,
     other: bool = 0,
 ) -> pd.DataFrame:
     """è¾“å…¥æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå°†å…¶é™å®šåœ¨æŸæŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨æ± å†…ï¼Œ
-    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œä»¥åŠè¿™å››ç§æŒ‡æ•°æˆåˆ†è‚¡çš„ç»„åˆå åŠ ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡
+    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯800ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡
 
     Parameters
     ----------
     fac : pd.DataFrame
         æœªé™å®šè‚¡ç¥¨æ± çš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     hs300 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºæ²ªæ·±300, by default 0
     zz500 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯500, by default 0
+    zz800 : bool, optional
+        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯800, by default 0
     zz1000 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯1000, by default 0
     gz2000 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºå›½è¯2000, by default 0
     other : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡, by default 0
 
@@ -108,105 +99,142 @@
     Raises
     ------
     `ValueError`
         å¦‚æœæœªæŒ‡å®šä»»ä½•ä¸€ç§æŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œå°†æŠ¥é”™
     """
     last = fac.resample("M").last()
     homeplace = HomePlace()
-    dummies = []
     if fac.shape[0] / last.shape[0] > 2:
         if hs300:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
-            dummies.append(df)
-        if zz500:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
-            dummies.append(df)
-        if zz1000:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
-            dummies.append(df)
-        if gz2000:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
-            dummies.append(df)
-        if other:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz500:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz800:
+            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
+            df1 = df1.set_index(list(df1.columns)[0])
+            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
+            df2 = df2.set_index(list(df2.columns)[0])
+            df = df1 + df2
+            df = df.replace(0, np.nan)
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz1000:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
+            df = df * fac
+            df = df.dropna(how="all")
+        elif gz2000:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
+            df = df * fac
+            df = df.dropna(how="all")
+        elif other:
             tr = read_daily(tr=1).fillna(0).replace(0, 1)
             tr = np.sign(tr)
             df1 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df1 = df1.set_index(list(df1.columns)[0])
             df2 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df2 = df2.set_index(list(df2.columns)[0])
             df3 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df3 = df3.set_index(list(df3.columns)[0])
             df = (1 - df1) * (1 - df2) * (1 - df3) * tr
             df = df.replace(0, np.nan) * fac
             df = df.dropna(how="all")
-        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
+        else:
             raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
     else:
         if hs300:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
+            df = pd.read_feather(
+                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
             df = df.resample("M").last()
-            dummies.append(df)
-        if zz500:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz500:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
             df = df.resample("M").last()
-            dummies.append(df)
-        if zz1000:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz800:
+            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
+            df1 = df1.set_index(list(df1.columns)[0])
+            df1 = df1.resample("M").last()
+            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
+            df2 = df2.set_index(list(df2.columns)[0])
+            df2 = df2.resample("M").last()
+            df = df1 + df2
+            df = df.replace(0, np.nan)
+            df = df * fac
+            df = df.dropna(how="all")
+        elif zz1000:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
             df = df.resample("M").last()
-            dummies.append(df)
-        if gz2000:
-            df = pd.read_parquet(
-                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
-            ).fillna(0)
+            df = df * fac
+            df = df.dropna(how="all")
+        elif gz2000:
+            df = pd.read_feather(
+                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
+            ).replace(0, np.nan)
+            df = df.set_index(list(df.columns)[0])
             df = df.resample("M").last()
-            dummies.append(df)
-        if other:
+            df = df * fac
+            df = df.dropna(how="all")
+        elif other:
             tr = read_daily(tr=1).fillna(0).replace(0, 1).resample("M").last()
             tr = np.sign(tr)
             df1 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df1 = df1.set_index(list(df1.columns)[0])
             df1 = df1.resample("M").last()
             df2 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df2 = df2.set_index(list(df2.columns)[0])
             df2 = df2.resample("M").last()
             df3 = (
-                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
+                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
             ).fillna(0)
+            df3 = df3.set_index(list(df3.columns)[0])
             df3 = df3.resample("M").last()
             df = (1 - df1) * (1 - df2) * (1 - df3)
             df = df.replace(0, np.nan) * fac
             df = df.dropna(how="all")
-        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
+        else:
             raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
-    if len(dummies) > 0:
-        dummies = sum(dummies).replace(0, np.nan)
-        df = (dummies * fac).dropna(how="all")
     return df
 
 
-@do_on_dfs
 def daily_factor_on_industry(
     df: pd.DataFrame, swindustry: bool = 0, zxindustry: bool = 0
 ) -> dict:
     """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨
 
     Parameters
     ----------
@@ -237,33 +265,29 @@
         swindustry=swindustry,
         zxindustry=zxindustry,
     )
     ress = {k: v * df for k, v in ress.items()}
     return ress
 
 
-@do_on_dfs
 def group_test_on_industry(
     df: pd.DataFrame,
     group_num: int = 10,
-    trade_cost_double_side: float = 0,
     net_values_writer: pd.ExcelWriter = None,
     swindustry: bool = 0,
     zxindustry: bool = 0,
 ) -> pd.DataFrame:
     """åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•æ¯ä¸ªè¡Œä¸šçš„åˆ†ç»„å›æµ‹
 
     Parameters
     ----------
     df : pd.DataFrame
         å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
     group_num : int, optional
         åˆ†ç»„æ•°é‡, by default 10
-    trade_cost_double_side : float, optional
-        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
     net_values_writer : pd.ExcelWriter, optional
         ç”¨äºå­˜å‚¨å„ä¸ªè¡Œä¸šåˆ†ç»„åŠå¤šç©ºå¯¹å†²å‡€å€¼åºåˆ—çš„excelæ–‡ä»¶, by default None
     swindustry : bool, optional
         é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
     zxindustry : bool, optional
         é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
 
@@ -277,41 +301,38 @@
     ks = []
     vs = []
     if swindustry:
         for k, v in dfs.items():
             shen = pure_moonnight(
                 v,
                 groups_num=group_num,
-                trade_cost_double_side=trade_cost_double_side,
                 net_values_writer=net_values_writer,
                 sheetname=INDUS_DICT[k],
                 plt_plot=0,
             )
             ks.append(k)
             vs.append(shen.shen.total_comments.T)
         vs = pd.concat(vs)
         vs.index = [INDUS_DICT[i] for i in ks]
     else:
         for k, v in dfs.items():
             shen = pure_moonnight(
                 v,
                 groups_num=group_num,
-                trade_cost_double_side=trade_cost_double_side,
                 net_values_writer=net_values_writer,
                 sheetname=k,
                 plt_plot=0,
             )
             ks.append(k)
             vs.append(shen.shen.total_comments.T)
         vs = pd.concat(vs)
         vs.index = ks
     return vs
 
 
-@do_on_dfs
 def rankic_test_on_industry(
     df: pd.DataFrame,
     excel_name: str = "è¡Œä¸šrankic.xlsx",
     png_name: str = "è¡Œä¸šrankicå›¾.png",
     swindustry: bool = 0,
     zxindustry: bool = 0,
 ) -> pd.DataFrame:
@@ -333,32 +354,30 @@
     Returns
     -------
     pd.DataFrame
         è¡Œä¸šåç§°ä¸å¯¹åº”çš„Rank IC
     """
     vs = group_test_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)
     rankics = vs[["RankIC"]].T
-    if excel_name is not None:
-        rankics.to_excel(excel_name)
+    rankics.to_excel(excel_name)
     rankics.plot(kind="bar")
     plt.show()
     plt.savefig(png_name)
     return rankics
 
 
-@do_on_dfs
 def long_test_on_industry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
-) -> List[dict]:
+) -> list[dict]:
     """å¯¹æ¯ä¸ªç”³ä¸‡/ä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -371,32 +390,32 @@
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     swindustry : bool, optional
         åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
     zxindusrty : bool, optional
         åœ¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
     Returns
     -------
-    List[dict]
+    list[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
     fac = decap_industry(df, monthly=True)
 
     if swindustry:
-        industry_dummy = pd.read_parquet(
-            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
+        industry_dummy = pd.read_feather(
+            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
         ).fillna(0)
         indus = read_swindustry_prices()
     else:
-        industry_dummy = pd.read_parquet(
-            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.parquet"
+        industry_dummy = pd.read_feather(
+            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.feather"
         ).fillna(0)
         indus = read_zxindustry_prices()
     inds = list(industry_dummy.columns)
     ret_next = (
         read_daily(close=1).resample("M").last()
         / read_daily(open=1).resample("M").first()
         - 1
@@ -431,18 +450,24 @@
         fi = fi.T.apply(sing).T
         fi = fi.replace(0, np.nan)
         fi = fi * ret_next
         ret_long = fi.mean(axis=1)
         return ret_long
 
     ret_longs = {k: [] for k in nums}
-    for num in tqdm.auto.tqdm(nums):
-        for code in inds[2:]:
-            df = save_ind(code, num).to_frame(code)
-            ret_longs[num] = ret_longs[num] + [df]
+    if is_notebook():
+        for num in tqdm.tqdm_notebook(nums):
+            for code in inds[2:]:
+                df = save_ind(code, num).to_frame(code)
+                ret_longs[num] = ret_longs[num] + [df]
+    else:
+        for num in tqdm.tqdm(nums):
+            for code in inds[2:]:
+                df = save_ind(code, num).to_frame(code)
+                ret_longs[num] = ret_longs[num] + [df]
 
     indus = indus.resample("M").last().pct_change()
 
     if swindustry:
         coms = {
             k: indus_name(pd.concat(v, axis=1).dropna(how="all").T).T
             for k, v in ret_longs.items()
@@ -523,18 +548,22 @@
                     raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
                 return tuple(thr.index)
 
             fi = fi.T.apply(sing)
             return fi
 
         stocks_longs = {k: {} for k in nums}
-
-        for num in tqdm.auto.tqdm(nums):
-            for code in inds[2:]:
-                stocks_longs[num][code] = save_ind_stocks(code, num)
+        if is_notebook():
+            for num in tqdm.tqdm_notebook(nums):
+                for code in inds[2:]:
+                    stocks_longs[num][code] = save_ind_stocks(code, num)
+        else:
+            for num in tqdm.tqdm(nums):
+                for code in inds[2:]:
+                    stocks_longs[num][code] = save_ind_stocks(code, num)
 
         for num in nums:
             w1 = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šä¹°{num}åªçš„è‚¡ç¥¨åå•.xlsx")
             for k, v in stocks_longs[num].items():
                 v = v.T
                 v.index = v.index.strftime("%Y/%m/%d")
                 v.to_excel(w1, sheet_name=INDUS_DICT[k])
@@ -542,22 +571,21 @@
             w1.close()
 
         return [coms_finals, rets_save, stocks_longs]
     else:
         return [coms_finals, rets_save]
 
 
-@do_on_dfs
 def long_test_on_swindustry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
-) -> List[dict]:
+) -> list[dict]:
     """å¯¹æ¯ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -566,15 +594,15 @@
         å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
     neg : bool, optional
         å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
     save_stock_list : bool, optional
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     Returns
     -------
-    List[dict]
+    list[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
@@ -585,22 +613,21 @@
         neg=neg,
         save_stock_list=save_stock_list,
         swindustry=1,
     )
     return res
 
 
-@do_on_dfs
 def long_test_on_zxindustry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
-) -> List[dict]:
+) -> list[dict]:
     """å¯¹æ¯ä¸ªä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -609,15 +636,15 @@
         å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
     neg : bool, optional
         å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
     save_stock_list : bool, optional
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     Returns
     -------
-    List[dict]
+    list[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
@@ -628,15 +655,14 @@
         neg=neg,
         save_stock_list=save_stock_list,
         zxindustry=1,
     )
     return res
 
 
-@do_on_dfs
 @kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
 def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
     """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–
 
     Parameters
     ----------
     df : pd.DataFrame
@@ -652,15 +678,18 @@
         å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
 
     Raises
     ------
     `NotImplementedError`
         å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
     """
-    tqdm.auto.tqdm.pandas()
+    if is_notebook():
+        tqdm.tqdm_notebook().pandas()
+    else:
+        tqdm.tqdm.pandas()
     share = read_daily(sharenum=1)
     undi_close = read_daily(close=1, unadjust=1)
     cap = (share * undi_close).stack().reset_index()
     cap.columns = ["date", "code", "cap"]
     cap.cap = ss.boxcox(cap.cap)[0]
 
     def single(x):
@@ -681,15 +710,14 @@
     elif monthly:
         df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
     else:
         raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
     return df
 
 
-@do_on_dfs
 @kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
 def decap_industry(
     df: pd.DataFrame,
     daily: bool = 0,
     monthly: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
@@ -758,49 +786,51 @@
         df.fac = df.fac - ols_w * df.cap - ols_b
         for k, v in ols_bs.items():
             df.fac = df.fac - v * df[k]
         df = df[["fac"]]
         return df
 
     if swindustry:
-        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
+        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
     else:
-        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"
+        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"
 
     if monthly:
         industry_dummy = (
-            pd.read_parquet(homeplace.daily_data_file + file_name)
+            pd.read_feather(homeplace.daily_data_file + file_name)
             .fillna(0)
             .set_index("date")
             .groupby("code")
             .resample("M")
             .last()
         )
         industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
         industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
         col = ["code", "date"] + industry_ws
     elif daily:
-        industry_dummy = pd.read_parquet(homeplace.daily_data_file + file_name).fillna(
+        industry_dummy = pd.read_feather(homeplace.daily_data_file + file_name).fillna(
             0
         )
         industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
         col = ["date", "code"] + industry_ws
     else:
         raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
     industry_dummy.columns = col
     df = pd.merge(df, industry_dummy, on=["date", "code"])
     df = df.set_index(["date", "code"])
-    tqdm.auto.tqdm.pandas()
+    if is_notebook():
+        tqdm.tqdm_notebook().pandas()
+    else:
+        tqdm.tqdm.pandas()
     df = df.groupby(["date"]).progress_apply(neutralize_factors)
     df = df.unstack()
     df.columns = [i[1] for i in list(df.columns)]
     return df
 
 
-@do_on_dfs
 def deboth(df: pd.DataFrame) -> pd.DataFrame:
     """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–
 
     Parameters
     ----------
     df : pd.DataFrame
         æœªä¸­æ€§åŒ–çš„å› å­
@@ -810,33 +840,17 @@
     `pd.DataFrame`
         è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
     """
     shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
     return shen()
 
 
-@do_on_dfs
-def boom_one(
-    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
-) -> pd.DataFrame:
-    if min_periods is None:
-        min_periods = int(backsee * 0.5)
-    if not daily:
-        df_mean = (
-            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
-        )
-    else:
-        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
-    return df_mean
-
-
-@do_on_dfs
 def boom_four(
     df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
-) -> Tuple[pd.DataFrame]:
+) -> tuple[pd.DataFrame]:
     """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­
 
     Parameters
     ----------
     df : pd.DataFrame
         åŸæ—¥é¢‘å› å­
     backsee : int, optional
@@ -844,15 +858,15 @@
     daily : bool, optional
         ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
     min_periods : int, optional
         rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ
 
     Returns
     -------
-    `Tuple[pd.DataFrame]`
+    `tuple[pd.DataFrame]`
         6ä¸ªå› å­çš„å…ƒç»„
     """
     if min_periods is None:
         min_periods = int(backsee * 0.5)
     if not daily:
         df_mean = (
             df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
@@ -868,42 +882,14 @@
         twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
         rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
         twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
         rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
     return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus
 
 
-def boom_fours(
-    dfs: List[pd.DataFrame],
-    backsee: Union[int, List[int]] = 20,
-    daily: Union[bool, List[bool]] = 0,
-    min_periods: Union[int, List[int]] = None,
-) -> List[List[pd.DataFrame]]:
-    """å¯¹å¤šä¸ªå› å­ï¼Œæ¯ä¸ªå› å­éƒ½è¿›è¡Œboom_fourçš„æ“ä½œ
-
-    Parameters
-    ----------
-    dfs : List[pd.DataFrame]
-        å¤šä¸ªå› å­çš„dataframeç»„æˆçš„list
-    backsee : Union[int,List[int]], optional
-        æ¯ä¸ªå› å­å›çœ‹æœŸæ•°, by default 20
-    daily : Union[bool,List[bool]], optional
-        æ¯ä¸ªå› å­æ˜¯å¦é€æ—¥è®¡ç®—, by default 0
-    min_periods : Union[int,List[int]], optional
-        æ¯ä¸ªå› å­è®¡ç®—çš„æœ€å°æœŸ, by default None
-
-    Returns
-    -------
-    List[List[pd.DataFrame]]
-        æ¯ä¸ªå› å­è¿›è¡Œboom_fouråçš„ç»“æœ
-    """
-    return boom_four(df=dfs, backsee=backsee, daily=daily, min_periods=min_periods)
-
-
-@do_on_dfs
 def add_cross_standardlize(*args: list) -> pd.DataFrame:
     """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 
 
     Returns
     -------
     `pd.DataFrame`
         åˆæˆåçš„å› å­
@@ -911,42 +897,37 @@
     fms = [pure_fallmount(i) for i in args]
     one = fms[0]
     others = fms[1:]
     final = one + others
     return final()
 
 
-@do_on_dfs
 def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
     """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥
 
     Parameters
     ----------
     df : pd.DataFrame
         indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©
 
     Returns
     -------
     `pd.DataFrame`
         ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
     """
     """"""
-    start = df.index.min()
-    start = start - pd.tseries.offsets.MonthBegin()
-    start = datetime.datetime.strftime(start, "%Y%m%d")
-    trs = read_daily(tr=1, start=start)
+    trs = read_daily(tr=1)
     trs = trs.assign(tradeends=list(trs.index))
     trs = trs[["tradeends"]]
     trs = trs.resample("M").last()
     df = pd.concat([trs, df], axis=1)
     df = df.set_index(["tradeends"])
     return df
 
 
-@do_on_dfs
 def market_kind(
     df: pd.DataFrame,
     zhuban: bool = 0,
     chuangye: bool = 0,
     kechuang: bool = 0,
     beijing: bool = 0,
 ) -> pd.DataFrame:
@@ -1020,41 +1001,38 @@
         è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼
 
     Returns
     -------
     `float`
         å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
     """
-    if method == "spearman":
-        corr = show_x_with_func(fac1, fac2, lambda x: x.rank().corr().iloc[0, 1])
-    else:
-        corr = show_x_with_func(fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1])
+    corr = show_x_with_func(fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1])
     if show_series:
         return corr
     else:
         if plt_plot:
             corr.plot(rot=60)
             plt.show()
         return corr.mean()
 
 
 def show_corrs(
-    factors: List[pd.DataFrame],
-    factor_names: List[str] = None,
+    factors: list[pd.DataFrame],
+    factor_names: list[str] = None,
     print_bool: bool = True,
     show_percent: bool = True,
     method: str = "spearman",
 ) -> pd.DataFrame:
     """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§
 
     Parameters
     ----------
-    factors : List[pd.DataFrame]
+    factors : list[pd.DataFrame]
         æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
-    factor_names : List[str], optional
+    factor_names : list[str], optional
         ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
     print_bool : bool, optional
         æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
     show_percent : bool, optional
         æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
     method : str, optional
         è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
@@ -1076,17 +1054,16 @@
     corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
     np.fill_diagonal(corrs.to_numpy(), 1)
     if show_percent:
         pcorrs = corrs.applymap(to_percent)
     else:
         pcorrs = corrs.copy()
     if print_bool:
-        return pcorrs
-    else:
-        return corrs
+        print(pcorrs)
+    return corrs
 
 
 def show_cov(
     fac1: pd.DataFrame,
     fac2: pd.DataFrame,
     plt_plot: bool = 1,
     show_series: bool = 0,
@@ -1149,31 +1126,34 @@
     befo1.columns = ["date", "code", "befo"]
     twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
     corr = twins.groupby("date").apply(the_func)
     return corr
 
 
 def show_covs(
-    factors: List[pd.DataFrame],
-    factor_names: List[str] = None,
+    factors: list[pd.DataFrame],
+    factor_names: list[str] = None,
     print_bool: bool = True,
     show_percent: bool = True,
+    method: str = "spearman",
 ) -> pd.DataFrame:
     """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§
 
     Parameters
     ----------
-    factors : List[pd.DataFrame]
+    factors : list[pd.DataFrame]
         æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
-    factor_names : List[str], optional
+    factor_names : list[str], optional
         ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
     print_bool : bool, optional
         æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
     show_percent : bool, optional
         æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
+    method : str, optional
+        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
 
     Returns
     -------
     `pd.DataFrame`
         ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
     """
     corrs = []
@@ -1193,166 +1173,82 @@
         pcorrs = corrs.copy()
     if print_bool:
         print(pcorrs)
     return corrs
 
 
 def de_cross(
-    y: pd.DataFrame, xs: Union[List[pd.DataFrame], pd.DataFrame]
+    y: pd.DataFrame, xs: Union[list[pd.DataFrame], pd.DataFrame]
 ) -> pd.DataFrame:
     """ä½¿ç”¨è‹¥å¹²å› å­å¯¹æŸä¸ªå› å­è¿›è¡Œæ­£äº¤åŒ–å¤„ç†
 
     Parameters
     ----------
     y : pd.DataFrame
         ç ”ç©¶çš„ç›®æ ‡ï¼Œå›å½’ä¸­çš„y
-    xs : Union[List[pd.DataFrame],pd.DataFrame]
+    xs : Union[list[pd.DataFrame],pd.DataFrame]
         ç”¨äºæ­£äº¤åŒ–çš„è‹¥å¹²å› å­ï¼Œå›å½’ä¸­çš„x
 
     Returns
     -------
     pd.DataFrame
         æ­£äº¤åŒ–ä¹‹åçš„å› å­
     """
     if not isinstance(xs, list):
         xs = [xs]
     y = pure_fallmount(y)
     xs = [pure_fallmount(i) for i in xs]
     return (y - xs)()
 
 
-@do_on_dfs
 def show_corrs_with_old(
-    df: pd.DataFrame = None, method: str = "spearman", only_new: bool = 1
+    df: pd.DataFrame = None, method: str = "spearman"
 ) -> pd.DataFrame:
     """è®¡ç®—æ–°å› å­å’Œå·²æœ‰å› å­çš„ç›¸å…³ç³»æ•°
 
     Parameters
     ----------
     df : pd.DataFrame, optional
         æ–°å› å­, by default None
     method : str, optional
         æ±‚ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default 'spearman'
-    only_new : bool, optional
-        ä»…è®¡ç®—æ–°å› å­ä¸æ—§å› å­ä¹‹é—´çš„ç›¸å…³ç³»æ•°, by default 1
 
     Returns
     -------
     pd.DataFrame
         ç›¸å…³ç³»æ•°çŸ©é˜µ
     """
     if df is not None:
         df0 = df.resample("M").last()
         if df.shape[0] / df0.shape[0] > 2:
             daily = 1
         else:
             daily = 0
-    nums = os.listdir(homeplace.final_factor_file)
-    nums = sorted(
-        set(
-            [
-                int(i.split("å¤šå› å­")[1].split("_æœˆ")[0])
-                for i in nums
-                if i.endswith("æœˆ.parquet")
-            ]
-        )
-    )
     olds = []
-    for i in nums:
+    for i in range(1, 100):
         try:
             if daily:
                 old = database_read_final_factors(order=i)[0]
             else:
                 old = database_read_final_factors(order=i)[0].resample("M").last()
             olds.append(old)
         except Exception:
             break
     if df is not None:
-        if only_new:
-            corrs = [
-                to_percent(show_corr(df, i, plt_plot=0, method=method)) for i in olds
-            ]
-            corrs = pd.Series(corrs, index=[f"old{i}" for i in nums])
-            corrs = corrs.to_frame(f"{method}ç›¸å…³ç³»æ•°").T
-        else:
-            olds = [df] + olds
-            corrs = show_corrs(olds, ["new"] + [f"old{i}" for i in nums], method=method)
+        olds = [df] + olds
+        corrs = show_corrs(
+            olds, ["new"] + [f"old{i}" for i in range(1, len(olds))], method=method
+        )
     else:
-        corrs = show_corrs(olds, [f"old{i}" for i in nums], method=method)
+        corrs = show_corrs(
+            olds, [f"old{i}" for i in range(1, len(olds))], method=method
+        )
     return corrs
 
 
-@do_on_dfs
-def remove_unavailable(df: pd.DataFrame) -> pd.DataFrame:
-    """å¯¹æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå‰”é™¤stè‚¡ã€ä¸æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨å’Œä¸Šå¸‚ä¸è¶³60å¤©çš„è‚¡ç¥¨
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-        å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
-
-    Returns
-    -------
-    pd.DataFrame
-        å‰”é™¤åçš„å› å­å€¼
-    """
-    df0 = df.resample("M").last()
-    if df.shape[0] / df0.shape[0] > 2:
-        daily = 1
-    else:
-        daily = 0
-    if daily:
-        state = read_daily(state=1).replace(0, np.nan)
-        st = read_daily(st=1)
-        age = read_daily(age=1)
-        st = (1 - st).replace(0, np.nan)
-        age = ((age >= 60) + 0).replace(0, np.nan)
-        df = df * age * st * state
-    else:
-        moon = pure_moon(no_read_indu=1)
-        moon.set_basic_data()
-        moon.judge_month()
-        df = moon.tris_monthly * df
-    return df
-
-
-class frequency_controller(object):
-    def __init__(self, freq: str):
-        self.homeplace = HomePlace()
-        self.freq = freq
-
-        if freq == "M":
-            self.counts_one_year = 12
-            self.time_shift = pd.DateOffset(months=1)
-            self.states_files = (
-                self.homeplace.daily_data_file + "states_monthly.parquet"
-            )
-            self.sts_files = self.homeplace.daily_data_file + "sts_monthly.parquet"
-            self.comment_name = "æœˆ"
-            self.days_in = 20
-        elif freq == "W":
-            self.counts_one_year = 52
-            self.time_shift = pd.DateOffset(weeks=1)
-            self.states_files = self.homeplace.daily_data_file + "states_weekly.parquet"
-            self.sts_files = self.homeplace.daily_data_file + "sts_weekly.parquet"
-            self.comment_name = "å‘¨"
-            self.days_in = 5
-        else:
-            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")
-
-    def next_end(self, x):
-        """æ‰¾åˆ°ä¸‹ä¸ªå‘¨æœŸçš„æœ€åä¸€å¤©"""
-        if self.freq == "M":
-            return x + pd.DateOffset(months=1) + pd.tseries.offsets.MonthEnd()
-        elif self.freq == "W":
-            return x + pd.DateOffset(weeks=1)
-        else:
-            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")
-
-
 class pure_moon(object):
     __slots__ = [
         "homeplace",
         "sts_monthly_file",
         "states_monthly_file",
         "factors",
         "codes",
@@ -1402,56 +1298,35 @@
         "__factors_out",
         "ics",
         "rankics",
         "factor_turnover_rates",
         "factor_turnover_rate",
         "group_rets_std",
         "group_rets_stds",
-        "group_rets_skews",
-        "group_rets_skew",
         "wind_out",
         "swindustry_dummy",
         "zxindustry_dummy",
         "closes2_monthly",
         "rets_monthly_last",
-        "freq_ctrl",
-        "freq",
-        "factor_cover",
-        "factor_cross_skew",
-        "factor_cross_skew_after_neu",
-        "pos_neg_rate",
-        "corr_itself",
-        "factor_cross_stds",
-        "corr_itself_shift2",
-        "rets_all",
-        "inner_long_ret_yearly",
-        "inner_short_ret_yearly",
-        "inner_long_net_values",
-        "inner_short_net_values",
     ]
 
     @classmethod
     @lru_cache(maxsize=None)
     def __init__(
         cls,
-        freq: str = "M",
         no_read_indu: bool = 0,
         swindustry_dummy: pd.DataFrame = None,
         zxindustry_dummy: pd.DataFrame = None,
         read_in_swindustry_dummy: bool = 0,
     ):
         cls.homeplace = HomePlace()
-        cls.freq = freq
-        cls.freq_ctrl = frequency_controller(freq)
         # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
-        # week_here
-        cls.sts_monthly_file = cls.freq_ctrl.sts_files
+        cls.sts_monthly_file = homeplace.daily_data_file + "sts_monthly.feather"
         # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
-        # week_here
-        cls.states_monthly_file = cls.freq_ctrl.states_files
+        cls.states_monthly_file = homeplace.daily_data_file + "states_monthly.feather"
 
         if swindustry_dummy is not None:
             cls.swindustry_dummy = swindustry_dummy
         if zxindustry_dummy is not None:
             cls.zxindustry_dummy = zxindustry_dummy
 
         def deal_dummy(industry_dummy):
@@ -1461,37 +1336,37 @@
             industry_dummy.columns = col
             industry_dummy = industry_dummy[
                 industry_dummy.date >= pd.Timestamp("20100101")
             ]
             return industry_dummy
 
         if (swindustry_dummy is None) and (zxindustry_dummy is None):
+
             if not no_read_indu:
                 if read_in_swindustry_dummy:
-                    # week_here
                     cls.swindustry_dummy = (
-                        pd.read_parquet(
-                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
+                        pd.read_feather(
+                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
                         )
                         .fillna(0)
                         .set_index("date")
                         .groupby("code")
-                        .resample(freq)
+                        .resample("M")
                         .last()
                     )
                     cls.swindustry_dummy = deal_dummy(cls.swindustry_dummy)
-                # week_here
+
                 cls.zxindustry_dummy = (
-                    pd.read_parquet(
-                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"
+                    pd.read_feather(
+                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"
                     )
                     .fillna(0)
                     .set_index("date")
                     .groupby("code")
-                    .resample(freq)
+                    .resample("M")
                     .last()
                     .fillna(0)
                 )
 
                 cls.zxindustry_dummy = deal_dummy(cls.zxindustry_dummy)
 
     @property
@@ -1499,83 +1374,47 @@
         return self.__factors_out
 
     def __call__(self):
         """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
         return self.factors_out
 
     @classmethod
-    @lru_cache(maxsize=None)
     def set_basic_data(
         cls,
-        ages: pd.DataFrame = None,
-        sts: pd.DataFrame = None,
-        states: pd.DataFrame = None,
-        opens: pd.DataFrame = None,
-        closes: pd.DataFrame = None,
-        capitals: pd.DataFrame = None,
-        opens_average_first_day: bool = 0,
-        total_cap: bool = 0,
+        age: pd.DataFrame,
+        st: pd.DataFrame,
+        state: pd.DataFrame,
+        open: pd.DataFrame,
+        close: pd.DataFrame,
+        capital: pd.DataFrame,
     ):
-        if ages is None:
-            ages = read_daily(age=1, start=20100101)
-        if sts is None:
-            sts = read_daily(st=1, start=20100101)
-        if states is None:
-            states = read_daily(state=1, start=20100101)
-        if opens is None:
-            if opens_average_first_day:
-                opens = read_daily(vwap=1, start=20100101)
-            else:
-                opens = read_daily(open=1, start=20100101)
-        if closes is None:
-            closes = read_daily(close=1, start=20100101)
-        if capitals is None:
-            if total_cap:
-                capitals = (
-                    read_daily(total_cap=1, start=20100101).resample(cls.freq).last()
-                )
-            else:
-                capitals = (
-                    read_daily(flow_cap=1, start=20100101).resample(cls.freq).last()
-                )
         # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
-        cls.ages = ages
+        cls.ages = age
         # stæ—¥å­æ ‡å¿—æ–‡ä»¶
-        cls.sts = sts.fillna(0)
+        cls.sts = st.fillna(0)
         # cls.sts = 1 - cls.sts.fillna(0)
         # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
-        cls.states = states
+        cls.states = state
         # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
-        cls.opens = opens
+        cls.opens = open
         # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
-        cls.closes = closes
+        cls.closes = close
         # æœˆåº•æµé€šå¸‚å€¼æ•°æ®
-        cls.capital = capitals
-        if cls.opens is not None:
-            cls.opens = cls.opens.replace(0, np.nan)
-        if cls.closes is not None:
-            cls.closes = cls.closes.replace(0, np.nan)
+        cls.capital = capital
+        cls.opens = cls.opens.replace(0, np.nan)
+        cls.closes = cls.closes.replace(0, np.nan)
 
-    def set_factor_df_date_as_index(self, df: pd.DataFrame):
+    def set_factor_df_date_as_index(self, df):
         """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
-        # week_here
-        self.factors = df.resample(self.freq).last().dropna(how="all")
-        self.factor_cover = np.sign(self.factors.abs() + 1).sum().sum()
-        opens = self.opens[self.opens.index >= self.factors.index.min()]
-        total = np.sign(opens.resample(self.freq).last()).sum().sum()
-        self.factor_cover = min(self.factor_cover / total, 1)
-        self.factor_cross_skew = self.factors.skew(axis=1).mean()
-        pos_num = ((self.factors > 0) + 0).sum().sum()
-        neg_num = ((self.factors < 0) + 0).sum().sum()
-        self.pos_neg_rate = pos_num / (neg_num + pos_num)
-        self.corr_itself = show_corr(self.factors, self.factors.shift(1), plt_plot=0)
-        self.corr_itself_shift2 = show_corr(
-            self.factors, self.factors.shift(2), plt_plot=0
-        )
-        self.factor_cross_stds = self.factors.std(axis=1)
+        df = df.reset_index()
+        df.columns = ["date"] + list(df.columns)[1:]
+        self.factors = df
+        self.factors = self.factors.set_index("date")
+        self.factors = self.factors.resample("M").last()
+        self.factors = self.factors.reset_index()
 
     @classmethod
     def judge_month_st(cls, df):
         """æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ"""
         st_count = len(df[df == 1])
         normal_count = len(df[df != 1])
         if st_count >= normal_count:
@@ -1594,93 +1433,62 @@
             return 1
 
     @classmethod
     def read_add(cls, pridf, df, func):
         """ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ """
         if pridf.index.max() > df.index.max():
             df_add = pridf[pridf.index > df.index.max()]
-            if df_add.shape[0] > int(cls.freq_ctrl.days_in / 2):
-                df_1 = df_add.index.max()
-                year = df_1.year
-                month = df_1.month
-                last = tt.date.get_close(year=year, m=month).pd_date()
-                if (last == df_1)[0]:
-                    # week_here
-                    df_add = df_add.resample(cls.freq).apply(func)
-                    df = pd.concat([df, df_add])
-                    return df
-                else:
-                    df_add = df_add[
-                        df_add.index < pd.Timestamp(year=year, month=month, day=1)
-                    ]
-                    if df_add.shape[0] > 0:
-                        df_add = df_add.resample(cls.freq).apply(func)
-                        df = pd.concat([df, df_add])
-                        return df
-                    else:
-                        return df
-            else:
-                return df
+            df_add = df_add.resample("M").apply(func)
+            df = pd.concat([df, df_add])
+            return df
         else:
             return df
 
     @classmethod
     def daily_to_monthly(cls, pridf, path, func):
         """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
         è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
         try:
-            month_df = pd.read_parquet(path)
+            month_df = pd.read_feather(path)
+            month_df = month_df.set_index(list(month_df.columns)[0])
             month_df = cls.read_add(pridf, month_df, func)
-            month_df.to_parquet(path)
+            month_df.reset_index().to_feather(path)
         except Exception as e:
             if not STATES["NO_LOG"]:
                 logger.error("error occurs when read state files")
                 logger.error(e)
             print("state file rewritingâ€¦â€¦")
-            # week_here
-            df_1 = pridf.index.max()
-            year = df_1.year
-            month = df_1.month
-            last = tt.date.get_close(year=year, m=month).pd_date()
-            if not (last == df_1)[0]:
-                pridf = pridf[pridf.index < pd.Timestamp(year=year, month=month, day=1)]
-            month_df = pridf.resample(cls.freq).apply(func)
-            month_df.to_parquet(path)
+            month_df = pridf.resample("M").apply(func)
+            month_df.reset_index().to_feather(path)
         return month_df
 
     @classmethod
     @lru_cache(maxsize=None)
     def judge_month(cls):
         """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""
-        if cls.freq == "M":
-            cls.sts_monthly = cls.daily_to_monthly(
-                cls.sts, cls.sts_monthly_file, cls.judge_month_st
-            )
-            cls.states_monthly = cls.daily_to_monthly(
-                cls.states, cls.states_monthly_file, cls.judge_month_state
-            )
-            # week_here
-            cls.ages_monthly = (cls.ages.resample(cls.freq).last() > 60) + 0
-            cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
-            cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)
-        else:
-            cls.tris_monthly = (
-                (1 - cls.sts).resample(cls.freq).last().ffill(limit=2)
-                * cls.states.resample(cls.freq).last().ffill(limit=2)
-                * ((cls.ages.resample(cls.freq).last() > 60) + 0)
-            ).replace(0, np.nan)
+
+        cls.sts_monthly = cls.daily_to_monthly(
+            cls.sts, cls.sts_monthly_file, cls.judge_month_st
+        )
+        cls.states_monthly = cls.daily_to_monthly(
+            cls.states, cls.states_monthly_file, cls.judge_month_state
+        )
+        cls.ages_monthly = cls.ages.resample("M").last()
+        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(
+            -1, 0
+        )
+        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
+        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)
 
     @classmethod
     @lru_cache(maxsize=None)
     def get_rets_month(cls):
         """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
-        # week_here
-        cls.opens_monthly = cls.opens.resample(cls.freq).first()
-        # week_here
-        cls.closes_monthly = cls.closes.resample(cls.freq).last()
+        cls.opens_monthly = cls.opens.resample("M").first()
+        cls.closes_monthly = cls.closes.resample("M").last()
         cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
         cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
         cls.rets_monthly = cls.rets_monthly.stack().reset_index()
         cls.rets_monthly.columns = ["date", "code", "ret"]
 
     @classmethod
     def neutralize_factors(cls, df):
@@ -1719,27 +1527,24 @@
         else:
             cls.cap["cap_size"] = np.log(cls.cap["cap_size"])
 
     def get_neutral_factors(
         self, zxindustry_dummies=0, swindustry_dummies=0, only_cap=0
     ):
         """å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
-        # week_here
-        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
-        # week_here
-        self.factors = self.factors.resample(self.freq).last()
-        # week_here
-        last_date = self.freq_ctrl.next_end(self.tris_monthly.index.max())
+        self.factors = self.factors.set_index("date")
+        self.factors.index = self.factors.index + pd.DateOffset(months=1)
+        self.factors = self.factors.resample("M").last()
+        last_date = self.tris_monthly.index.max() + pd.DateOffset(months=1)
+        last_date = last_date + pd.tseries.offsets.MonthEnd()
         add_tail = pd.DataFrame(1, index=[last_date], columns=self.tris_monthly.columns)
         tris_monthly = pd.concat([self.tris_monthly, add_tail])
         self.factors = self.factors * tris_monthly
-        # week_here
-        self.factors.index = self.factors.index - self.freq_ctrl.time_shift
-        # week_here
-        self.factors = self.factors.resample(self.freq).last()
+        self.factors.index = self.factors.index - pd.DateOffset(months=1)
+        self.factors = self.factors.resample("M").last()
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
         self.factors = pd.merge(
             self.factors, self.cap, how="inner", on=["date", "code"]
         )
         if not only_cap:
             if swindustry_dummies:
@@ -1752,33 +1557,31 @@
                 )
         self.factors = self.factors.set_index(["date", "code"])
         self.factors = self.factors.groupby(["date"]).apply(self.neutralize_factors)
         self.factors = self.factors.reset_index()
 
     def deal_with_factors(self):
         """åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®"""
+        self.factors = self.factors.set_index("date")
         self.__factors_out = self.factors.copy()
-        # week_here
-        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
-        # week_here
-        self.factors = self.factors.resample(self.freq).last()
+        self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
+        self.factors.index = self.factors.index + pd.DateOffset(months=1)
+        self.factors = self.factors.resample("M").last()
         self.factors = self.factors * self.tris_monthly
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
 
     def deal_with_factors_after_neutralize(self):
         """ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•"""
         self.factors = self.factors.set_index(["date", "code"])
         self.factors = self.factors.unstack()
         self.__factors_out = self.factors.copy()
         self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
-        # week_here
-        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
-        # week_here
-        self.factors = self.factors.resample(self.freq).last()
+        self.factors.index = self.factors.index + pd.DateOffset(months=1)
+        self.factors = self.factors.resample("M").last()
         self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
 
     @classmethod
     def find_limit(cls, df, up=1):
         """è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
@@ -1797,38 +1600,35 @@
         """æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ"""
         """æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨"""
         cls.opens_monthly_shift = cls.opens_monthly.copy()
         cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
         cls.rets_monthly_begin = (
             cls.opens_monthly_shift - cls.closes_monthly
         ) / cls.closes_monthly
-        # week_here
-        cls.closes2_monthly = cls.closes.shift(1).resample(cls.freq).last()
+        cls.closes2_monthly = cls.closes.shift(1).resample("M").last()
         cls.rets_monthly_last = (
             cls.closes_monthly - cls.closes2_monthly
         ) / cls.closes2_monthly
         cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
         cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)
 
     def get_ic_rankic(cls, df):
         """è®¡ç®—ICå’ŒRankIC"""
         df1 = df[["ret", "fac"]]
         ic = df1.corr(method="pearson").iloc[0, 1]
-        rankic = df1.rank().corr().iloc[0, 1]
+        rankic = df1.corr(method="spearman").iloc[0, 1]
         df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic]})
         return df2
 
     def get_icir_rankicir(cls, df):
         """è®¡ç®—ICIRå’ŒRankICIR"""
         ic = df.ic.mean()
         rankic = df.rankic.mean()
-        # week_here
-        icir = ic / np.std(df.ic) * (cls.freq_ctrl.counts_one_year ** (0.5))
-        # week_here
-        rankicir = rankic / np.std(df.rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
+        icir = ic / np.std(df.ic) * (12 ** (0.5))
+        rankicir = rankic / np.std(df.rankic) * (12 ** (0.5))
         return pd.DataFrame(
             {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
             index=["è¯„ä»·æŒ‡æ ‡"],
         )
 
     def get_ic_icir_and_rank(cls, df):
         """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
@@ -1837,15 +1637,15 @@
         cls.rankics = df1.rankic
         cls.ics = cls.ics.reset_index(drop=True, level=1).to_frame()
         cls.rankics = cls.rankics.reset_index(drop=True, level=1).to_frame()
         df2 = cls.get_icir_rankicir(df1)
         df2 = df2.T
         dura = (df.date.max() - df.date.min()).days / 365
         t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
-        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankIC.t"])
+        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankICå‡å€¼tå€¼"])
         df4 = pd.concat([df2, df3])
         return df4
 
     @classmethod
     def get_groups(cls, df, groups_num):
         """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
         if "group" in list(df.columns):
@@ -1863,25 +1663,32 @@
         if len(l) < df.shape[0]:
             l = l + [groups_num] * (df.shape[0] - len(l))
         l = l[: df.shape[0]]
         df.insert(0, "group", l)
         return df
 
     @classmethod
+    def next_month_end(cls, x):
+        """æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©"""
+        x1 = x = x + relativedelta(months=1)
+        while x1.month == x.month:
+            x1 = x1 + relativedelta(days=1)
+        return x1 - relativedelta(days=1)
+
+    @classmethod
     def limit_old_to_new(cls, limit, data):
         """è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
         æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»"""
         data1 = data.copy()
         data1 = data1.reset_index()
         data1.columns = ["data_index"] + list(data1.columns)[1:]
         old = pd.merge(limit, data1, how="inner", on=["date", "code"])
         old = old.set_index("data_index")
         old = old[["group", "date", "code"]]
-        # week_here
-        old.date = list(map(cls.freq_ctrl.next_end, list(old.date)))
+        old.date = list(map(cls.next_month_end, list(old.date)))
         return old
 
     def get_data(self, groups_num):
         """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
         self.data = pd.merge(
             self.rets_monthly, self.factors, how="inner", on=["date", "code"]
         )
@@ -1889,124 +1696,91 @@
         self.data = self.data.groupby("date").apply(
             lambda x: self.get_groups(x, groups_num)
         )
         self.wind_out = self.data.copy()
         self.factor_turnover_rates = self.data.pivot(
             index="date", columns="code", values="group"
         )
-        rates = []
-        for i in range(1, groups_num + 1):
-            son = (self.factor_turnover_rates == i) + 0
-            son1 = son.diff()
-            # self.factor_turnover_rates = self.factor_turnover_rates.diff()
-            change = ((np.abs(np.sign(son1)) == 1) + 0).sum(axis=1)
-            still = (((son1 == 0) + 0) * son).sum(axis=1)
-            rate = change / (change + still)
-            rates.append(rate.to_frame(f"group{i}"))
-        rates = pd.concat(rates, axis=1).fillna(0)
-        self.factor_turnover_rates = rates
+        self.factor_turnover_rates = self.factor_turnover_rates.diff()
+        change = ((np.abs(np.sign(self.factor_turnover_rates)) == 1) + 0).sum(axis=1)
+        still = ((self.factor_turnover_rates == 0) + 0).sum(axis=1)
+        self.factor_turnover_rates = change / (change + still)
+        self.factor_turnover_rate = self.factor_turnover_rates.mean()
         self.data = self.data.reset_index(drop=True)
         limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
         limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
         self.data = self.data.drop(limit_ups_object.index)
         rets_monthly_limit_downs = pd.merge(
             self.rets_monthly, limit_downs_object, how="inner", on=["date", "code"]
         )
         self.data = pd.concat([self.data, rets_monthly_limit_downs])
 
+    def select_data_time(self, time_start, time_end):
+        """ç­›é€‰ç‰¹å®šçš„æ—¶é—´æ®µ"""
+        if time_start:
+            self.data = self.data[self.data.date >= time_start]
+        if time_end:
+            self.data = self.data[self.data.date <= time_end]
+
     def make_start_to_one(self, l):
         """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
         min_date = self.factors.date.min()
         add_date = min_date - relativedelta(days=min_date.day)
         add_l = pd.Series([1], index=[add_date])
         l = pd.concat([add_l, l])
         return l
 
     def to_group_ret(self, l):
         """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
-        # week_here
-        ret = l[-1] ** (self.freq_ctrl.counts_one_year / len(l)) - 1
+        ret = l[-1] ** (12 / len(l)) - 1
         return ret
 
-    def get_group_rets_net_values(
-        self, groups_num=10, value_weighted=False, trade_cost_double_side=0
-    ):
+    def get_group_rets_net_values(self, groups_num=10, value_weighted=False):
         """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
         if value_weighted:
             cap_value = self.capital.copy()
-            # week_here
-            cap_value = cap_value.resample(self.freq).last().shift(1)
+            cap_value = cap_value.resample("M").last().shift(1)
             cap_value = cap_value * self.tris_monthly
             # cap_value=np.log(cap_value)
             cap_value = cap_value.stack().reset_index()
             cap_value.columns = ["date", "code", "cap_value"]
             self.data = pd.merge(self.data, cap_value, on=["date", "code"])
 
             def in_g(df):
                 df.cap_value = df.cap_value / df.cap_value.sum()
                 df.ret = df.ret * df.cap_value
                 return df.ret.sum()
 
             self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
-            self.rets_all = self.data.groupby(["date"]).apply(in_g)
             self.group_rets_std = "å¸‚å€¼åŠ æƒæš‚æœªè®¾ç½®è¯¥åŠŸèƒ½ï¼Œæ•¬è¯·æœŸå¾…ğŸŒ™"
         else:
             self.group_rets = self.data.groupby(["date", "group"]).apply(
                 lambda x: x.ret.mean()
             )
-            self.rets_all = self.data.groupby(["date"]).apply(lambda x: x.ret.mean())
-            self.group_rets_stds = self.data.groupby(["date", "group"]).ret.std()
-            self.group_rets_std = (
-                self.group_rets_stds.reset_index().groupby("group").mean()
-            )
-            self.group_rets_skews = self.data.groupby(["date", "group"]).ret.skew()
-            self.group_rets_skew = (
-                self.group_rets_skews.reset_index().groupby("group").mean()
+            self.group_rets_stds = self.data.groupby(["date", "group"]).apply(
+                lambda x: x.ret.std()
             )
+            self.group_rets_std = self.group_rets_stds.groupby("group").mean()
         # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
         self.group_rets = self.group_rets.unstack()
         self.group_rets = self.group_rets[
             self.group_rets.index <= self.factors.date.max()
         ]
         self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
         self.group_rets = self.group_rets.add_prefix("group")
-        self.group_rets = (
-            self.group_rets - self.factor_turnover_rates * trade_cost_double_side
-        )
-        self.rets_all = (
-            self.rets_all
-            - self.factor_turnover_rates.mean(axis=1) * trade_cost_double_side
-        )
         self.long_short_rets = (
             self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
         )
-        self.inner_rets_long = self.group_rets.group1 - self.rets_all
-        self.inner_rets_short = (
-            self.rets_all - self.group_rets["group" + str(groups_num)]
-        )
-        self.long_short_net_values = self.make_start_to_one(
-            (self.long_short_rets + 1).cumprod()
-        )
+        self.long_short_net_values = (self.long_short_rets + 1).cumprod()
         if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
             self.long_short_rets = (
                 self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
             )
-            self.long_short_net_values = self.make_start_to_one(
-                (self.long_short_rets + 1).cumprod()
-            )
-            self.inner_rets_long = (
-                self.group_rets["group" + str(groups_num)] - self.rets_all
-            )
-            self.inner_rets_short = self.rets_all - self.group_rets.group1
-        self.inner_long_net_values = self.make_start_to_one(
-            (self.inner_rets_long + 1).cumprod()
-        )
-        self.inner_short_net_values = self.make_start_to_one(
-            (self.inner_rets_short + 1).cumprod()
-        )
+            self.long_short_net_values = (self.long_short_rets + 1).cumprod()
+        self.long_short_net_values = self.make_start_to_one(self.long_short_net_values)
         self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
         self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
         self.group_net_values = self.group_net_values.cumprod()
         self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
         a = groups_num ** (0.5)
         # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
         if a == int(a):
@@ -2020,34 +1794,18 @@
                 index=list(range(1, int(a) + 1)),
             )
             print("è¿™æ˜¯self.square_rets", self.square_rets)
 
     def get_long_short_comments(self, on_paper=False):
         """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
         åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
-        # week_here
         self.long_short_ret_yearly = (
-            self.long_short_net_values[-1]
-            ** (self.freq_ctrl.counts_one_year / len(self.long_short_net_values))
-            - 1
-        )
-        self.inner_long_ret_yearly = (
-            self.inner_long_net_values[-1]
-            ** (self.freq_ctrl.counts_one_year / len(self.inner_long_net_values))
-            - 1
-        )
-        self.inner_short_ret_yearly = (
-            self.inner_short_net_values[-1]
-            ** (self.freq_ctrl.counts_one_year / len(self.inner_short_net_values))
-            - 1
-        )
-        # week_here
-        self.long_short_vol_yearly = np.std(self.long_short_rets) * (
-            self.freq_ctrl.counts_one_year**0.5
+            self.long_short_net_values[-1] ** (12 / len(self.long_short_net_values)) - 1
         )
+        self.long_short_vol_yearly = np.std(self.long_short_rets) * (12**0.5)
         self.long_short_info_ratio = (
             self.long_short_ret_yearly / self.long_short_vol_yearly
         )
         self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
         self.long_short_win_ratio = self.long_short_win_times / len(
             self.long_short_rets
         )
@@ -2062,163 +1820,102 @@
                         self.long_short_ret_yearly,
                         self.long_short_vol_yearly,
                         self.long_short_info_ratio,
                         self.long_short_win_ratio,
                         self.max_retreat,
                     ]
                 },
-                # week_here
-                index=[
-                    "å¹´åŒ–æ”¶ç›Šç‡",
-                    "å¹´åŒ–æ³¢åŠ¨ç‡",
-                    "æ”¶ç›Šæ³¢åŠ¨æ¯”",
-                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
-                    "æœ€å¤§å›æ’¤ç‡",
-                ],
+                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "æ”¶ç›Šæ³¢åŠ¨æ¯”", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
             )
         else:
             self.long_short_comments = pd.DataFrame(
                 {
                     "è¯„ä»·æŒ‡æ ‡": [
                         self.long_short_ret_yearly,
                         self.long_short_vol_yearly,
                         self.long_short_info_ratio,
                         self.long_short_win_ratio,
                         self.max_retreat,
                     ]
                 },
-                # week_here
-                index=[
-                    "å¹´åŒ–æ”¶ç›Šç‡",
-                    "å¹´åŒ–æ³¢åŠ¨ç‡",
-                    "ä¿¡æ¯æ¯”ç‡",
-                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
-                    "æœ€å¤§å›æ’¤ç‡",
-                ],
+                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
             )
 
-    def get_total_comments(self, groups_num):
-        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
-        rankic = self.rankics.mean()
-        rankic_win = self.rankics[self.rankics * rankic > 0]
-        rankic_win_ratio = rankic_win.dropna().shape[0] / self.rankics.dropna().shape[0]
-        self.factor_cross_skew_after_neu = self.__factors_out.skew(axis=1).mean()
-        if self.ic_icir_and_rank.iloc[2, 0] > 0:
-            self.factor_turnover_rate = self.factor_turnover_rates[
-                f"group{groups_num}"
-            ].mean()
-        else:
-            self.factor_turnover_rate = self.factor_turnover_rates["group1"].mean()
+    def get_total_comments(self):
+        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
         self.total_comments = pd.concat(
             [
                 self.ic_icir_and_rank,
-                pd.DataFrame(
-                    {"è¯„ä»·æŒ‡æ ‡": [rankic_win_ratio]},
-                    index=["RankICèƒœç‡"],
-                ),
                 self.long_short_comments,
-                # week_here
-                pd.DataFrame(
-                    {
-                        "è¯„ä»·æŒ‡æ ‡": [
-                            self.factor_turnover_rate,
-                            self.factor_cover,
-                            self.pos_neg_rate,
-                            self.factor_cross_skew,
-                            self.inner_long_ret_yearly,
-                            self.inner_long_ret_yearly
-                            / (
-                                self.inner_long_ret_yearly + self.inner_short_ret_yearly
-                            ),
-                            self.corr_itself,
-                        ]
-                    },
-                    index=[
-                        f"å¤šå¤´{self.freq_ctrl.comment_name}å‡æ¢æ‰‹",
-                        "å› å­è¦†ç›–ç‡",
-                        "å› å­æ­£å€¼å æ¯”",
-                        "å› å­æˆªé¢ååº¦",
-                        "å¤šå¤´è¶…å‡æ”¶ç›Š",
-                        "å¤šå¤´æ”¶ç›Šå æ¯”",
-                        "ä¸€é˜¶è‡ªç›¸å…³æ€§",
-                    ],
-                ),
+                pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [self.factor_turnover_rate]}, index=["æœˆå‡æ¢æ‰‹ç‡"]),
             ]
         )
 
-    def plot_net_values(self, y2, filename, iplot=1, ilegend=1, without_breakpoint=0):
+    def plot_net_values(self, y2, filename, iplot=1, ilegend=1):
         """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
         if not iplot:
             fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(33, 8))
             self.group_net_values.plot(secondary_y=y2, rot=60, ax=ax[0])
             self.group_net_values.plot(secondary_y=y2, ax=ax[0])
             b = self.rankics.copy()
             b.index = [int(i.year) if i.month == 1 else "" for i in list(b.index)]
             b.plot(kind="bar", rot=60, ax=ax[1])
-            self.factor_cross_stds.plot(rot=60, ax=ax[2])
+            self.factor_turnover_rates.plot(rot=60, ax=ax[2])
 
             filename_path = filename + ".png"
             if not STATES["NO_SAVE"]:
                 plt.savefig(filename_path)
         else:
-            tris = (
-                pd.concat(
-                    [self.group_net_values, self.factor_cross_stds, self.rankics],
-                    axis=1,
-                )
-                .rename(columns={0: "å› å­æˆªé¢æ ‡å‡†å·®"})
-                .dropna()
-            )
-            if without_breakpoint:
-                tris = tris.dropna()
+            tris = pd.concat(
+                [self.group_net_values, self.factor_turnover_rates, self.rankics],
+                axis=1,
+            ).rename(columns={0: "turnover_rate"})
             figs = cf.figures(
                 tris,
                 [
                     dict(kind="line", y=list(self.group_net_values.columns)),
-                    dict(kind="bar", y="å› å­æˆªé¢æ ‡å‡†å·®"),
+                    dict(kind="line", y="turnover_rate"),
                     dict(kind="bar", y="rankic"),
                 ],
                 asList=True,
             )
             comments = (
                 self.total_comments.applymap(lambda x: round(x, 4))
                 .rename(index={"RankICå‡å€¼tå€¼": "RankIC.t"})
                 .reset_index()
             )
             here = pd.concat(
                 [
-                    comments.iloc[:6, :].reset_index(drop=True),
-                    comments.iloc[6:12, :].reset_index(drop=True),
-                    comments.iloc[12:, :].reset_index(drop=True),
+                    comments.iloc[:5, :].reset_index(drop=True),
+                    comments.iloc[5:, :].reset_index(drop=True),
                 ],
                 axis=1,
             )
-            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ", "å…¶ä»–æŒ‡æ ‡", "ç»“æœ"]
+            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ"]
             # here=here.to_numpy().tolist()+[['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']]
             table = FF.create_table(here.iloc[::-1])
             table.update_yaxes(matches=None)
             # table=go.Figure([go.Table(header=dict(values=list(here.columns)),cells=dict(values=here.to_numpy().tolist()))])
             figs.append(table)
             figs = [figs[-1]] + figs[:-1]
             figs[1].update_layout(
                 legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
             )
             base_layout = cf.tools.get_base_layout(figs)
 
             sp = cf.subplots(
                 figs,
-                shape=(2, 11),
+                shape=(2, 10),
                 base_layout=base_layout,
                 vertical_spacing=0.15,
                 horizontal_spacing=0.03,
                 shared_yaxes=False,
                 specs=[
                     [
-                        {"rowspan": 2, "colspan": 4},
-                        None,
+                        {"rowspan": 2, "colspan": 3},
                         None,
                         None,
                         {"rowspan": 2, "colspan": 4},
                         None,
                         None,
                         None,
                         {"colspan": 3},
@@ -2229,21 +1926,20 @@
                         None,
                         None,
                         None,
                         None,
                         None,
                         None,
                         None,
-                        None,
                         {"colspan": 3},
                         None,
                         None,
                     ],
                 ],
-                subplot_titles=["å‡€å€¼æ›²çº¿", "å› å­æˆªé¢æ ‡å‡†å·®", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
+                subplot_titles=["å‡€å€¼æ›²çº¿", "æœˆæ¢æ‰‹ç‡", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
             )
             sp["layout"].update(showlegend=ilegend)
             # los=sp['layout']['annotations']
             # los[0]['font']['color']='#000000'
             # los[1]['font']['color']='#000000'
             # los[2]['font']['color']='#000000'
             # los[3]['font']['color']='#000000'
@@ -2283,40 +1979,41 @@
     @classmethod
     @lru_cache(maxsize=None)
     def prerpare(cls):
         """é€šç”¨æ•°æ®å‡†å¤‡"""
         cls.judge_month()
         cls.get_rets_month()
 
+    @kk.desktop_sender(title="å˜¿ï¼Œå›æµ‹ç»“æŸå•¦ï½ğŸ—“")
     def run(
         self,
         groups_num=10,
         neutralize=False,
         boxcox=False,
-        trade_cost_double_side=0,
         value_weighted=False,
         y2=False,
         plt_plot=True,
         plotly_plot=False,
         filename="åˆ†ç»„å‡€å€¼å›¾",
+        time_start=None,
+        time_end=None,
         print_comments=True,
         comments_writer=None,
         net_values_writer=None,
         rets_writer=None,
         comments_sheetname=None,
         net_values_sheetname=None,
         rets_sheetname=None,
         on_paper=False,
         sheetname=None,
         zxindustry_dummies=0,
         swindustry_dummies=0,
         only_cap=0,
         iplot=1,
-        ilegend=0,
-        without_breakpoint=0,
+        ilegend=1,
     ):
         """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
         if comments_writer and not (comments_sheetname or sheetname):
             raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
         if net_values_writer and not (net_values_sheetname or sheetname):
             raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
         if rets_writer and not (rets_sheetname or sheetname):
@@ -2336,21 +2033,20 @@
                 only_cap=only_cap,
             )
             self.deal_with_factors_after_neutralize()
         else:
             self.deal_with_factors()
         self.get_limit_ups_downs()
         self.get_data(groups_num)
+        self.select_data_time(time_start, time_end)
         self.get_group_rets_net_values(
-            groups_num=groups_num,
-            value_weighted=value_weighted,
-            trade_cost_double_side=trade_cost_double_side,
+            groups_num=groups_num, value_weighted=value_weighted
         )
         self.get_long_short_comments(on_paper=on_paper)
-        self.get_total_comments(groups_num=groups_num)
+        self.get_total_comments()
         if on_paper:
             group1_ttest = ss.ttest_1samp(self.group_rets.group1, 0).pvalue
             group10_ttest = ss.ttest_1samp(
                 self.group_rets[f"group{groups_num}"], 0
             ).pvalue
             group_long_short_ttest = ss.ttest_1samp(self.long_short_rets, 0).pvalue
             group1_ret = self.group_rets.group1.mean()
@@ -2378,29 +2074,24 @@
             )
             self.total_comments = pd.concat([papers, self.total_comments])
 
         if plt_plot:
             if not STATES["NO_PLOT"]:
                 if filename:
                     self.plot_net_values(
-                        y2=y2,
-                        filename=filename,
-                        iplot=iplot,
-                        ilegend=bool(ilegend),
-                        without_breakpoint=without_breakpoint,
+                        y2=y2, filename=filename, iplot=iplot, ilegend=bool(ilegend)
                     )
                 else:
                     self.plot_net_values(
                         y2=y2,
                         filename=self.factors_file.split(".")[-2].split("/")[-1]
                         + str(groups_num)
                         + "åˆ†ç»„",
                         iplot=iplot,
                         ilegend=bool(ilegend),
-                        without_breakpoint=without_breakpoint,
                     )
                 plt.show()
         if plotly_plot:
             if not STATES["NO_PLOT"]:
                 if filename:
                     self.plotly_net_values(filename=filename)
                 else:
@@ -2408,18 +2099,16 @@
                         filename=self.factors_file.split(".")[-2].split("/")[-1]
                         + str(groups_num)
                         + "åˆ†ç»„"
                     )
         if print_comments:
             if not STATES["NO_COMMENT"]:
                 tb = Texttable()
-                tb.set_cols_width(
-                    [8] * 5 + [9] + [8] * 2 + [7] * 2 + [8] + [8] + [9] + [10] * 5
-                )
-                tb.set_cols_dtype(["f"] * 18)
+                tb.set_cols_width([8] * 4 + [12] + [8] * 2 + [7] * 2 + [8] + [10])
+                tb.set_cols_dtype(["f"] * 11)
                 tb.header(list(self.total_comments.T.columns))
                 tb.add_rows(self.total_comments.T.to_numpy(), header=False)
                 print(tb.draw())
         if sheetname:
             if comments_writer:
                 if not on_paper:
                     total_comments = self.total_comments.copy()
@@ -2427,25 +2116,18 @@
                     tc[0] = str(round(tc[0] * 100, 2)) + "%"
                     tc[1] = str(round(tc[1], 2))
                     tc[2] = str(round(tc[2] * 100, 2)) + "%"
                     tc[3] = str(round(tc[3], 2))
                     tc[4] = str(round(tc[4], 2))
                     tc[5] = str(round(tc[5] * 100, 2)) + "%"
                     tc[6] = str(round(tc[6] * 100, 2)) + "%"
-                    tc[7] = str(round(tc[7] * 100, 2)) + "%"
-                    tc[8] = str(round(tc[8], 2))
+                    tc[7] = str(round(tc[7], 2))
+                    tc[8] = str(round(tc[8] * 100, 2)) + "%"
                     tc[9] = str(round(tc[9] * 100, 2)) + "%"
                     tc[10] = str(round(tc[10] * 100, 2)) + "%"
-                    tc[11] = str(round(tc[11] * 100, 2)) + "%"
-                    tc[12] = str(round(tc[12] * 100, 2)) + "%"
-                    tc[13] = str(round(tc[13] * 100, 2)) + "%"
-                    tc[14] = str(round(tc[14], 2))
-                    tc[15] = str(round(tc[15], 2))
-                    tc[16] = str(round(tc[16] * 100, 2)) + "%"
-                    tc[17] = str(round(tc[17] * 100, 2)) + "%"
                     new_total_comments = pd.DataFrame(
                         {sheetname: tc}, index=total_comments.index
                     )
                     new_total_comments.T.to_excel(comments_writer, sheet_name=sheetname)
                 else:
                     self.total_comments.rename(columns={"è¯„ä»·æŒ‡æ ‡": sheetname}).to_excel(
                         comments_writer, sheet_name=sheetname
@@ -2471,25 +2153,18 @@
                 tc[0] = str(round(tc[0] * 100, 2)) + "%"
                 tc[1] = str(round(tc[1], 2))
                 tc[2] = str(round(tc[2] * 100, 2)) + "%"
                 tc[3] = str(round(tc[3], 2))
                 tc[4] = str(round(tc[4], 2))
                 tc[5] = str(round(tc[5] * 100, 2)) + "%"
                 tc[6] = str(round(tc[6] * 100, 2)) + "%"
-                tc[7] = str(round(tc[7] * 100, 2)) + "%"
-                tc[8] = str(round(tc[8], 2))
+                tc[7] = str(round(tc[7], 2))
+                tc[8] = str(round(tc[8] * 100, 2)) + "%"
                 tc[9] = str(round(tc[9] * 100, 2)) + "%"
                 tc[10] = str(round(tc[10] * 100, 2)) + "%"
-                tc[11] = str(round(tc[11] * 100, 2)) + "%"
-                tc[12] = str(round(tc[12] * 100, 2)) + "%"
-                tc[13] = str(round(tc[13] * 100, 2)) + "%"
-                tc[14] = str(round(tc[14], 2))
-                tc[15] = str(round(tc[15], 2))
-                tc[16] = str(round(tc[16] * 100, 2)) + "%"
-                tc[17] = str(round(tc[17] * 100, 2)) + "%"
                 new_total_comments = pd.DataFrame(
                     {comments_sheetname: tc}, index=total_comments.index
                 )
                 new_total_comments.T.to_excel(
                     comments_writer, sheet_name=comments_sheetname
                 )
             if net_values_writer and net_values_sheetname:
@@ -2506,28 +2181,25 @@
                 group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                 group_rets.columns = [
                     f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                 ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                 group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)
 
 
-@do_on_dfs
 class pure_moonnight(object):
     """å°è£…é€‰è‚¡æ¡†æ¶"""
 
     __slots__ = ["shen"]
 
     def __init__(
         self,
         factors: pd.DataFrame,
         groups_num: int = 10,
-        freq: str = "M",
         neutralize: bool = 0,
         boxcox: bool = 1,
-        trade_cost_double_side: float = 0,
         value_weighted: bool = 0,
         y2: bool = 0,
         plt_plot: bool = 1,
         plotly_plot: bool = 0,
         filename: str = "åˆ†ç»„å‡€å€¼å›¾",
         time_start: int = None,
         time_end: int = None,
@@ -2549,53 +2221,46 @@
         closes: pd.DataFrame = None,
         capitals: pd.DataFrame = None,
         swindustry_dummy: pd.DataFrame = None,
         zxindustry_dummy: pd.DataFrame = None,
         no_read_indu: bool = 0,
         only_cap: bool = 0,
         iplot: bool = 1,
-        ilegend: bool = 0,
-        without_breakpoint: bool = 0,
-        opens_average_first_day: bool = 0,
-        total_cap: bool = 0,
+        ilegend: bool = 1,
     ) -> None:
         """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
         æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
         å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
         æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
         ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„
 
         Parameters
         ----------
         factors : pd.DataFrame
             è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
         groups_num : int, optional
             åˆ†ç»„æ•°é‡, by default 10
-        freq : str, optional
-            å›æµ‹é¢‘ç‡, by default 'M'
         neutralize : bool, optional
             å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
         boxcox : bool, optional
             å¯¹æµé€šå¸‚å€¼åšæˆªé¢boxcoxå˜æ¢ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 1
-        trade_cost_double_side : float, optional
-            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
         value_weighted : bool, optional
             æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
         y2 : bool, optional
             ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
         plt_plot : bool, optional
             å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
         plotly_plot : bool, optional
             å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
         filename : str, optional
             åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
         time_start : int, optional
-            å›æµ‹èµ·å§‹æ—¶é—´, by default None
+            å›æµ‹èµ·å§‹æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
         time_end : int, optional
-            å›æµ‹ç»ˆæ­¢æ—¶é—´, by default None
+            å›æµ‹ç»ˆæ­¢æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
         print_comments : bool, optional
             æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
         comments_writer : pd.ExcelWriter, optional
             ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
         net_values_writer : pd.ExcelWriter, optional
             ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
         rets_writer : pd.ExcelWriter, optional
@@ -2636,163 +2301,98 @@
             ä¸è¯»å…¥è¡Œä¸šæ•°æ®, by default 0
         only_cap : bool, optional
             ä»…åšå¸‚å€¼ä¸­æ€§åŒ–, by default 0
         iplot : bool, optional
             ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»“æœ, by default 1
         ilegend : bool, optional
             ä½¿ç”¨cufflinksç»˜å›¾æ—¶ï¼Œæ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹, by default 1
-        without_breakpoint : bool, optional
-            ç”»å›¾çš„æ—¶å€™æ˜¯å¦å»é™¤é—´æ–­ç‚¹, by default 0
-        opens_average_first_day : bool, optional
-            ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
-        total_cap : bool, optional
-            åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
         """
 
         if not isinstance(factors, pd.DataFrame):
             factors = factors()
+        start = datetime.datetime.strftime(factors.index.min(), "%Y%m%d")
+        if ages is None:
+            ages = read_daily(age=1, start=start)
+        if sts is None:
+            sts = read_daily(st=1, start=start)
+        if states is None:
+            states = read_daily(state=1, start=start)
+        if opens is None:
+            opens = read_daily(open=1, start=start)
+        if closes is None:
+            closes = read_daily(close=1, start=start)
+        if capitals is None:
+            capitals = read_daily(flow_cap=1, start=start).resample("M").last()
         if comments_writer is None and sheetname is not None:
-            from pure_ocean_breeze.state.states import COMMENTS_WRITER
+            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
 
             comments_writer = COMMENTS_WRITER
         if net_values_writer is None and sheetname is not None:
-            from pure_ocean_breeze.state.states import NET_VALUES_WRITER
+            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
 
             net_values_writer = NET_VALUES_WRITER
         if not on_paper:
-            from pure_ocean_breeze.state.states import ON_PAPER
+            from pure_ocean_breeze.legacy_version.v3p4.state.states import ON_PAPER
 
             on_paper = ON_PAPER
-        if time_start is None:
-            from pure_ocean_breeze.state.states import MOON_START
+        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_START
 
-            if MOON_START is not None:
-                factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
-        else:
-            factors = factors[factors.index >= pd.Timestamp(str(time_start))]
-        if time_end is None:
-            from pure_ocean_breeze.state.states import MOON_END
+        if MOON_START is not None:
+            factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
+        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_END
 
-            if MOON_END is not None:
-                factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
-        else:
-            factors = factors[factors.index <= pd.Timestamp(str(time_end))]
+        if MOON_END is not None:
+            factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
         if boxcox + neutralize == 0:
             no_read_indu = 1
         if only_cap + no_read_indu > 0:
             only_cap = no_read_indu = 1
         if iplot:
             print_comments = 0
-        if total_cap:
-            if opens_average_first_day:
-                if freq == "M":
-                    self.shen = pure_moon_b(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-                elif freq == "W":
-                    self.shen = pure_week_b(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-            else:
-                if freq == "M":
-                    self.shen = pure_moon_c(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-                elif freq == "W":
-                    self.shen = pure_week_c(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-        else:
-            if opens_average_first_day:
-                if freq == "M":
-                    self.shen = pure_moon(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-                elif freq == "W":
-                    self.shen = pure_week(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-            else:
-                if freq == "M":
-                    self.shen = pure_moon_a(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
-                elif freq == "W":
-                    self.shen = pure_week_a(
-                        freq=freq,
-                        no_read_indu=no_read_indu,
-                        swindustry_dummy=swindustry_dummy,
-                        zxindustry_dummy=zxindustry_dummy,
-                        read_in_swindustry_dummy=swindustry_dummies,
-                    )
+        self.shen = pure_moon(
+            no_read_indu=no_read_indu,
+            swindustry_dummy=swindustry_dummy,
+            zxindustry_dummy=zxindustry_dummy,
+            read_in_swindustry_dummy=swindustry_dummies,
+        )
         self.shen.set_basic_data(
-            ages=ages,
-            sts=sts,
-            states=states,
-            opens=opens,
-            closes=closes,
-            capitals=capitals,
-            opens_average_first_day=opens_average_first_day,
-            total_cap=total_cap,
+            age=ages,
+            st=sts,
+            state=states,
+            open=opens,
+            close=closes,
+            capital=capitals,
         )
         self.shen.set_factor_df_date_as_index(factors)
         self.shen.prerpare()
         self.shen.run(
             groups_num=groups_num,
             neutralize=neutralize,
             boxcox=boxcox,
-            trade_cost_double_side=trade_cost_double_side,
             value_weighted=value_weighted,
             y2=y2,
             plt_plot=plt_plot,
             plotly_plot=plotly_plot,
             filename=filename,
+            time_start=time_start,
+            time_end=time_end,
             print_comments=print_comments,
             comments_writer=comments_writer,
             net_values_writer=net_values_writer,
             rets_writer=rets_writer,
             comments_sheetname=comments_sheetname,
             net_values_sheetname=net_values_sheetname,
             rets_sheetname=rets_sheetname,
             on_paper=on_paper,
             sheetname=sheetname,
             swindustry_dummies=swindustry_dummies,
             zxindustry_dummies=zxindustry_dummies,
             only_cap=only_cap,
             iplot=iplot,
             ilegend=ilegend,
-            without_breakpoint=without_breakpoint,
         )
 
     def __call__(self) -> pd.DataFrame:
         """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®
 
         Returns
         -------
@@ -2817,64 +2417,24 @@
             net = self.shen.group_net_values[i]
             com = comments_on_twins(net, ret)
             com = com.to_frame(i)
             coms.append(com)
         df = pd.concat(coms, axis=1)
         return df.T
 
-    def comment_yearly(self) -> pd.DataFrame:
-        """å¯¹å›æµ‹çš„æ¯å¹´è¡¨ç°ç»™å‡ºè¯„ä»·
-
-        Returns
-        -------
-        pd.DataFrame
-            å„å¹´åº¦çš„æ”¶ç›Šç‡
-        """
-        df = self.shen.group_net_values.resample("Y").last().pct_change()
-        df.index = df.index.year
-        return df
-
-
-class pure_week(pure_moon):
-    ...
-
-
-class pure_moon_a(pure_moon):
-    ...
-
-
-class pure_week_a(pure_moon):
-    ...
-
-
-class pure_moon_b(pure_moon):
-    ...
-
-
-class pure_week_b(pure_moon):
-    ...
-
-
-class pure_moon_c(pure_moon):
-    ...
-
-
-class pure_week_c(pure_moon):
-    ...
-
 
 class pure_fall(object):
     # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
     def __init__(self, daily_path: str) -> None:
         """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶
 
         Parameters
         ----------
         daily_path : str
-            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.parquet'ç»“å°¾
+            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.feather'ç»“å°¾
         """
         self.homeplace = HomePlace()
         # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
         self.daily_factors = None
         self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path
 
     def __call__(self, monthly=False):
@@ -2883,14 +2443,160 @@
             return self.monthly_factors.copy()
         else:
             try:
                 return self.daily_factors.copy()
             except Exception:
                 return self.monthly_factors.copy()
 
+    def __add__(self, selfas):
+        """å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ """
+        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
+        fac2s = []
+        if not isinstance(selfas, Iterable):
+            if not STATES["NO_LOG"]:
+                logger.warning(f"{selfas} is changed into Iterable")
+            selfas = (selfas,)
+        for selfa in selfas:
+            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
+            fac2s.append(fac2)
+        for i in fac2s:
+            fac1 = fac1 + i
+        new_pure = pure_fall()
+        new_pure.monthly_factors = fac1
+        return new_pure
+
+    def __mul__(self, selfas):
+        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
+        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
+        fac1 = fac1 - fac1.min()
+        fac2s = []
+        if not isinstance(selfas, Iterable):
+            if not STATES["NO_LOG"]:
+                logger.warning(f"{selfas} is changed into Iterable")
+            selfas = (selfas,)
+        for selfa in selfas:
+            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
+            fac2 = fac2 - fac2.min()
+            fac2s.append(fac2)
+        for i in fac2s:
+            fac1 = fac1 * i
+        new_pure = pure_fall()
+        new_pure.monthly_factors = fac1
+        return new_pure
+
+    def __truediv__(self, selfa):
+        """ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡"""
+        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
+        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
+        fac = fac1 - fac2
+        new_pure = pure_fall()
+        new_pure.monthly_factors = fac
+        return new_pure
+
+    def __floordiv__(self, selfa):
+        """ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤"""
+        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
+        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
+        fac1 = fac1 - fac1.min()
+        fac2 = fac2 - fac2.min()
+        fac = fac1 / fac2
+        fac = fac.replace(np.inf, np.nan)
+        new_pure = pure_fall()
+        new_pure.monthly_factors = fac
+        return new_pure
+
+    @kk.desktop_sender(title="å˜¿ï¼Œæ­£äº¤åŒ–ç»“æŸå•¦ï½ğŸ¬")
+    def __sub__(self, selfa):
+        """ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
+        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯"""
+        if is_notebook():
+            tqdm.tqdm_notebook().pandas()
+        else:
+            tqdm.tqdm.pandas()
+        if not isinstance(selfa, Iterable):
+            if not STATES["NO_LOG"]:
+                logger.warning(f"{selfa} is changed into Iterable")
+            selfa = (selfa,)
+        fac_main = self.wide_to_long(self.monthly_factors, "fac")
+        fac_helps = [i.monthly_factors for i in selfa]
+        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
+        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
+        fac_helps = pd.concat(fac_helps, axis=1)
+        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
+        facs = facs.groupby("date").progress_apply(
+            lambda x: self.de_in_group(x, help_names)
+        )
+        facs = facs.unstack()
+        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
+        return facs
+
+    def __gt__(self, selfa):
+        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
+        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
+        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
+        x = self.monthly_factors.copy()
+        y = selfa.monthly_factors.copy()
+        x = x.stack().reset_index()
+        y = y.stack().reset_index()
+        x.columns = ["date", "code", "fac"]
+        y.columns = ["date", "code", "fac"]
+        shen = pure_moon()
+        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
+        x = (
+            x.reset_index(drop=True)
+            .drop(columns=["fac"])
+            .rename(columns={"group": "groupx"})
+        )
+        xy = pd.merge(x, y, on=["date", "code"])
+        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
+        xy = (
+            xy.reset_index(drop=True)
+            .drop(columns=["fac"])
+            .rename(columns={"group": "groupy"})
+        )
+        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
+        xy = xy[["date", "code", "fac"]]
+        xy = xy.set_index(["date", "code"]).unstack()
+        xy.columns = [i[1] for i in list(xy.columns)]
+        new_pure = pure_fall()
+        new_pure.monthly_factors = xy
+        return new_pure
+
+    def __rshift__(self, selfa):
+        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
+        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
+        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
+        x = self.monthly_factors.copy()
+        y = selfa.monthly_factors.copy()
+        x = x.stack().reset_index()
+        y = y.stack().reset_index()
+        x.columns = ["date", "code", "fac"]
+        y.columns = ["date", "code", "fac"]
+        shen = pure_moon()
+        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
+        x = (
+            x.reset_index(drop=True)
+            .drop(columns=["fac"])
+            .rename(columns={"group": "groupx"})
+        )
+        xy = pd.merge(x, y, on=["date", "code"])
+        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
+        xy = (
+            xy.reset_index(drop=True)
+            .drop(columns=["fac"])
+            .rename(columns={"group": "groupy"})
+        )
+        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
+        xy = xy[["date", "code", "fac"]]
+        xy = xy.set_index(["date", "code"]).unstack()
+        xy.columns = [i[1] for i in list(xy.columns)]
+        new_pure = pure_fall()
+        new_pure.monthly_factors = xy
+        return new_pure
+
     def wide_to_long(self, df, i):
         """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
         df = df.stack().reset_index()
         df.columns = ["date", "code", i]
         df = df.set_index(["date", "code"])
         return df
 
@@ -2922,20 +2628,14 @@
         the_func = partial(func)
         df = df.groupby(["code"]).apply(the_func).to_frame()
         df.columns = [str(day)]
         df = df.T
         df.index = pd.to_datetime(df.index, format="%Y%m%d")
         return df
 
-    @deprecation.deprecated(
-        deprecated_in="3.7.5",
-        removed_in="4.0",
-        current_version=__version__,
-        details="ä½¿ç”¨mysqlæå–åˆ†é’Ÿæ•°æ®å¹¶æ›´æ–°å› å­å€¼çš„æ–¹æ³•åŠŸèƒ½å±€é™ä¸”ä¸¥é‡æ»åï¼Œå°†ç§»é™¤ğŸ‘‹",
-    )
     @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
     def get_daily_factors_alter(self, func: Callable) -> None:
         """ç”¨mysqlé€æ—¥æ›´æ–°åˆ†é’Ÿæ•°æ®æ„é€ çš„å› å­
 
         Parameters
         ----------
         func : Callable
@@ -2945,50 +2645,61 @@
         ------
         `IOError`
             å¦‚æœæ²¡æœ‰å†å²å› å­æ•°æ®ï¼Œå°†æŠ¥é”™
         """
         """é€šè¿‡minute_data_stock_alteræ•°æ®åº“ä¸€å¤©ä¸€å¤©è®¡ç®—å› å­å€¼"""
         try:
             try:
-                self.daily_factors = pd.read_parquet(self.daily_factors_path)
+                self.daily_factors = pd.read_feather(self.daily_factors_path)
             except Exception:
                 self.daily_factors_path = self.daily_factors_path.split("æ—¥é¢‘_")
                 self.daily_factors_path = (
                     self.daily_factors_path[0] + self.daily_factors_path[1]
                 )
-                self.daily_factors = drop_duplicates_index(
-                    pd.read_parquet(self.daily_factors_path)
-                )
+                self.daily_factors = pd.read_feather(self.daily_factors_path)
+            self.daily_factors = self.daily_factors.rename(
+                columns={list(self.daily_factors.columns)[0]: "date"}
+            )
+            self.daily_factors = self.daily_factors.drop_duplicates(
+                subset=["date"], keep="last"
+            )
+            self.daily_factors = self.daily_factors.set_index("date")
             sql = sqlConfig("minute_data_stock_alter")
             now_minute_datas = sql.show_tables(full=False)
             now_minute_data = now_minute_datas[-1]
             now_minute_data = pd.Timestamp(now_minute_data)
             if self.daily_factors.index.max() < now_minute_data:
                 if not STATES["NO_LOG"]:
                     logger.info(
                         f"ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦"
                     )
                 old_end = datetime.datetime.strftime(
                     self.daily_factors.index.max(), "%Y%m%d"
                 )
                 now_minute_datas = [i for i in now_minute_datas if i > old_end]
                 dfs = []
-                for c in tqdm.auto.tqdm(now_minute_datas, desc="æ¡‚æ£¹å…®å…°æ¡¨ï¼Œå‡»ç©ºæ˜å…®é‚æµå…‰ğŸŒŠ"):
+                for c in tqdm.tqdm(now_minute_datas, desc="æ¡‚æ£¹å…®å…°æ¡¨ï¼Œå‡»ç©ºæ˜å…®é‚æµå…‰ğŸŒŠ"):
                     df = self.get_single_day_factor(func, c)
                     dfs.append(df)
                 dfs = pd.concat(dfs)
                 dfs = dfs.sort_index()
                 self.daily_factors = pd.concat([self.daily_factors, dfs])
                 self.daily_factors = self.daily_factors.dropna(how="all")
                 self.daily_factors = self.daily_factors[
                     self.daily_factors.index >= pd.Timestamp(str(STATES["START"]))
                 ]
-            drop_duplicates_index(self.daily_factors).to_parquet(
-                self.daily_factors_path
+            self.daily_factors = self.daily_factors.reset_index()
+            self.daily_factors = self.daily_factors.rename(
+                columns={list(self.daily_factors.columns)[0]: "date"}
             )
+            self.daily_factors = self.daily_factors.drop_duplicates(
+                subset=["date"], keep="first"
+            )
+            self.daily_factors = self.daily_factors.set_index("date")
+            self.daily_factors.reset_index().to_feather(self.daily_factors_path)
             if not STATES["NO_LOG"]:
                 logger.success("æ›´æ–°å·²å®Œæˆ")
 
         except Exception:
             raise IOError(
                 "æ‚¨è¿˜æ²¡æœ‰è¯¥å› å­çš„åˆçº§æ•°æ®ï¼Œæš‚æ—¶ä¸èƒ½æ›´æ–°ã€‚è¯·å…ˆä½¿ç”¨pure_fall_frequentæˆ–pure_fall_flexibleè®¡ç®—å†å²å› å­å€¼ã€‚"
             )
@@ -3045,15 +2756,18 @@
             fac1 = fac1 * i
         new_pure = pure_fall()
         new_pure.monthly_factors = fac1
         return new_pure
 
     def __sub__(self, selfa):
         """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
-        tqdm.auto.tqdm.pandas()
+        if is_notebook():
+            tqdm.tqdm_notebook().pandas()
+        else:
+            tqdm.tqdm.pandas()
         if not isinstance(selfa, Iterable):
             if not STATES["NO_LOG"]:
                 logger.warning(f"{selfa} is changed into Iterable")
             selfa = (selfa,)
         fac_main = self.wide_to_long(self.monthly_factors, "fac")
         fac_helps = [i.monthly_factors for i in selfa]
         help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
@@ -3133,115 +2847,112 @@
 
 class pure_fall_frequent(object):
     """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""
 
     def __init__(
         self,
         factor_file: str,
-        project: str = None,
         startdate: int = None,
         enddate: int = None,
-        questdb_host: str = "127.0.0.1",
         kind: str = "stock",
         clickhouse: bool = 0,
         questdb: bool = 0,
-        questdb_web_port: str = "9001",
         ignore_history_in_questdb: bool = 0,
         groupby_target: list = ["date", "code"],
     ) -> None:
         """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®
 
         Parameters
         ----------
         factor_file : str
-            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
-        project : str, optional
-            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
+            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºfeatheræ–‡ä»¶ï¼Œä»¥'.feather'ç»“å°¾
         startdate : int, optional
             èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
         enddate : int, optional
             æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
-        questdb_host: str, optional
-            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
         kind : str, optional
             ç±»å‹ä¸ºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°ä¸º'index', by default "stock"
         clickhouse : bool, optional
             ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
         questdb : bool, optional
             ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
-        questdb_web_port : str, optional
-            questdbçš„web_port, by default '9001'
         ignore_history_in_questdb : bool, optional
             æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
         groupby_target: list, optional
             groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
             åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
         """
         homeplace = HomePlace()
         self.kind = kind
         self.groupby_target = groupby_target
         if clickhouse == 0 and questdb == 0:
             clickhouse = 1
         self.clickhouse = clickhouse
         self.questdb = questdb
-        self.questdb_web_port = questdb_web_port
         if clickhouse == 1:
             # è¿æ¥clickhouse
             self.chc = ClickHouseClient("minute_data")
         elif questdb == 1:
-            self.chc = Questdb(host=questdb_host, web_port=questdb_web_port)
+            self.chc = Questdb()
         # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
         pinyin = Pinyin()
         self.factor_file_pinyin = pinyin.get_pinyin(
-            factor_file.replace(".parquet", ""), ""
+            factor_file.replace(".feather", ""), ""
         )
-        self.factor_steps = Questdb(host=questdb_host, web_port=questdb_web_port)
-        if project is not None:
-            if not os.path.exists(homeplace.factor_data_file + project):
-                os.makedirs(homeplace.factor_data_file + project)
-            else:
-                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
-        else:
-            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
+        self.factor_steps = Questdb()
         # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
-        if project is not None:
-            factor_file = homeplace.factor_data_file + project + "/" + factor_file
-        else:
-            factor_file = homeplace.factor_data_file + factor_file
+        factor_file = homeplace.factor_data_file + factor_file
         self.factor_file = factor_file
         # è¯»å…¥ä¹‹å‰çš„å› å­
         if os.path.exists(factor_file):
-            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
+            factor_old = pd.read_feather(self.factor_file)
+            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
+            factor_old = factor_old.drop_duplicates(subset=["date"])
+            factor_old = factor_old.set_index("date")
             self.factor_old = factor_old
             # å·²ç»ç®—å¥½çš„æ—¥å­
             dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
             self.dates_old = dates_old
         elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
             self.factor_steps.get_data("show tables").table
         ):
             logger.info(
                 f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
             )
-            factor_old = self.factor_steps.get_data_with_tuple(
-                f"select * from '{self.factor_file_pinyin}'"
-            ).drop_duplicates(subset=["date", "code"])
+            factor_old = self.factor_steps.get_data(
+                f"select * from {self.factor_file_pinyin}"
+            )
+            # åˆ¤æ–­ä¸€ä¸‹æ¯å¤©æ˜¯å¦ç”Ÿæˆå¤šä¸ªæ•°æ®ï¼Œå•ä¸ªæ•°æ®å°±ä»¥floatå½¢å¼å­˜å‚¨ï¼Œå¤šä¸ªæ•°æ®ä»¥listå½¢å¼å­˜å‚¨
+            if "f0" in list(factor_old.columns):
+                factor_old = factor_old[factor_old.f0 != "date"]
+                factor_old.columns = ["date", "code", "fac"]
+                try:
+                    factor_old.fac = factor_old.fac.apply(
+                        lambda x: [float(i) for i in x[1:-1].split(" ") if i != ""]
+                    )
+                except Exception:
+                    factor_old.fac = factor_old.fac.apply(
+                        lambda x: [float(i) for i in x[1:-1].split(", ") if i != ""]
+                    )
             factor_old = factor_old.pivot(index="date", columns="code", values="fac")
+            factor_old.index = pd.to_datetime(factor_old.index)
             factor_old = factor_old.sort_index()
+            factor_old = drop_duplicates_index(factor_old)
             self.factor_old = factor_old
             # å·²ç»ç®—å¥½çš„æ—¥å­
             dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
             self.dates_old = dates_old
         elif ignore_history_in_questdb and self.factor_file_pinyin in list(
             self.factor_steps.get_data("show tables").table
         ):
             logger.info(
                 f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
             )
             factor_old = self.factor_steps.do_order(
-                f"drop table '{self.factor_file_pinyin}'"
+                f"drop table {self.factor_file_pinyin}"
             )
             self.factor_old = None
             self.dates_old = []
             logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
         else:
             self.factor_old = None
             self.dates_old = []
@@ -3291,25 +3002,14 @@
         Returns
         -------
         `pd.DataFrame`
             ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
         """
         return self.factor.copy()
 
-    def forward_dates(self, dates, many_days):
-        dates_index = [self.dates_all.index(i) for i in dates]
-
-        def value(x, a):
-            if x >= 0:
-                return a[x]
-            else:
-                return None
-
-        return [value(i - many_days, self.dates_all) for i in dates_index]
-
     def select_one_calculate(
         self,
         date: pd.Timestamp,
         func: Callable,
         fields: str = "*",
         show_time: bool = 0,
     ) -> None:
@@ -3335,174 +3035,162 @@
             df = df.sort_values(["date", "num"])
         df = df.groupby(self.groupby_target).apply(the_func)
         if self.groupby_target == ["date", "code"]:
             df = df.to_frame("fac").reset_index()
             df.columns = ["date", "code", "fac"]
         else:
             df = df.reset_index()
-        if (df is not None) and (df.shape[0] > 0):
-            df = df.pivot(columns="code", index="date", values="fac")
-            df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-            to_save = df.stack().reset_index()
-            to_save.columns = ["date", "code", "fac"]
-            self.factor_steps.write_via_df(
-                to_save, self.factor_file_pinyin, tuple_col="fac"
-            )
-            return df
+        df = df.pivot(columns="code", index="date", values="fac")
+        df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+        return df
 
     def select_many_calculate(
         self,
-        dates: List[pd.Timestamp],
+        dates: list[pd.Timestamp],
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        many_days: int = 1,
-        n_jobs: int = 1,
+        tqdm_inside: bool = 0,
     ) -> None:
         the_func = partial(func)
-        factor_new = []
         dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
-        if many_days == 1:
-            # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
-            dates_new_len = len(dates)
-            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
-            if cut_points[-1] == cut_points[-2]:
-                cut_points = cut_points[:-1]
-            cuts = tuple(zip(cut_points[:-many_days], cut_points[many_days:]))
-            df_first = self.select_one_calculate(
-                date=dates[0],
-                func=func,
-                fields=fields,
-                show_time=show_time,
-            )
-            factor_new.append(df_first)
-
-            def cal_one(date1, date2):
+        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
+        dates_new_len = len(dates)
+        cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
+        if cut_points[-1] == cut_points[-2]:
+            cut_points = cut_points[:-1]
+        cut_first = cut_points[0]
+        cuts = tuple(zip(cut_points[:-1], cut_points[1:]))
+        print(f"å…±{len(cuts)}æ®µ")
+        factor_new = []
+        df_first = self.select_one_calculate(
+            date=dates[0],
+            func=func,
+            fields=fields,
+            show_time=show_time,
+        )
+        factor_new.append(df_first)
+        to_save = df_first.stack().reset_index()
+        to_save.columns = ["date", "code", "fac"]
+        self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
+
+        if tqdm_inside == 1:
+            # å¼€å§‹è®¡ç®—å› å­å€¼
+            for date1, date2 in cuts:
                 if self.clickhouse == 1:
                     sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                 else:
-                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]} order by code,date,num"
+                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
                 if show_time:
                     df = self.chc.get_data_show_time(sql_order)
                 else:
                     df = self.chc.get_data(sql_order)
                 if self.clickhouse == 1:
                     df = ((df.set_index("code")) / 100).reset_index()
                 else:
                     df.num = df.num.astype(int)
                     df.date = df.date.astype(int)
                     df = df.sort_values(["date", "num"])
-                df = df.groupby(self.groupby_target).apply(the_func)
-                if self.groupby_target == ["date", "code"]:
-                    df = df.to_frame("fac").reset_index()
-                    df.columns = ["date", "code", "fac"]
+                if is_notebook():
+                    tqdm.tqdm_notebook().pandas()
                 else:
-                    df = df.reset_index()
+                    tqdm.tqdm.pandas()
+                df = df.groupby(self.groupby_target).progress_apply(the_func)
+                df = df.to_frame("fac").reset_index()
+                df.columns = ["date", "code", "fac"]
                 df = df.pivot(columns="code", index="date", values="fac")
                 df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                factor_new.append(df)
                 to_save = df.stack().reset_index()
                 to_save.columns = ["date", "code", "fac"]
-                self.factor_steps.write_via_df(
-                    to_save, self.factor_file_pinyin, tuple_col="fac"
-                )
-                return df
-
-            if n_jobs > 1:
-                with WorkerPool(n_jobs=n_jobs) as pool:
-                    factor_new_more = pool.map(cal_one, cuts, progress_bar=True)
-                factor_new = factor_new + factor_new_more
-            else:
+                self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
+        else:
+            if is_notebook():
                 # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in tqdm.auto.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
-                    df = cal_one(date1, date2)
+                for date1, date2 in tqdm.tqdm_notebook(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
+                    if self.clickhouse == 1:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
+                    else:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1]} and date<={dates[date2]} order by code,date,num"
+                    if show_time:
+                        df = self.chc.get_data_show_time(sql_order)
+                    else:
+                        df = self.chc.get_data(sql_order)
+                    if self.clickhouse == 1:
+                        df = ((df.set_index("code")) / 100).reset_index()
+                    df = df.groupby(self.groupby_target).apply(the_func)
+                    if self.groupby_target == ["date", "code"]:
+                        df = df.to_frame("fac").reset_index()
+                        df.columns = ["date", "code", "fac"]
+                    else:
+                        df = df.reset_index()
+                    df = df.pivot(columns="code", index="date", values="fac")
+                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                     factor_new.append(df)
-        else:
-
-            def cal_two(date1, date2):
-                if date1 is not None:
+                    to_save = df.stack().reset_index()
+                    to_save.columns = ["date", "code", "fac"]
+                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in tqdm.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                     if self.clickhouse == 1:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{date1*100} and date<={date2*100} order by code,date,num"
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                     else:
-                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{date1} and cast(date as int)<={date2} order by code,date,num"
+                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
                     if show_time:
                         df = self.chc.get_data_show_time(sql_order)
                     else:
                         df = self.chc.get_data(sql_order)
                     if self.clickhouse == 1:
                         df = ((df.set_index("code")) / 100).reset_index()
                     else:
                         df.num = df.num.astype(int)
                         df.date = df.date.astype(int)
                         df = df.sort_values(["date", "num"])
-                    if self.groupby_target == [
-                        "date",
-                        "code",
-                    ] or self.groupby_target == ["code"]:
-                        df = df.groupby(["code"]).apply(the_func).reset_index()
+                    df = df.groupby(self.groupby_target).apply(the_func)
+                    if self.groupby_target == ["date", "code"]:
+                        df = df.to_frame("fac").reset_index()
+                        df.columns = ["date", "code", "fac"]
                     else:
-                        df = the_func(df)
-                    df = df.assign(date=date2)
-                    df.columns = ["code", "fac", "date"]
+                        df = df.reset_index()
                     df = df.pivot(columns="code", index="date", values="fac")
                     df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                    factor_new.append(df)
                     to_save = df.stack().reset_index()
                     to_save.columns = ["date", "code", "fac"]
-                    self.factor_steps.write_via_df(
-                        to_save, self.factor_file_pinyin, tuple_col="fac"
-                    )
-                    return df
-
-            pairs = self.forward_dates(dates, many_days=many_days)
-            cuts2 = tuple(zip(pairs, dates))
-            if n_jobs > 1:
-                with WorkerPool(n_jobs=n_jobs) as pool:
-                    factor_new_more = pool.map(cal_two, cuts2, progress_bar=True)
-                factor_new = factor_new + factor_new_more
-            else:
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in tqdm.auto.tqdm(cuts2, desc="çŸ¥ä¸å¯ä¹éª¤å¾—ï¼Œæ‰˜é—å“äºæ‚²é£ã€‚"):
-                    df = cal_two(date1, date2)
-                    factor_new.append(df)
-
-        if len(factor_new) > 0:
-            factor_new = pd.concat(factor_new)
-            return factor_new
-        else:
-            return None
+                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
+        factor_new = pd.concat(factor_new)
+        return factor_new
 
     def select_any_calculate(
         self,
-        dates: List[pd.Timestamp],
+        dates: list[pd.Timestamp],
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        many_days: int = 1,
-        n_jobs: int = 1,
+        tqdm_inside: bool = 0,
     ) -> None:
-        if len(dates) == 1 and many_days == 1:
+        if len(dates) == 1:
             res = self.select_one_calculate(
                 dates[0],
                 func=func,
                 fields=fields,
                 show_time=show_time,
             )
         else:
             res = self.select_many_calculate(
                 dates=dates,
                 func=func,
                 fields=fields,
                 chunksize=chunksize,
                 show_time=show_time,
-                many_days=many_days,
-                n_jobs=n_jobs,
+                tqdm_inside=tqdm_inside,
             )
-        if res is not None:
-            self.factor_new.append(res)
         return res
 
     @staticmethod
     def for_cross_via_str(func):
         """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
         ä¾‹å¦‚
         ```python
@@ -3536,45 +3224,31 @@
                 )
         ```
         ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
         """
 
         def full_run(df, *args, **kwargs):
             res = func(df, *args, **kwargs)
-            if isinstance(res, pd.Series):
-                res = res.reset_index()
-                res.columns = ["code", "fac"]
-                return res
-            elif isinstance(res, pd.DataFrame):
-                res.columns = [f"fac{i}" for i in range(len(res.columns))]
-                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
-                res = res[["fac"]].reset_index()
-                res.columns = ["code", "fac"]
-                return res
-            elif res is None:
-                ...
-            else:
-                res = pd.concat(res, axis=1)
-                res.columns = [f"fac{i}" for i in range(len(res.columns))]
-                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
-                res = res[["fac"]].reset_index()
-                res.columns = ["code", "fac"]
-                return res
+            res = pd.concat(res, axis=1)
+            res.columns = [f"fac{i}" for i in range(len(res.columns))]
+            res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
+            res = res[["fac"]].reset_index()
+            res.columns = ["code", "fac"]
+            return res
 
         return full_run
 
     @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
     def get_daily_factors(
         self,
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        many_days: int = 1,
-        n_jobs: int = 1,
+        tqdm_inside: bool = 0,
     ) -> None:
         """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
         å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼
 
         Parameters
         ----------
         func : Callable
@@ -3582,67 +3256,292 @@
         fields : str, optional
             è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
             æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
         chunksize : int, optional
             æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
         show_time : bool, optional
             å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
-        many_days : int, optional
-            è®¡ç®—æŸå¤©çš„å› å­å€¼æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¹‹å‰å¤šå°‘å¤©çš„æ•°æ®
-        n_jobs : int, optional
-            å¹¶è¡Œæ•°é‡ï¼Œä¸å»ºè®®è®¾ç½®ä¸ºå¤§äº2çš„æ•°ï¼Œæ­¤å¤–å½“æ­¤å‚æ•°å¤§äº1æ—¶ï¼Œè¯·ä½¿ç”¨questdbæ•°æ®åº“æ¥è¯»å–åˆ†é’Ÿæ•°æ®, by default 1
+        tqdm_inside : bool, optional
+            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
         """
         if len(self.dates_new) > 0:
             for interval in self.dates_new_intervals:
                 df = self.select_any_calculate(
                     dates=interval,
                     func=func,
                     fields=fields,
                     chunksize=chunksize,
                     show_time=show_time,
-                    many_days=many_days,
-                    n_jobs=n_jobs,
+                    tqdm_inside=tqdm_inside,
                 )
+                self.factor_new.append(df)
             self.factor_new = pd.concat(self.factor_new)
             # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
             self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
-            self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
+            self.factor = self.factor.dropna(how="all")
+            self.factor = self.factor.reset_index()
+            self.factor = self.factor.rename(
+                columns={list(self.factor.columns)[0]: "date"}
+            )
+            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
+            self.factor = self.factor.set_index("date")
             new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
             # å­˜å…¥æœ¬åœ°
-            self.factor.to_parquet(self.factor_file)
+            self.factor.reset_index().to_feather(self.factor_file)
             logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
             # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
-            try:
-                self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
-                logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
-            except Exception:
-                logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")
+            self.factor_steps.do_order(f"drop table {self.factor_file_pinyin}")
+            logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
 
         else:
-            self.factor = drop_duplicates_index(self.factor_old)
+            self.factor = self.factor_old
+            self.factor = self.factor.reset_index()
+            self.factor = self.factor.rename(
+                columns={list(self.factor.columns)[0]: "date"}
+            )
+            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
+            self.factor = self.factor.set_index("date")
             # å­˜å…¥æœ¬åœ°
-            self.factor.to_parquet(self.factor_file)
+            self.factor.reset_index().to_feather(self.factor_file)
             new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
             logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
 
-    def drop_table(self):
-        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
-        try:
-            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
-            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
-        except Exception:
-            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")
+
+class pure_fall_flexible(object):
+    def __init__(
+        self,
+        factor_file: str,
+        startdate: int = None,
+        enddate: int = None,
+        kind: str = "stock",
+        clickhouse: bool = 0,
+        questdb: bool = 0,
+    ) -> None:
+        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼ç”¨åˆ°å¤šæ—¥çš„æ•°æ®ï¼Œæˆ–è€…ç”¨åˆ°æˆªé¢çš„æ•°æ®
+        å¯¹ä¸€æ®µæ—¶é—´çš„æˆªé¢æ•°æ®è¿›è¡Œæ“ä½œï¼Œåœ¨get_daily_factorsçš„funcå‡½æ•°ä¸­
+        è¯·å†™å…¥df=df.groupby([xxx]).apply(fff)ä¹‹ç±»çš„è¯­å¥
+        ç„¶åå•ç‹¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½œä¸ºè¦applyçš„fffï¼Œå¯ä»¥åœ¨applyä¸ŠåŠ è¿›åº¦æ¡
+
+        Parameters
+        ----------
+        factor_file : str
+            ç”¨äºå­˜å‚¨å› å­çš„æ–‡ä»¶åç§°ï¼Œè¯·ä»¥'.feather'ç»“å°¾
+        startdate : int, optional
+            è®¡ç®—å› å­çš„èµ·å§‹æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
+        enddate : int, optional
+            è®¡ç®—å› å­çš„ç»ˆæ­¢æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
+        kind : str, optional
+            æŒ‡å®šè®¡ç®—è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°åˆ™ä¸º'index', by default "stock"
+        clickhouse : bool, optional
+            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
+        questdb : bool, optional
+            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
+        """
+        homeplace = HomePlace()
+        self.kind = kind
+        if clickhouse == 0 and questdb == 0:
+            clickhouse = 1
+        self.clickhouse = clickhouse
+        self.questdb = questdb
+        if clickhouse == 1:
+            # è¿æ¥clickhouse
+            self.chc = ClickHouseClient("minute_data")
+        elif questdb:
+            self.chc = Questdb()
+        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
+        factor_file = homeplace.factor_data_file + factor_file
+        self.factor_file = factor_file
+        # è¯»å…¥ä¹‹å‰çš„å› å­
+        if os.path.exists(factor_file):
+            factor_old = pd.read_feather(self.factor_file)
+            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
+            factor_old = factor_old.drop_duplicates(subset=["date"])
+            factor_old = factor_old.set_index("date")
+            self.factor_old = factor_old
+            # å·²ç»ç®—å¥½çš„æ—¥å­
+            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
+            self.dates_old = dates_old
+        else:
+            self.factor_old = None
+            self.dates_old = []
+            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
+        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
+        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
+        if startdate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i >= startdate]
+        if enddate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i <= enddate]
+        self.dates_all = dates_all
+        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
+        self.dates_new = sorted([i for i in dates_all if i not in dates_old])
+
+    def __call__(self) -> pd.DataFrame:
+        """ç›´æ¥è¿”å›å› å­å€¼çš„pd.DataFrame
+
+        Returns
+        -------
+        `pd.DataFrame`
+            è®¡ç®—å‡ºçš„å› å­å€¼
+        """
+        return self.factor.copy()
+
+    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
+    def get_daily_factors(
+        self,
+        func: Callable,
+        fields: str = "*",
+        chunksize: int = 250,
+        show_time: bool = 0,
+        tqdm_inside: bool = 0,
+    ) -> None:
+        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
+        ä¾ç…§å®šä¹‰çš„å‡½æ•°è®¡ç®—å› å­å€¼
+
+        Parameters
+        ----------
+        func : Callable
+            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
+        fields : str, optional
+            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
+            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
+        chunksize : int, optional
+            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
+        show_time : bool, optional
+            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
+        tqdm_inside : bool, optional
+            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
+        """
+        the_func = partial(func)
+        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
+        dates_new_len = len(self.dates_new)
+        if dates_new_len > 0:
+            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
+            if cut_points[-1] == cut_points[-2]:
+                cut_points = cut_points[:-1]
+            self.cut_points = cut_points
+            self.factor_new = []
+            # å¼€å§‹è®¡ç®—å› å­å€¼
+            if tqdm_inside:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in cut_points:
+                    if self.clickhouse == 1:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
+                    else:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]}"
+                    if show_time:
+                        df = self.chc.get_data_show_time(sql_order)
+                    else:
+                        df = self.chc.get_data(sql_order)
+                    if self.clickhouse == 1:
+                        df = ((df.set_index("code")) / 100).reset_index()
+                    if is_notebook():
+                        tqdm.tqdm_notebook().pandas()
+                    else:
+                        tqdm.tqdm.pandas()
+                    df = the_func(df)
+                    if isinstance(df, pd.Series):
+                        df = df.reset_index()
+                    df.columns = ["date", "code", "fac"]
+                    df = df.pivot(columns="code", index="date", values="fac")
+                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                    self.factor_new.append(df)
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in tqdm.tqdm(cut_points, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
+                    if self.clickhouse == 1:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
+                    else:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
+                    if show_time:
+                        df = self.chc.get_data_show_time(sql_order)
+                    else:
+                        df = self.chc.get_data(sql_order)
+                    if self.clickhouse == 1:
+                        df = ((df.set_index("code")) / 100).reset_index()
+                    df = the_func(df)
+                    if isinstance(df, pd.Series):
+                        df = df.reset_index()
+                    df.columns = ["date", "code", "fac"]
+                    df = df.pivot(columns="code", index="date", values="fac")
+                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                    self.factor_new.append(df)
+            self.factor_new = pd.concat(self.factor_new)
+            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            # å­˜å…¥æœ¬åœ°
+            self.factor.reset_index().to_feather(self.factor_file)
+            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
+        elif dates_new_len == 1:
+            print("å…±1å¤©")
+            if tqdm_inside:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                if self.clickhouse == 1:
+                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
+                else:
+                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
+                if show_time:
+                    df = self.chc.get_data_show_time(sql_order)
+                else:
+                    df = self.chc.get_data(sql_order)
+                if self.clickhouse == 1:
+                    df = ((df.set_index("code")) / 100).reset_index()
+                tqdm.tqdm.pandas()
+                df = the_func(df)
+                if isinstance(df, pd.Series):
+                    df = df.reset_index()
+                df.columns = ["date", "code", "fac"]
+                df = df.pivot(columns="code", index="date", values="fac")
+                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                if self.clickhouse == 1:
+                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
+                else:
+                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
+                if show_time:
+                    df = self.chc.get_data_show_time(sql_order)
+                else:
+                    df = self.chc.get_data(sql_order)
+                if self.clickhouse == 1:
+                    df = ((df.set_index("code")) / 100).reset_index()
+                df = the_func(df)
+                if isinstance(df, pd.Series):
+                    df = df.reset_index()
+                df.columns = ["date", "code", "fac"]
+                df = df.pivot(columns="code", index="date", values="fac")
+                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                self.factor_new.append(df)
+            self.factor_new = df
+            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+            self.factor = (
+                pd.concat([self.factor_old, self.factor_new])
+                .sort_index()
+                .drop_duplicates()
+            )
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            # å­˜å…¥æœ¬åœ°
+            self.factor.reset_index().to_feather(self.factor_file)
+            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
+        else:
+            self.factor = self.factor_old
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
 
 
 class pure_coldwinter(object):
     # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­
 
     def __init__(
         self,
-        facs_dict: Dict = None,
+        facs_dict: dict = None,
         momentum: bool = 1,
         earningsyield: bool = 1,
         growth: bool = 1,
         liquidity: bool = 1,
         size: bool = 1,
         leverage: bool = 1,
         beta: bool = 1,
@@ -3650,15 +3549,15 @@
         residualvolatility: bool = 1,
         booktoprice: bool = 1,
     ) -> None:
         """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­
 
         Parameters
         ----------
-        facs_dict : Dict, optional
+        facs_dict : dict, optional
             é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
         momentum : bool, optional
             æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
         earningsyield : bool, optional
             æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
         growth : bool, optional
             æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
@@ -3676,19 +3575,21 @@
             æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
         booktoprice : bool, optional
             æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
         """
         self.homeplace = HomePlace()
         # barraå› å­æ•°æ®
         styles = os.listdir(self.homeplace.barra_data_file)
-        styles = [i for i in styles if i.endswith(".parquet")]
+        styles = [i for i in styles if i.endswith(".feather")]
         barras = {}
         for s in styles:
             k = s.split(".")[0]
-            v = pd.read_parquet(self.homeplace.barra_data_file + s)
+            v = pd.read_feather(self.homeplace.barra_data_file + s)
+            v.columns = ["date"] + list(v.columns)[1:]
+            v = v.set_index("date")
             barras[k] = v
         rename_dict = {
             "fac": "å› å­è‡ªèº«",
             "size": "å¸‚å€¼",
             "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
             "booktoprice": "ä¼°å€¼",
             "earningsyield": "ç›ˆåˆ©",
@@ -3784,15 +3685,15 @@
             [self.factors] + list(self.barras.values()), axis=1
         ).dropna()
 
     # DONE: ä¿®æ”¹é£æ ¼å› å­å±•ç¤ºé¡ºåºè‡³æŠ¥å‘Šçš„é¡ºåº
     def get_corr(self):
         """è®¡ç®—æ¯ä¸€æœŸçš„ç›¸å…³ç³»æ•°ï¼Œå†æ±‚å¹³å‡å€¼"""
         self.corr_by_step = self.corr_pri.groupby(["date"]).apply(
-            lambda x: x.rank().corr().head(1)
+            lambda x: x.corr().head(1)
         )
         self.__corr = self.corr_by_step.mean()
         self.__corr = self.__corr.rename(index=self.rename_dict)
         self.__corr = self.__corr.to_frame("ç›¸å…³ç³»æ•°").T
 
         self.__corr = self.__corr[self.sort_names]
         self.__corr = self.__corr.T
@@ -3835,22 +3736,21 @@
         self.get_monthly_barras_industrys()
         self.get_wide_barras_industrys()
         self.get_corr_pri_ols_pri()
         self.get_corr()
         self.get_snow_fac()
 
 
-@do_on_dfs
 class pure_snowtrain(object):
     """ç›´æ¥è¿”å›çº¯å‡€å› å­"""
 
     def __init__(
         self,
         factors: pd.DataFrame,
-        facs_dict: Dict = None,
+        facs_dict: dict = None,
         momentum: bool = 1,
         earningsyield: bool = 1,
         growth: bool = 1,
         liquidity: bool = 1,
         size: bool = 1,
         leverage: bool = 1,
         beta: bool = 1,
@@ -3860,36 +3760,36 @@
     ) -> None:
         """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­
 
         Parameters
         ----------
         factors : pd.DataFrame
             è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-        facs_dict : Dict, optional
-            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
+        facs_dict : dict, optional
+            _description_, by default None
         momentum : bool, optional
-            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
+            _description_, by default 1
         earningsyield : bool, optional
-            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
+            _description_, by default 1
         growth : bool, optional
-            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
+            _description_, by default 1
         liquidity : bool, optional
-            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
+            _description_, by default 1
         size : bool, optional
-            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
+            _description_, by default 1
         leverage : bool, optional
-            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
+            _description_, by default 1
         beta : bool, optional
-            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
+            _description_, by default 1
         nonlinearsize : bool, optional
-            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
+            _description_, by default 1
         residualvolatility : bool, optional
-            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
+            _description_, by default 1
         booktoprice : bool, optional
-            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
+            _description_, by default 1
         """
         self.winter = pure_coldwinter(
             facs_dict=facs_dict,
             momentum=momentum,
             earningsyield=earningsyield,
             growth=growth,
             liquidity=liquidity,
@@ -3910,34 +3810,23 @@
         Returns
         -------
         pd.DataFrame
             çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
         """
         return self.winter.snow_fac.copy()
 
-    def show_corr(self) -> pd.DataFrame:
-        """å±•ç¤ºå› å­ä¸barraé£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
-
-        Returns
-        -------
-        pd.DataFrame
-            ç›¸å…³ç³»æ•°è¡¨æ ¼
-        """
-        return self.corr.T.applymap(lambda x: to_percent(x))
-
 
 class pure_newyear(object):
     """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""
 
     def __init__(
         self,
         facx: pd.DataFrame,
         facy: pd.DataFrame,
         group_num_single: int,
-        trade_cost_double_side: float = 0,
         namex: str = "ä¸»",
         namey: str = "æ¬¡",
     ) -> None:
         """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
         ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡
 
         Parameters
@@ -3946,33 +3835,27 @@
             é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
             indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
         facy : pd.DataFrame
             åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
             indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
         group_num_single : int
             å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
-        trade_cost_double_side : float, optional
-            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
         namex : str, optional
             facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
         namey : str, optional
             facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
         """
         homex = pure_fallmount(facx)
         homey = pure_fallmount(facy)
         if group_num_single == 5:
             homexy = homex > homey
         elif group_num_single == 10:
             homexy = homex >> homey
         shen = pure_moonnight(
-            homexy(),
-            group_num_single**2,
-            trade_cost_double_side=trade_cost_double_side,
-            plt_plot=False,
-            print_comments=False,
+            homexy(), group_num_single**2, plt_plot=False, print_comments=False
         )
         sq = shen.shen.square_rets.copy()
         sq.index = [namex + str(i) for i in list(sq.index)]
         sq.columns = [namey + str(i) for i in list(sq.columns)]
         self.square_rets = sq
 
     def __call__(self) -> pd.DataFrame:
@@ -3982,528 +3865,290 @@
         -------
         `pd.DataFrame`
             æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
         """
         return self.square_rets.copy()
 
 
-@do_on_dfs
+class pure_dawn(object):
+    """
+    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
+    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
+    ä½¿ç”¨ä¸¾ä¾‹
+    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
+    ```python
+    class Cut(pure_dawn):
+
+    def cut(self,df):
+        df=df.sort_values('fac1')
+        df=df.assign(fac3=df.fac1*df.fac2)
+        ret0=df.fac2.iloc[:4].mean()
+        ret1=df.fac2.iloc[4:8].mean()
+        ret2=df.fac2.iloc[8:12].mean()
+        ret3=df.fac2.iloc[12:16].mean()
+        ret4=df.fac2.iloc[16:].mean()
+        aret0=df.fac3.iloc[:4].mean()
+        aret1=df.fac3.iloc[4:8].mean()
+        aret2=df.fac3.iloc[8:12].mean()
+        aret3=df.fac3.iloc[12:16].mean()
+        aret4=df.fac3.iloc[16:].mean()
+        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4
+
+    cut=Cut(ct,ret_inday)
+    cut.run(cut.cut)
+
+    cut0=get_value(cut(),0)
+    cut1=get_value(cut(),1)
+    cut2=get_value(cut(),2)
+    cut3=get_value(cut(),3)
+    cut4=get_value(cut(),4)
+    ```
+    """
+
+    def __init__(self, fac1: pd.DataFrame, fac2: pd.DataFrame, *args: list) -> None:
+        """å‡ ä¸ªå› å­çš„æ“ä½œï¼Œæ¯ä¸ªæœˆæ“ä½œä¸€æ¬¡
+
+        Parameters
+        ----------
+        fac1 : pd.DataFrame
+            å› å­å€¼1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
+        fac2 : pd.DataFrame
+            å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
+        """
+        self.fac1 = fac1
+        self.fac1 = self.fac1.stack().reset_index()
+        self.fac1.columns = ["date", "code", "fac1"]
+        self.fac2 = fac2
+        self.fac2 = self.fac2.stack().reset_index()
+        self.fac2.columns = ["date", "code", "fac2"]
+        fac_all = pd.merge(self.fac1, self.fac2, on=["date", "code"])
+        for i, fac in enumerate(args):
+            fac = fac.stack().reset_index()
+            fac.columns = ["date", "code", f"fac{i+3}"]
+            fac_all = pd.merge(fac_all, fac, on=["date", "code"])
+        fac_all = fac_all.sort_values(["date", "code"])
+        self.fac = fac_all.copy()
+
+    def __call__(self) -> pd.DataFrame:
+        """è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼
+
+        Returns
+        -------
+        `pd.DataFrame`
+            æœ€ç»ˆå› å­å€¼
+        """
+        return self.fac.copy()
+
+    def get_fac_long_and_tradedays(self):
+        """å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨"""
+        self.tradedays = sorted(list(set(self.fac.date)))
+
+    def get_month_starts_and_ends(self, backsee=20):
+        """è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥"""
+        self.month_ends = [
+            i
+            for i, j in zip(self.tradedays[:-1], self.tradedays[1:])
+            if i.month != j.month
+        ]
+        self.month_ends.append(self.tradedays[-1])
+        self.month_starts = [
+            self.find_begin(self.tradedays, i, backsee=backsee) for i in self.month_ends
+        ]
+        self.month_starts[0] = self.tradedays[0]
+
+    def find_begin(self, tradedays, end_day, backsee=20):
+        """æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20"""
+        end_day_index = tradedays.index(end_day)
+        start_day_index = end_day_index - backsee + 1
+        start_day = tradedays[start_day_index]
+        return start_day
+
+    def make_monthly_factors_single_code(self, df, func):
+        """
+        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
+        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
+        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
+        """
+        res = {}
+        for start, end in zip(self.month_starts, self.month_ends):
+            this_month = df[(df.date >= start) & (df.date <= end)]
+            res[end] = func(this_month)
+        dates = list(res.keys())
+        corrs = list(res.values())
+        part = pd.DataFrame({"date": dates, "corr": corrs})
+        return part
+
+    def get_monthly_factor(self, func):
+        """è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­"""
+        if is_notebook():
+            tqdm.tqdm_notebook().pandas()
+        else:
+            tqdm.tqdm.pandas(desc="when the dawn comes, tonight will be a memory too.")
+        self.fac = self.fac.groupby(["code"]).progress_apply(
+            lambda x: self.make_monthly_factors_single_code(x, func)
+        )
+        self.fac = (
+            self.fac.reset_index(level=1, drop=True)
+            .reset_index()
+            .set_index(["date", "code"])
+            .unstack()
+        )
+        self.fac.columns = [i[1] for i in list(self.fac.columns)]
+        self.fac = self.fac.resample("M").last()
+
+    @kk.desktop_sender(title="å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›")
+    def run(self, func: Callable, backsee: int = 20) -> None:
+        """æ‰§è¡Œè®¡ç®—çš„æ¡†æ¶ï¼Œäº§ç”Ÿå› å­å€¼
+
+        Parameters
+        ----------
+        func : Callable
+            æ¯ä¸ªæœˆè¦è¿›è¡Œçš„æ“ä½œ
+        backsee : int, optional
+            å›çœ‹æœŸï¼Œå³æ¯ä¸ªæœˆæœˆåº•å¯¹è¿‡å»å¤šå°‘å¤©è¿›è¡Œè®¡ç®—, by default 20
+        """
+        self.get_fac_long_and_tradedays()
+        self.get_month_starts_and_ends(backsee=backsee)
+        self.get_monthly_factor(func)
+
+
 def follow_tests(
     fac: pd.DataFrame,
-    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
-    index_member_value_weighted: bool = 1,
     comments_writer: pd.ExcelWriter = None,
     net_values_writer: pd.ExcelWriter = None,
     pos: bool = 0,
     neg: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
-    nums: List[int] = [3],
-    opens_average_first_day: bool = 0,
-    total_cap: bool = 0,
+    nums: list[int] = [3],
 ):
     """å› å­å®Œæˆå…¨Aæµ‹è¯•åï¼Œè¿›è¡Œçš„ä¸€äº›å¿…è¦çš„åç»­æµ‹è¯•ï¼ŒåŒ…æ‹¬å„ä¸ªåˆ†ç»„è¡¨ç°ã€ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–ã€3510çš„å¤šç©ºå’Œå¤šå¤´ã€å„ä¸ªè¡Œä¸šRank ICã€å„ä¸ªè¡Œä¸šä¹°3åªè¶…é¢è¡¨ç°
 
     Parameters
     ----------
     fac : pd.DataFrame
         è¦è¿›è¡Œåç»­æµ‹è¯•çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
-    trade_cost_double_side : float, optional
-        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
-    index_member_value_weighted : bool, optional
-        æˆåˆ†è‚¡å¤šå¤´é‡‡å–æµé€šå¸‚å€¼åŠ æƒ
     comments_writer : pd.ExcelWriter, optional
         å†™å…¥è¯„ä»·æŒ‡æ ‡çš„excel, by default None
     net_values_writer : pd.ExcelWriter, optional
         å†™å…¥å‡€å€¼åºåˆ—çš„excel, by default None
     pos : bool, optional
         å› å­çš„æ–¹å‘ä¸ºæ­£, by default 0
     neg : bool, optional
         å› å­çš„æ–¹å‘ä¸ºè´Ÿ, by default 0
     swindustry : bool, optional
         ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
     zxindustry : bool, optional
         ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
-    nums : List[int], optional
+    nums : list[int], optional
         å„ä¸ªè¡Œä¸šä¹°å‡ åªè‚¡ç¥¨, by default [3]
-    opens_average_first_day : bool, optional
-        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
-    total_cap : bool, optional
-        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Raises
     ------
     IOError
         å¦‚æœæœªæŒ‡å®šå› å­æ­£è´Ÿæ–¹å‘ï¼Œå°†æŠ¥é”™
     """
     if comments_writer is None:
-        from pure_ocean_breeze.state.states import COMMENTS_WRITER
+        from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
 
         comments_writer = COMMENTS_WRITER
     if net_values_writer is None:
-        from pure_ocean_breeze.state.states import NET_VALUES_WRITER
+        from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
 
         net_values_writer = NET_VALUES_WRITER
 
-    shen = pure_moonnight(
-        fac,
-        opens_average_first_day=opens_average_first_day,
-    )
-    if (
-        shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values.group10.iloc[-1]
-    ):
-        neg = 1
-    else:
-        pos = 1
-    if comments_writer is not None:
-        shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
-    print(shen.comments_ten())
+    shen = pure_moonnight(fac)
+    shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
     """ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–"""
     pure_fac = pure_snowtrain(fac)
-    if comments_writer is not None:
-        pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
-    print(pure_fac.corr)
+    pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
     shen = pure_moonnight(
         pure_fac(),
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="çº¯å‡€",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
     )
     """3510å¤šç©ºå’Œå¤šå¤´"""
     # 300
     fi300 = daily_factor_on300500(fac, hs300=1)
     shen = pure_moonnight(
         fi300,
-        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="300å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
     )
     if pos:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group10, hs300=1).to_excel(
-                comments_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    hs300=1,
-                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group10, hs300=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1).to_excel(
-                net_values_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    hs300=1,
-                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1)
+        make_relative_comments(shen.shen.group_rets.group10, hs300=1).to_excel(
+            comments_writer, sheet_name="300è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1).to_excel(
+            net_values_writer, sheet_name="300è¶…é¢"
+        )
     elif neg:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
-                comments_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    hs300=1,
-                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
-                net_values_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    hs300=1,
-                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
+        make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
+            comments_writer, sheet_name="300è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
+            net_values_writer, sheet_name="300è¶…é¢"
+        )
     else:
         raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
     # 500
     fi500 = daily_factor_on300500(fac, zz500=1)
     shen = pure_moonnight(
         fi500,
-        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="500å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
     )
     if pos:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group10, zz500=1).to_excel(
-                comments_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    zz500=1,
-                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group10, zz500=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1).to_excel(
-                net_values_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    zz500=1,
-                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1)
+        make_relative_comments(shen.shen.group_rets.group10, zz500=1).to_excel(
+            comments_writer, sheet_name="500è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1).to_excel(
+            net_values_writer, sheet_name="500è¶…é¢"
+        )
     else:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
-                comments_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz500=1,
-                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
-                net_values_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz500=1,
-                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
+        make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
+            comments_writer, sheet_name="500è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
+            net_values_writer, sheet_name="500è¶…é¢"
+        )
     # 1000
     fi1000 = daily_factor_on300500(fac, zz1000=1)
     shen = pure_moonnight(
         fi1000,
-        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="1000å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
     )
     if pos:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group10, zz1000=1).to_excel(
-                comments_writer, sheet_name="1000è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    zz1000=1,
-                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group10, zz1000=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(
-                shen.shen.group_rets.group10, zz1000=1
-            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group10
-                    - shen.shen.factor_turnover_rates.group10 * i,
-                    zz1000=1,
-                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group10, zz1000=1)
+        make_relative_comments(shen.shen.group_rets.group10, zz1000=1).to_excel(
+            comments_writer, sheet_name="1000è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group10, zz1000=1).to_excel(
+            net_values_writer, sheet_name="1000è¶…é¢"
+        )
     else:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
-                comments_writer, sheet_name="1000è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz1000=1,
-                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
-                net_values_writer, sheet_name="1000è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz1000=1,
-                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)
+        make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
+            comments_writer, sheet_name="1000è¶…é¢"
+        )
+        make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
+            net_values_writer, sheet_name="1000è¶…é¢"
+        )
     # å„è¡Œä¸šRank IC
     rankics = rankic_test_on_industry(fac, comments_writer)
     # ä¹°3åªè¶…é¢è¡¨ç°
     rets = long_test_on_industry(
         fac, nums, pos=pos, neg=neg, swindustry=swindustry, zxindustry=zxindustry
     )
     logger.success("å› å­åç»­çš„å¿…è¦æµ‹è¯•å…¨éƒ¨å®Œæˆ")
 
 
-def test_on_index_four_out(
-    fac: pd.DataFrame,
-    value_weighted: bool = 1,
-    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
-    group_num: int = 10,
-    boxcox: bool = 1,
-    comments_writer: pd.ExcelWriter = None,
-    net_values_writer: pd.ExcelWriter = None,
-    opens_average_first_day: bool = 0,
-    total_cap: bool = 0,
-):
-    if comments_writer is None:
-        from pure_ocean_breeze.state.states import COMMENTS_WRITER
-
-        comments_writer = COMMENTS_WRITER
-    if net_values_writer is None:
-        from pure_ocean_breeze.state.states import NET_VALUES_WRITER
-
-        net_values_writer = NET_VALUES_WRITER
-
-    shen = pure_moonnight(
-        fac,
-        opens_average_first_day=opens_average_first_day,
-    )
-    if (
-        shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values10.iloc[-1]
-    ):
-        neg = 1
-        pos = 0
-    else:
-        pos = 1
-        neg = 0
-
-    """3510å¤šç©ºå’Œå¤šå¤´"""
-    # 300
-    fi300 = daily_factor_on300500(fac, hs300=1)
-    shen = pure_moonnight(
-        fi300,
-        value_weighted=value_weighted,
-        groups_num=group_num,
-        boxcox=boxcox,
-        comments_writer=comments_writer,
-        net_values_writer=net_values_writer,
-        sheetname="300å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
-    )
-    if pos:
-        if comments_writer is not None:
-            make_relative_comments(
-                shen.shen.group_rets[f"group{group_num}"], hs300=1
-            ).to_excel(comments_writer, sheet_name="300è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    hs300=1,
-                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], hs300=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], hs300=1
-            ).to_excel(net_values_writer, sheet_name="300è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    hs300=1,
-                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], hs300=1
-            )
-    elif neg:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
-                comments_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    hs300=1,
-                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
-                net_values_writer, sheet_name="300è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    hs300=1,
-                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
-    else:
-        raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
-    # 500
-    fi500 = daily_factor_on300500(fac, zz500=1)
-    shen = pure_moonnight(
-        fi500,
-        value_weighted=value_weighted,
-        groups_num=group_num,
-        boxcox=boxcox,
-        comments_writer=comments_writer,
-        net_values_writer=net_values_writer,
-        sheetname="500å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
-    )
-    if pos:
-        if comments_writer is not None:
-            make_relative_comments(
-                shen.shen.group_rets[f"group{group_num}"], zz500=1
-            ).to_excel(comments_writer, sheet_name="500è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    zz500=1,
-                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz500=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], zz500=1
-            ).to_excel(net_values_writer, sheet_name="500è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    zz500=1,
-                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], zz500=1
-            )
-    else:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
-                comments_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz500=1,
-                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
-                net_values_writer, sheet_name="500è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz500=1,
-                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
-    # 1000
-    fi1000 = daily_factor_on300500(fac, zz1000=1)
-    shen = pure_moonnight(
-        fi1000,
-        value_weighted=value_weighted,
-        groups_num=group_num,
-        boxcox=boxcox,
-        comments_writer=comments_writer,
-        net_values_writer=net_values_writer,
-        sheetname="1000å¤šç©º",
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
-    )
-    if pos:
-        if comments_writer is not None:
-            make_relative_comments(
-                shen.shen.group_rets[f"group{group_num}"], zz1000=1
-            ).to_excel(comments_writer, sheet_name="1000è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    zz1000=1,
-                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz1000=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], zz1000=1
-            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets[f"group{group_num}"]
-                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
-                    zz1000=1,
-                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(
-                shen.shen.group_rets[f"group{group_num}"], zz1000=1
-            )
-    else:
-        if comments_writer is not None:
-            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
-                comments_writer, sheet_name="1000è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz1000=1,
-                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
-        if net_values_writer is not None:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
-                net_values_writer, sheet_name="1000è¶…é¢"
-            )
-            for i in trade_cost_double_side_list:
-                make_relative_comments_plot(
-                    shen.shen.group_rets.group1
-                    - shen.shen.factor_turnover_rates.group1 * i,
-                    zz1000=1,
-                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
-        else:
-            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)
-
-
 class pure_helper(object):
     def __init__(
         self,
         df_main: pd.DataFrame,
         df_helper: pd.DataFrame,
         func: Callable = None,
         group: int = 10,
@@ -4560,31 +4205,30 @@
         return df
 
 
 class pure_fama(object):
     # @lru_cache(maxsize=None)
     def __init__(
         self,
-        factors: List[pd.DataFrame],
+        factors: list[pd.DataFrame],
         minus_group: Union[list, float] = 3,
         backsee: int = 20,
         rets: pd.DataFrame = None,
         value_weighted: bool = 1,
         add_market: bool = 1,
         add_market_series: pd.Series = None,
         factors_names: list = None,
         betas_rets: bool = 0,
-        total_cap: bool = 0,
     ) -> None:
         """ä½¿ç”¨famaä¸‰å› å­çš„æ–¹æ³•ï¼Œå°†ä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œæ‹†åˆ†å‡ºå„ä¸ªå› å­å¸¦æ¥çš„æ”¶ç›Šç‡ä»¥åŠç‰¹è´¨çš„æ”¶ç›Šç‡
         åˆ†åˆ«è®¡ç®—æ¯ä¸€æœŸï¼Œå„ä¸ªå› å­æ”¶ç›Šç‡çš„å€¼ï¼Œè¶…é¢æ”¶ç›Šç‡ï¼Œå› å­çš„æš´éœ²ï¼Œä»¥åŠç‰¹è´¨æ”¶ç›Šç‡
 
         Parameters
         ----------
-        factors : List[pd.DataFrame]
+        factors : list[pd.DataFrame]
             ç”¨äºè§£é‡Šæ”¶ç›Šçš„å„ä¸ªå› å­å€¼ï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„dataframe
         minus_group : Union[list, float], optional
             æ¯ä¸€ä¸ªå› å­å°†æˆªé¢ä¸Šçš„è‚¡ç¥¨åˆ†ä¸ºå‡ ç»„, by default 3
         backsee : int, optional
             åšæ—¶åºå›å½’æ—¶ï¼Œå›çœ‹çš„å¤©æ•°, by default 20
         rets : pd.DataFrame, optional
             æ¯åªä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ”¶ç›Šç‡ï¼Œé»˜è®¤ä½¿ç”¨å½“æ—¥æ—¥é—´æ”¶ç›Šç‡, by default None
@@ -4594,16 +4238,14 @@
             æ˜¯å¦åŠ å…¥å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œé»˜è®¤ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ¯æ—¥æ—¥é—´æ”¶ç›Šç‡, by default 1
         add_market_series : bool, optional
             åŠ å…¥çš„å¸‚åœºæ”¶ç›Šç‡çš„æ•°æ®ï¼Œå¦‚æœæ²¡æŒ‡å®šï¼Œåˆ™ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ—¥é—´æ”¶ç›Šç‡, by default None
         factors_names : list, optional
             å„ä¸ªå› å­çš„åå­—ï¼Œé»˜è®¤ä¸ºfac0(å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»fac1å¼€å§‹),fac1,fac2,fac3, by default None
         betas_rets : bool, optional
             æ˜¯å¦è®¡ç®—æ¯åªä¸ªè‚¡çš„ç”±äºæš´éœ²åœ¨æ¯ä¸ªå› å­ä¸Šæ‰€å¸¦æ¥çš„æ”¶ç›Šç‡, by default 0
-        total_cap : bool, optional
-            åŠ æƒæ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
         """
         start = max(
             [int(datetime.datetime.strftime(i.index.min(), "%Y%m%d")) for i in factors]
         )
         self.backsee = backsee
         self.factors = factors
         self.factors_names = factors_names
@@ -4619,16 +4261,14 @@
         ]
         self.factors_group_long = [(i == 0) + 0 for i in self.factors_group]
         self.factors_group_short = [
             (i == (j - 1)) + 0 for i, j in zip(self.factors_group, self.minus_group)
         ]
         self.value_weighted = value_weighted
         if value_weighted:
-            if total_cap:
-                self.cap = read_daily(total_cap=1, start=start)
             self.cap = read_daily(flow_cap=1, start=start)
             self.factors_group_long = [self.cap * i for i in self.factors_group_long]
             self.factors_group_short = [self.cap * i for i in self.factors_group_short]
             self.factors_group_long = [
                 (i.T / i.T.sum()).T for i in self.factors_group_long
             ]
             self.factors_group_short = [
@@ -4665,15 +4305,18 @@
             else:
                 closes = add_market_series.to_frame("fac0")
             rets = closes / closes.shift(1) - 1
             self.__factors_rets = pd.concat([rets, self.__factors_rets], axis=1)
             if factors_names is not None:
                 factors_names = ["å¸‚åœº"] + factors_names
         self.__data = self.make_df(self.rets, self.__factors_rets)
-        tqdm.auto.tqdm.pandas()
+        if is_notebook():
+            tqdm.tqdm_notebook().pandas()
+        else:
+            tqdm.tqdm.pandas()
         self.__coefficients = (
             self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
         )
         self.__coefficients = self.__coefficients.rename(
             columns={
                 i: "co" + i for i in list(self.__coefficients.columns) if "fac" in i
             }
@@ -4786,29 +4429,29 @@
             ...
 
 
 class pure_rollingols(object):
     def __init__(
         self,
         y: pd.DataFrame,
-        xs: Union[List[pd.DataFrame], pd.DataFrame],
+        xs: Union[list[pd.DataFrame], pd.DataFrame],
         backsee: int = 20,
-        factors_names: List[str] = None,
+        factors_names: list[str] = None,
     ) -> None:
         """ä½¿ç”¨è‹¥å¹²ä¸ªdataframeï¼Œå¯¹åº”çš„è‚¡ç¥¨è¿›è¡ŒæŒ‡å®šçª—å£çš„æ—¶åºæ»šåŠ¨å›å½’
 
         Parameters
         ----------
         y : pd.DataFrame
             æ»šåŠ¨å›å½’ä¸­çš„å› å˜é‡yï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
-        xs : Union[List[pd.DataFrame], pd.DataFrame]
+        xs : Union[list[pd.DataFrame], pd.DataFrame]
             æ»šåŠ¨å›å½’ä¸­çš„è‡ªå˜é‡xiï¼Œæ¯ä¸€ä¸ªdataframeï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
         backsee : int, optional
             æ»šåŠ¨å›å½’çš„æ—¶é—´çª—å£, by default 20
-        factors_names : List[str], optional
+        factors_names : list[str], optional
             xsä¸­ï¼Œæ¯ä¸ªå› å­çš„åå­—, by default None
         """
         self.backsee = backsee
         self.y = y
         if not isinstance(xs, list):
             xs = [xs]
         self.xs = xs
@@ -4822,15 +4465,18 @@
             for j, i in enumerate(xs)
         ]
         xs = [y] + xs
         xs = reduce(lambda x, y: pd.merge(x, y, on=["date", "code"]), xs)
         xs = xs.set_index("date")
         self.__data = xs
         self.haha = xs
-        tqdm.auto.tqdm.pandas()
+        if is_notebook():
+            tqdm.tqdm_notebook().pandas()
+        else:
+            tqdm.tqdm.pandas()
         self.__coefficients = (
             self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
         )
         self.__coefficients = self.__coefficients.rename(
             columns={i: "co" + i for i in list(self.__coefficients.columns) if "x" in i}
         )
         self.__data = pd.merge(
@@ -4851,18 +4497,18 @@
         self.__alphas = self.__data.pivot(
             index="date", columns="code", values="intercept"
         )
         if factors_names is None:
             self.__betas = {
                 i: self.__data.pivot(index="date", columns="code", values=i)
                 for i in list(self.__data.columns)
-                if i.startswith("cox")
+                if i.startswith("x")
             }
         else:
-            facs = [i for i in list(self.__data.columns) if i.startswith("cox")]
+            facs = [i for i in list(self.__data.columns) if i.startswith("x")]
             self.__betas = {
                 factors_names[num]: self.__data.pivot(
                     index="date", columns="code", values=i
                 )
                 for num, i in enumerate(facs)
             }
         if len(list(self.__betas)) == 1:
@@ -4900,77 +4546,51 @@
             return pd.concat([alpha, betas], axis=1)
 
         except Exception:
             # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
             ...
 
 
-@do_on_dfs
 def test_on_300500(
     df: pd.DataFrame,
-    trade_cost_double_side: float = 0,
-    group_num: int = 10,
-    value_weighted: bool = 1,
-    boxcox: bool = 0,
     hs300: bool = 0,
     zz500: bool = 0,
     zz1000: bool = 0,
     gz2000: bool = 0,
     iplot: bool = 1,
-    opens_average_first_day: bool = 0,
-    total_cap: bool = 0,
 ) -> pd.Series:
     """å¯¹å› å­åœ¨æŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´æµ‹è¯•
 
     Parameters
     ----------
     df : pd.DataFrame
         å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
-    trade_cost_double_side : float, optional
-        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
-    group_num : int
-        åˆ†ç»„æ•°é‡, by default 10
-    value_weighted : bool
-        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
     hs300 : bool, optional
         åœ¨æ²ªæ·±300æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     zz500 : bool, optional
         åœ¨ä¸­è¯500æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     zz1000 : bool, optional
         åœ¨ä¸­è¯1000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     gz1000 : bool, optional
         åœ¨å›½è¯2000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
-    iplot : bo0l,optional
+    iplot : bol,optional
         å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
-    opens_average_first_day : bool, optional
-        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
-    total_cap : bool, optional
-        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Returns
     -------
     pd.Series
         å¤šå¤´ç»„åœ¨è¯¥æŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
     """
     fi300 = daily_factor_on300500(
         df, hs300=hs300, zz500=zz500, zz1000=zz1000, gz2000=gz2000
     )
-    shen = pure_moonnight(
-        fi300,
-        value_weighted=value_weighted,
-        groups_num=group_num,
-        trade_cost_double_side=trade_cost_double_side,
-        boxcox=boxcox,
-        iplot=iplot,
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
-    )
+    shen = pure_moonnight(fi300, iplot=iplot)
     if (
         shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
+        > shen.shen.group_net_values.group10.iloc[-1]
     ):
         print(
             make_relative_comments(
                 shen.shen.group_rets.group1,
                 hs300=hs300,
                 zz500=zz500,
                 zz1000=zz1000,
@@ -4984,177 +4604,96 @@
             zz1000=zz1000,
             gz2000=gz2000,
         )
         return abrets
     else:
         print(
             make_relative_comments(
-                shen.shen.group_rets[f"group{group_num}"],
+                shen.shen.group_rets.group10,
                 hs300=hs300,
                 zz500=zz500,
                 zz1000=zz1000,
                 gz2000=gz2000,
             )
         )
         abrets = make_relative_comments_plot(
-            shen.shen.group_rets[f"group{group_num}"],
+            shen.shen.group_rets.group10,
             hs300=hs300,
             zz500=zz500,
             zz1000=zz1000,
             gz2000=gz2000,
         )
         return abrets
 
 
-@do_on_dfs
 def test_on_index_four(
-    df: pd.DataFrame,
-    value_weighted: bool = 1,
-    group_num: int = 10,
-    trade_cost_double_side: float = 0,
-    iplot: bool = 1,
-    gz2000: bool = 0,
-    boxcox: bool = 1,
-    opens_average_first_day: bool = 0,
-    total_cap: bool = 0,
+    df: pd.DataFrame, iplot: bool = 1, gz2000: bool = 0, boxcox: bool = 1
 ) -> pd.DataFrame:
     """å¯¹å› å­åŒæ—¶åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000è¿™4ä¸ªæŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´è¶…é¢æµ‹è¯•
 
     Parameters
     ----------
     df : pd.DataFrame
         å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
-    value_weighted : bool
-        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
-    group_num : int
-        åˆ†ç»„æ•°é‡, by default 10
-    trade_cost_double_side : float, optional
-        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
     iplot : bol,optional
         å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
     gz2000 : bool, optional
         æ˜¯å¦è¿›è¡Œå›½è¯2000ä¸Šçš„æµ‹è¯•, by default 0
     boxcox : bool, optional
         æ˜¯å¦è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†, by default 1
-    opens_average_first_day : bool, optional
-        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
-    total_cap : bool, optional
-        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Returns
     -------
     pd.DataFrame
         å¤šå¤´ç»„åœ¨å„ä¸ªæŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
     """
     fi300 = daily_factor_on300500(df, hs300=1)
-    shen = pure_moonnight(
-        fi300,
-        groups_num=group_num,
-        value_weighted=value_weighted,
-        trade_cost_double_side=trade_cost_double_side,
-        iplot=iplot,
-        boxcox=boxcox,
-        opens_average_first_day=opens_average_first_day,
-        total_cap=total_cap,
-    )
+    shen = pure_moonnight(fi300, iplot=iplot, boxcox=boxcox)
     if (
         shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
+        > shen.shen.group_net_values.group10.iloc[-1]
     ):
         com300, net300 = make_relative_comments(
             shen.shen.group_rets.group1, hs300=1, show_nets=1
         )
         fi500 = daily_factor_on300500(df, zz500=1)
-        shen = pure_moonnight(
-            fi500,
-            groups_num=group_num,
-            value_weighted=value_weighted,
-            trade_cost_double_side=trade_cost_double_side,
-            iplot=iplot,
-            boxcox=boxcox,
-            opens_average_first_day=opens_average_first_day,
-            total_cap=total_cap,
-        )
+        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
         com500, net500 = make_relative_comments(
             shen.shen.group_rets.group1, zz500=1, show_nets=1
         )
         fi1000 = daily_factor_on300500(df, zz1000=1)
-        shen = pure_moonnight(
-            fi1000,
-            groups_num=group_num,
-            value_weighted=value_weighted,
-            trade_cost_double_side=trade_cost_double_side,
-            iplot=iplot,
-            boxcox=boxcox,
-            opens_average_first_day=opens_average_first_day,
-            total_cap=total_cap,
-        )
+        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
         com1000, net1000 = make_relative_comments(
             shen.shen.group_rets.group1, zz1000=1, show_nets=1
         )
         if gz2000:
             fi2000 = daily_factor_on300500(df, gz2000=1)
-            shen = pure_moonnight(
-                fi2000,
-                groups_num=group_num,
-                trade_cost_double_side=trade_cost_double_side,
-                iplot=iplot,
-                boxcox=boxcox,
-                opens_average_first_day=opens_average_first_day,
-                total_cap=total_cap,
-            )
+            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
             com2000, net2000 = make_relative_comments(
                 shen.shen.group_rets.group1, gz2000=1, show_nets=1
             )
     else:
         com300, net300 = make_relative_comments(
-            shen.shen.group_rets[f"group{group_num}"], hs300=1, show_nets=1
+            shen.shen.group_rets.group10, hs300=1, show_nets=1
         )
         fi500 = daily_factor_on300500(df, zz500=1)
-        shen = pure_moonnight(
-            fi500,
-            groups_num=group_num,
-            value_weighted=value_weighted,
-            trade_cost_double_side=trade_cost_double_side,
-            iplot=iplot,
-            boxcox=boxcox,
-            opens_average_first_day=opens_average_first_day,
-            total_cap=total_cap,
-        )
+        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
         com500, net500 = make_relative_comments(
-            shen.shen.group_rets[f"group{group_num}"], zz500=1, show_nets=1
+            shen.shen.group_rets.group10, zz500=1, show_nets=1
         )
         fi1000 = daily_factor_on300500(df, zz1000=1)
-        shen = pure_moonnight(
-            fi1000,
-            groups_num=group_num,
-            value_weighted=value_weighted,
-            trade_cost_double_side=trade_cost_double_side,
-            iplot=iplot,
-            boxcox=boxcox,
-            opens_average_first_day=opens_average_first_day,
-            total_cap=total_cap,
-        )
+        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
         com1000, net1000 = make_relative_comments(
-            shen.shen.group_rets[f"group{group_num}"], zz1000=1, show_nets=1
+            shen.shen.group_rets.group10, zz1000=1, show_nets=1
         )
         if gz2000:
             fi2000 = daily_factor_on300500(df, gz2000=1)
-            shen = pure_moonnight(
-                fi2000,
-                groups_num=group_num,
-                value_weighted=value_weighted,
-                trade_cost_double_side=trade_cost_double_side,
-                iplot=iplot,
-                boxcox=boxcox,
-                opens_average_first_day=opens_average_first_day,
-                total_cap=total_cap,
-            )
+            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
             com2000, net2000 = make_relative_comments(
-                shen.shen.group_rets[f"group{group_num}"], gz2000=1, show_nets=1
+                shen.shen.group_rets.group10, gz2000=1, show_nets=1
             )
     com300 = com300.to_frame("300è¶…é¢")
     com500 = com500.to_frame("500è¶…é¢")
     com1000 = com1000.to_frame("1000è¶…é¢")
     if gz2000:
         com2000 = com2000.to_frame("2000è¶…é¢")
         coms = pd.concat([com300, com500, com1000, com2000], axis=1)
@@ -5194,15 +4733,15 @@
                 specs=[
                     [
                         None,
                         {"rowspan": 2, "colspan": 4},
                         None,
                         None,
                         None,
-                        {"rowspan": 2, "colspan": 3},
+                        {"rowspan": 2, "colspan": 5},
                         None,
                         None,
                         None,
                         None,
                     ],
                     [
                         None,
@@ -5228,15 +4767,15 @@
                 shared_yaxes=False,
                 specs=[
                     [
                         None,
                         {"rowspan": 2, "colspan": 3},
                         None,
                         None,
-                        {"rowspan": 2, "colspan": 3},
+                        {"rowspan": 2, "colspan": 6},
                         None,
                         None,
                         None,
                         None,
                         None,
                     ],
                     [
@@ -5262,26 +4801,24 @@
         tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
         tb.set_cols_dtype(["f"] * 7)
         tb.header(list(coms.T.reset_index().columns))
         tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
         print(tb.draw())
 
 
-@do_on_dfs
 class pure_star(object):
     def __init__(
         self,
         fac: pd.Series,
         code: str = None,
         price_opens: pd.Series = None,
         iplot: bool = 1,
         comments_writer: pd.ExcelWriter = None,
         net_values_writer: pd.ExcelWriter = None,
         sheetname: str = None,
-        questdb_host: str = "127.0.0.1",
     ):
         """æ‹©æ—¶å›æµ‹æ¡†æ¶ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æˆ–ä¿¡å·å€¼ï¼Œä¾æ®ä¿¡å·ä¹°å…¥å¯¹åº”çš„è‚¡ç¥¨æˆ–æŒ‡æ•°ï¼Œå¹¶è€ƒå¯Ÿç»å¯¹æ”¶ç›Šã€è¶…é¢æ”¶ç›Šå’ŒåŸºå‡†æ”¶ç›Š
         å›æµ‹æ–¹å¼ä¸ºï¼Œtæ—¥æ”¶ç›˜æ—¶è·å¾—ä¿¡å·ï¼Œt+1æ—¥å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·ä¹°å…¥ï¼Œt+2å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·å–å‡º
 
         Parameters
         ----------
         fac : pd.Series
@@ -5294,29 +4831,27 @@
             ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»©æ•ˆå’Œèµ°åŠ¿å›¾, by default 1
         comments_writer : pd.ExcelWriter, optional
             ç»©æ•ˆè¯„ä»·çš„å­˜å‚¨æ–‡ä»¶, by default None
         net_values_writer : pd.ExcelWriter, optional
             å‡€å€¼åºåˆ—çš„å­˜å‚¨æ–‡ä»¶, by default None
         sheetname : str, optional
             å­˜å‚¨æ–‡ä»¶çš„å·¥ä½œè¡¨çš„åå­—, by default None
-        questdb_host: str, optional
-            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
         """
         if code is not None:
             x1 = code.split(".")[0]
             x2 = code.split(".")[1]
             if (x1[0] == "0" or x1[:2] == "30") and x2 == "SZ":
                 kind = "stock"
             elif x1[0] == "6" and x2 == "SH":
                 kind = "stock"
             else:
                 kind = "index"
             self.kind = kind
             if kind == "index":
-                qdb = Questdb(host=questdb_host)
+                qdb = Questdb()
                 price_opens = qdb.get_data(
                     f"select date,num,close from minute_data_{kind} where code='{code}'"
                 )
                 price_opens = price_opens[price_opens.num == "1"]
                 price_opens = price_opens.set_index("date").close
                 price_opens.index = pd.to_datetime(price_opens.index, format="%Y%m%d")
             else:
@@ -5349,20 +4884,20 @@
         self.total_comments.columns = (
             self.total_nets.columns
         ) = self.total_rets.columns = ["å› å­ç»å¯¹", "å› å­è¶…é¢", "ä¹°å…¥æŒæœ‰"]
         self.total_comments = np.around(self.total_comments, 3)
         self.iplot = iplot
         self.plot()
         if comments_writer is None and sheetname is not None:
-            from pure_ocean_breeze.state.states import COMMENTS_WRITER
+            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
 
             comments_writer = COMMENTS_WRITER
             self.total_comments.to_excel(comments_writer, sheet_name=sheetname)
         if net_values_writer is None and sheetname is not None:
-            from pure_ocean_breeze.state.states import NET_VALUES_WRITER
+            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
 
             net_values_writer = NET_VALUES_WRITER
             self.total_nets.to_excel(net_values_writer, sheet_name=sheetname)
 
     def plot(self):
         coms = self.total_comments.copy().reset_index()
         if self.iplot:
@@ -5421,528 +4956,7 @@
             plt.show()
             tb = Texttable()
             tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
             tb.set_cols_dtype(["f"] * 7)
             tb.header(list(coms.T.reset_index().columns))
             tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
             print(tb.draw())
-
-
-@do_on_dfs
-def get_group(df: pd.DataFrame, group_num: int = 10) -> pd.DataFrame:
-    """ä½¿ç”¨groupbyçš„æ–¹æ³•ï¼Œå°†ä¸€ç»„å› å­å€¼æ”¹ä¸ºæˆªé¢ä¸Šçš„åˆ†ç»„å€¼ï¼Œæ­¤æ–¹æ³•ç›¸æ¯”qcutçš„æ–¹æ³•æ›´åŠ ç¨³å¥ï¼Œä½†é€Ÿåº¦æ›´æ…¢ä¸€äº›
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-    group_num : int, optional
-        åˆ†ç»„çš„æ•°é‡, by default 10
-
-    Returns
-    -------
-    pd.DataFrame
-        è½¬åŒ–ä¸ºåˆ†ç»„å€¼åçš„dfï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºåˆ†ç»„å€¼
-    """
-    a = pure_moon(no_read_indu=1)
-    df = df.stack().reset_index()
-    df.columns = ["date", "code", "fac"]
-    df = a.get_groups(df, group_num).pivot(index="date", columns="code", values="group")
-    return df
-
-
-class pure_linprog(object):
-    def __init__(
-        self,
-        facs: pd.DataFrame,
-        total_caps: pd.DataFrame = None,
-        indu_dummys: pd.DataFrame = None,
-        index_weights_hs300: pd.DataFrame = None,
-        index_weights_zz500: pd.DataFrame = None,
-        index_weights_zz1000: pd.DataFrame = None,
-        opens: pd.DataFrame = None,
-        closes: pd.DataFrame = None,
-        hs300_closes: pd.DataFrame = None,
-        zz500_closes: pd.DataFrame = None,
-        zz1000_closes: pd.DataFrame = None,
-    ) -> None:
-        """çº¿æ€§è§„åˆ’æ±‚è§£ï¼Œç›®æ ‡ä¸ºé¢„æœŸæ”¶ç›Šç‡æœ€å¤§ï¼ˆå³å› å­æ–¹å‘ä¸ºè´Ÿæ—¶ï¼Œç»„åˆå› å­å€¼æœ€å°ï¼‰
-        æ¡ä»¶ä¸ºï¼Œä¸¥æ ¼æ§åˆ¶å¸‚å€¼ä¸­æ€§ï¼ˆæ•°æ®ï¼šæ€»å¸‚å€¼çš„å¯¹æ•°ï¼›å«ä¹‰ï¼šç»„åˆåœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ä¸æŒ‡æ•°åœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ç›¸ç­‰ï¼‰
-        ä¸¥æ ¼æ§åˆ¶è¡Œä¸šä¸­æ€§ï¼ˆæ•°æ®ï¼šä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼‰ï¼Œä¸ªè‚¡åç¦»åœ¨1%ä»¥å†…ï¼Œæˆåˆ†è‚¡æƒé‡ä¹‹å’Œåœ¨80%ä»¥ä¸Š
-        åˆ†åˆ«åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä¸Šä¼˜åŒ–æ±‚è§£
-
-        Parameters
-        ----------
-        facs : pd.DataFrame
-            å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-        total_caps : pd.DataFrame, optional
-            æ€»å¸‚å€¼æ•°æ®ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ€»å¸‚å€¼, by default None
-        indu_dummys : pd.DataFrame, optional
-            è¡Œä¸šå“‘å˜é‡ï¼ŒåŒ…å«ä¸¤åˆ—åä¸ºdateçš„æ—¶é—´å’Œcodeçš„è‚¡ç¥¨ä»£ç ï¼Œä»¥åŠ30+åˆ—è¡Œä¸šå“‘å˜é‡, by default None
-        index_weights_hs300 : pd.DataFrame, optional
-            æ²ªæ·±300æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
-        index_weights_zz500 : pd.DataFrame, optional
-            ä¸­è¯500æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
-        index_weights_zz1000 : pd.DataFrame, optional
-            ä¸­è¯1000æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
-        opens : pd.DataFrame, optional
-            æ¯æœˆæœˆåˆå¼€ç›˜ä»·æ•°æ®, by default None
-        closes : pd.DataFrame, optional
-            æ¯æœˆæœˆæœ«æ”¶ç›˜ä»·æ•°æ®, by default None
-        hs300_closes : pd.DataFrame, optional
-            æ²ªæ·±300æ¯æœˆæ”¶ç›˜ä»·æ•°æ®, by default None
-        zz500_closes : pd.DataFrame, optional
-            ä¸­è¯500æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
-        zz1000_closes : pd.DataFrame, optional
-            ä¸­è¯1000æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
-        """
-        self.facs = facs.resample("M").last()
-        if total_caps is None:
-            total_caps = standardlize(
-                np.log(read_daily(total_cap=1).resample("M").last())
-            )
-        if indu_dummys is None:
-            indu_dummys = read_daily(zxindustry_dummy_code=1)
-        if index_weights_hs300 is None:
-            index_weights_hs300 = read_daily(hs300_member_weight=1)
-        if index_weights_zz500 is None:
-            index_weights_zz500 = read_daily(zz500_member_weight=1)
-        if index_weights_zz1000 is None:
-            index_weights_zz1000 = read_daily(zz1000_member_weight=1)
-        if opens is None:
-            opens = read_daily(open=1).resample("M").first()
-        if closes is None:
-            closes = read_daily(close=1).resample("M").last()
-        if hs300_closes is None:
-            hs300_closes = read_index_single("000300.SH").resample("M").last()
-        if zz500_closes is None:
-            zz500_closes = read_index_single("000905.SH").resample("M").last()
-        if zz1000_closes is None:
-            zz1000_closes = read_index_single("000852.SH").resample("M").last()
-        self.total_caps = total_caps
-        self.indu_dummys = indu_dummys
-        self.index_weights_hs300 = index_weights_hs300
-        self.index_weights_zz500 = index_weights_zz500
-        self.index_weights_zz1000 = index_weights_zz1000
-        self.hs300_weights = []
-        self.zz500_weights = []
-        self.zz1000_weights = []
-        self.ret_next = closes / opens - 1
-        self.ret_hs300 = hs300_closes.pct_change()
-        self.ret_zz500 = zz500_closes.pct_change()
-        self.ret_zz1000 = zz1000_closes.pct_change()
-
-    def optimize_one_day(
-        self,
-        fac: pd.DataFrame,
-        flow_cap: pd.DataFrame,
-        indu_dummy: pd.DataFrame,
-        index_weight: pd.DataFrame,
-        name: str,
-    ) -> pd.DataFrame:
-        """ä¼˜åŒ–å•æœŸæ±‚è§£
-
-        Parameters
-        ----------
-        fac : pd.DataFrame
-            å•æœŸå› å­å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºå› å­å€¼
-        flow_cap : pd.DataFrame
-            æµé€šå¸‚å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæˆªé¢æ ‡å‡†åŒ–çš„æµé€šå¸‚å€¼
-        indu_dummy : pd.DataFrame
-            è¡Œä¸šå“‘å˜é‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºè¡Œä¸šä»£ç ï¼Œvaluesä¸ºå“‘å˜é‡
-        index_weight : pd.DataFrame
-            æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæƒé‡
-
-        Returns
-        -------
-        pd.DataFrame
-            å½“æœŸæœ€ä½³æƒé‡
-        """
-        if fac.shape[0] > 0 and index_weight.shape[1] > 0:
-            date = fac.columns.tolist()[0]
-            codes = list(
-                set(fac.index)
-                | set(flow_cap.index)
-                | set(indu_dummy.index)
-                | set(index_weight.index)
-            )
-            fac, flow_cap, indu_dummy, index_weight = list(
-                map(
-                    lambda x: x.reindex(codes).fillna(0).to_numpy(),
-                    [fac, flow_cap, indu_dummy, index_weight],
-                )
-            )
-            sign_index_weight = np.sign(index_weight)
-            # ä¸ªè‚¡æƒé‡å¤§äºé›¶ã€åç¦»1%
-            bounds = list(
-                zip(
-                    select_max(index_weight - 0.01, 0).flatten(),
-                    select_min(index_weight + 0.01, 1).flatten(),
-                )
-            )
-            # å¸‚å€¼ä¸­æ€§+è¡Œä¸šä¸­æ€§+æƒé‡å’Œä¸º1
-            huge = np.vstack([flow_cap.T, indu_dummy.T, np.array([1] * len(codes))])
-            target = (
-                list(flow_cap.T @ index_weight.flatten())
-                + list((indu_dummy.T @ index_weight).flatten())
-                + [np.sum(index_weight)]
-            )
-            # å†™çº¿æ€§æ¡ä»¶
-            c = fac.T.flatten().tolist()
-            a = sign_index_weight.reshape((1, -1)).tolist()
-            b = [0.8]
-            # ä¼˜åŒ–æ±‚è§£
-            res = linprog(c, a, b, huge, target, bounds)
-            if res.success:
-                return pd.DataFrame({date: res.x.tolist()}, index=codes)
-            else:
-                # raise NotImplementedError(f"{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
-                logger.warning(f"{name}åœ¨{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
-                return None
-        else:
-            return None
-
-    def optimize_many_days(self, startdate: int = 20130101):
-        dates = [i for i in self.facs.index if i >= pd.Timestamp(str(startdate))]
-        for date in tqdm.auto.tqdm(dates):
-            fac = self.facs[self.facs.index == date].T.dropna()
-            total_cap = self.total_caps[self.total_caps.index == date].T.dropna()
-            indu_dummy = self.indu_dummys[self.indu_dummys.date <= date]
-            indu_dummy = (
-                indu_dummy[indu_dummy.date == indu_dummy.date.max()]
-                .drop(columns=["date"])
-                .set_index("code")
-            )
-            index_weight_hs300 = self.index_weights_hs300[
-                self.index_weights_hs300.index == date
-            ].T.dropna()
-            index_weight_zz500 = self.index_weights_zz500[
-                self.index_weights_zz500.index == date
-            ].T.dropna()
-            index_weight_zz1000 = self.index_weights_zz1000[
-                self.index_weights_zz1000.index == date
-            ].T.dropna()
-            weight_hs300 = self.optimize_one_day(
-                fac, total_cap, indu_dummy, index_weight_hs300, "hs300"
-            )
-            weight_zz500 = self.optimize_one_day(
-                fac, total_cap, indu_dummy, index_weight_zz500, "zz500"
-            )
-            weight_zz1000 = self.optimize_one_day(
-                fac, total_cap, indu_dummy, index_weight_zz1000, "zz1000"
-            )
-            self.hs300_weights.append(weight_hs300)
-            self.zz500_weights.append(weight_zz500)
-            self.zz1000_weights.append(weight_zz1000)
-        self.hs300_weights = pd.concat(self.hs300_weights, axis=1).T
-        self.zz500_weights = pd.concat(self.zz500_weights, axis=1).T
-        self.zz1000_weights = pd.concat(self.zz1000_weights, axis=1).T
-
-    def make_contrast(self, weight, index, name) -> list[pd.DataFrame]:
-        ret = (weight.shift(1) * self.ret_next).sum(axis=1)
-        abret = ret - index
-        rets = pd.concat([ret, index, abret], axis=1).dropna()
-        rets.columns = [f"{name}å¢å¼ºç»„åˆå‡€å€¼", f"{name}æŒ‡æ•°å‡€å€¼", f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"]
-        rets = (rets + 1).cumprod()
-        rets = rets.apply(lambda x: x / x.iloc[0])
-        comments = comments_on_twins(rets[f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"], abret.dropna())
-        return comments, rets
-
-    def run(self, startdate: int = 20130101) -> pd.DataFrame:
-        """è¿è¡Œè§„åˆ’æ±‚è§£
-
-        Parameters
-        ----------
-        startdate : int, optional
-            èµ·å§‹æ—¥æœŸ, by default 20130101
-
-        Returns
-        -------
-        pd.DataFrame
-            è¶…é¢ç»©æ•ˆæŒ‡æ ‡
-        """
-        self.optimize_many_days(startdate=startdate)
-        self.hs300_comments, self.hs300_nets = self.make_contrast(
-            self.hs300_weights, self.ret_hs300, "æ²ªæ·±300"
-        )
-        self.zz500_comments, self.zz500_nets = self.make_contrast(
-            self.zz500_weights, self.ret_zz500, "ä¸­è¯500"
-        )
-        self.zz1000_comments, self.zz1000_nets = self.make_contrast(
-            self.zz1000_weights, self.ret_zz1000, "ä¸­è¯1000"
-        )
-
-        figs = cf.figures(
-            pd.concat([self.hs300_nets, self.zz500_nets, self.zz1000_nets]),
-            [
-                dict(kind="line", y=list(self.hs300_nets.columns)),
-                dict(kind="line", y=list(self.zz500_nets.columns)),
-                dict(kind="line", y=list(self.zz1000_nets.columns)),
-            ],
-            asList=True,
-        )
-        base_layout = cf.tools.get_base_layout(figs)
-
-        sp = cf.subplots(
-            figs,
-            shape=(1, 3),
-            base_layout=base_layout,
-            vertical_spacing=0.15,
-            horizontal_spacing=0.03,
-            shared_yaxes=False,
-            subplot_titles=["æ²ªæ·±300å¢å¼º", "ä¸­è¯500å¢å¼º", "ä¸­è¯1000å¢å¼º"],
-        )
-        sp["layout"].update(showlegend=True)
-        cf.iplot(sp)
-
-        self.comments = pd.concat(
-            [self.hs300_comments, self.zz500_comments, self.zz1000_comments], axis=1
-        )
-        self.comments.columns = ["æ²ªæ·±300è¶…é¢", "ä¸­è¯500è¶…é¢", "ä¸­è¯1000è¶…é¢"]
-
-        from pure_ocean_breeze.state.states import COMMENTS_WRITER, NET_VALUES_WRITER
-
-        comments_writer = COMMENTS_WRITER
-        net_values_writer = NET_VALUES_WRITER
-        if comments_writer is not None:
-            self.hs300_comments.to_excel(comments_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
-            self.zz500_comments.to_excel(comments_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
-            self.zz1000_comments.to_excel(comments_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
-        if net_values_writer is not None:
-            self.hs300_nets.to_excel(net_values_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–å‡€å€¼")
-            self.zz500_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–å‡€å€¼")
-            self.zz1000_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–å‡€å€¼")
-
-        return self.comments.T
-
-
-def symmetrically_orthogonalize(dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:
-    """å¯¹å¤šä¸ªå› å­åšå¯¹ç§°æ­£äº¤ï¼Œæ¯ä¸ªå› å­å¾—åˆ°æ­£äº¤å…¶ä»–å› å­åçš„ç»“æœ
-
-    Parameters
-    ----------
-    dfs : list[pd.DataFrame]
-        å¤šä¸ªè¦åšæ­£äº¤çš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
-
-    Returns
-    -------
-    list[pd.DataFrame]
-        å¯¹ç§°æ­£äº¤åçš„å„ä¸ªå› å­
-    """
-
-    def sing(dfs: list[pd.DataFrame], date: pd.Timestamp):
-        dds = []
-        for num, i in enumerate(dfs):
-            i = i[i.index == date]
-            i.index = [f"fac{num}"]
-            i = i.T
-            dds.append(i)
-        dds = pd.concat(dds, axis=1)
-        cov = dds.cov()
-        d, u = np.linalg.eig(cov)
-        d = np.diag(d ** (-0.5))
-        new_facs = pd.DataFrame(
-            np.dot(dds, np.dot(np.dot(u, d), u.T)), columns=dds.columns, index=dds.index
-        )
-        new_facs = new_facs.stack().reset_index()
-        new_facs.columns = ["code", "fac_number", "fac"]
-        new_facs = new_facs.assign(date=date)
-        dds = []
-        for num, i in enumerate(dfs):
-            i = new_facs[new_facs.fac_number == f"fac{num}"]
-            i = i.pivot(index="date", columns="code", values="fac")
-            dds.append(i)
-        return dds
-
-    dfs = [standardlize(i) for i in dfs]
-    date_first = max([i.index.min() for i in dfs])
-    date_last = min([i.index.max() for i in dfs])
-    dfs = [i[(i.index >= date_first) & (i.index <= date_last)] for i in dfs]
-    fac_num = len(dfs)
-    ddss = [[] for i in range(fac_num)]
-    for date in tqdm.auto.tqdm(dfs[0].index):
-        dds = sing(dfs, date)
-        for num, i in enumerate(dds):
-            ddss[num].append(i)
-    ds = []
-    for i in tqdm.auto.tqdm(ddss):
-        ds.append(pd.concat(i))
-    return ds
-
-
-def icir_weight(
-    facs: list[pd.DataFrame],
-    backsee: int = 6,
-    boxcox: bool = 0,
-    rank_corr: bool = 0,
-    only_ic: bool = 0,
-) -> pd.DataFrame:
-    """ä½¿ç”¨iciræ»šåŠ¨åŠ æƒçš„æ–¹å¼ï¼ŒåŠ æƒåˆæˆå‡ ä¸ªå› å­
-
-    Parameters
-    ----------
-    facs : list[pd.DataFrame]
-        è¦åˆæˆçš„è‹¥å¹²å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
-    backsee : int, optional
-        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
-    boxcox : bool, optional
-        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
-    rank_corr : bool, optional
-        æ˜¯å¦è®¡ç®—rankicir, by default 0
-    only_ic : bool, optional
-        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0
-
-    Returns
-    -------
-    pd.DataFrame
-        åˆæˆåçš„å› å­
-
-    Raises
-    ------
-    ValueError
-        å› å­æœŸæ•°å°‘äºå›çœ‹æœŸæ•°æ—¶å°†æŠ¥é”™
-    """
-    date_first_max = max([i.index[0] for i in facs])
-    facs = [i[i.index >= date_first_max] for i in facs]
-    date_last_min = min([i.index[-1] for i in facs])
-    facs = [i[i.index <= date_last_min] for i in facs]
-    facs = [i.shift(1) for i in facs]
-    ret = read_daily(
-        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
-    )
-    ret = ret / ret.shift(20) - 1
-    if boxcox:
-        facs = [decap_industry(i) for i in facs]
-    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
-    dates = list(facs[0].index)
-    fis = []
-    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
-        if num < backsee:
-            ...
-        else:
-            nears = [i.iloc[num - backsee : num, :] for i in facs]
-            targets = [i[i.index == date] for i in facs]
-            if rank_corr:
-                weights = [
-                    show_corr(
-                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
-                    )
-                    for i in nears
-                ]
-            else:
-                weights = [
-                    show_corr(
-                        i,
-                        ret[ret.index.isin(i.index)],
-                        plt_plot=0,
-                        show_series=1,
-                        method="pearson",
-                    )
-                    for i in nears
-                ]
-            if only_ic:
-                weights = [i.mean() for i in weights]
-            else:
-                weights = [i.mean() / i.std() for i in weights]
-            fi = sum([i * j for i, j in zip(weights, targets)])
-            fis.append(fi)
-    if len(fis) > 0:
-        return pd.concat(fis).shift(-1)
-    else:
-        raise ValueError("è¾“å…¥çš„å› å­å€¼é•¿åº¦ä¸å¤ªå¤Ÿå§ï¼Ÿ")
-
-
-def scipy_weight(
-    facs: list[pd.DataFrame],
-    backsee: int = 6,
-    boxcox: bool = 0,
-    rank_corr: bool = 0,
-    only_ic: bool = 0,
-    upper_bound: float = None,
-    lower_bound: float = 0,
-) -> pd.DataFrame:
-    """ä½¿ç”¨scipyçš„minimizeä¼˜åŒ–æ±‚è§£çš„æ–¹å¼ï¼Œå¯»æ‰¾æœ€ä¼˜çš„å› å­åˆæˆæƒé‡ï¼Œé»˜è®¤ä¼˜åŒ–æ¡ä»¶ä¸ºæœ€å¤§ICIR
-
-    Parameters
-    ----------
-    facs : list[pd.DataFrame]
-        è¦åˆæˆçš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
-    backsee : int, optional
-        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
-    boxcox : bool, optional
-        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
-    rank_corr : bool, optional
-        æ˜¯å¦è®¡ç®—rankicir, by default 0
-    only_ic : bool, optional
-        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0
-    upper_bound : float, optional
-        æ¯ä¸ªå› å­çš„æƒé‡ä¸Šé™ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œåˆ™ä¸ºæ¯ä¸ªå› å­å¹³å‡æƒé‡çš„2å€ï¼Œå³2é™¤ä»¥å› å­æ•°é‡, by default None
-    lower_bound : float, optional
-        æ¯ä¸ªå› å­çš„æƒé‡ä¸‹é™, by default 0
-
-    Returns
-    -------
-    pd.DataFrame
-        åˆæˆåçš„å› å­
-    """
-    date_first_max = max([i.index[0] for i in facs])
-    facs = [i[i.index >= date_first_max] for i in facs]
-    date_last_min = min([i.index[-1] for i in facs])
-    facs = [i[i.index <= date_last_min] for i in facs]
-    facs = [i.shift(1) for i in facs]
-    ret = read_daily(
-        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
-    )
-    ret = ret / ret.shift(20) - 1
-    if boxcox:
-        facs = [decap_industry(i) for i in facs]
-    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
-    if upper_bound is None:
-        upper_bound = 2 / len(facs)
-    dates = list(facs[0].index)
-    fis = []
-    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
-        if num <= backsee:
-            ...
-        else:
-            nears = [i.iloc[num - backsee : num, :] for i in facs]
-            targets = [i[i.index == date] for i in facs]
-            if rank_corr:
-                weights = [
-                    show_corr(
-                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
-                    )
-                    for i in nears
-                ]
-            else:
-                weights = [
-                    show_corr(
-                        i,
-                        ret[ret.index.isin(i.index)],
-                        plt_plot=0,
-                        show_series=1,
-                        method="pearson",
-                    )
-                    for i in nears
-                ]
-            if only_ic:
-                weights = [i.mean() for i in weights]
-            else:
-                weights = [i.mean() / i.std() for i in weights]
-            weights = pd.concat(weights, axis=1)
-
-            def func(x):
-                w = np.array(x).reshape((-1, 1))
-                y = weights @ w
-                return np.mean(y) / np.std(y)
-
-            cons = {"type": "eq", "fun": lambda x: np.sum(x) - 1}
-            res = minimize(
-                func,
-                np.random.rand(weights.shape[1], 1),
-                constraints=cons,
-                bounds=[(lower_bound, upper_bound)] * weights.shape[1],
-            )
-            xs = res.x.tolist()
-            fac = sum([i * j for i, j in zip(xs, targets)])
-            fis.append(fac)
-    return pd.concat(fis).shift(-1)
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/initialize.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1/pure_ocean_breeze.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1/pure_ocean_breeze.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/initialize.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v1p10/pure_ocean_breeze.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v1p10/pure_ocean_breeze.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/initialize.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v2/pure_ocean_breeze.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v2/pure_ocean_breeze.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/__init__.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/__init__.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/database.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/database.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/dicts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/dicts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/read_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/read_data.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/tools.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/tools.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/data/write_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/data/write_data.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/half_way.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/half_way.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/future_version/in_thoughts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/future_version/in_thoughts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/initialize/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/initialize/initialize.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/comment.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/comment.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/labor/process.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/labor/process.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/mail/email.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/mail/email.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/decorators.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/decorators.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/state/homeplace.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/state/homeplace.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p1/withs/requires.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p1/withs/requires.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/__init__.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/__init__.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/__init__.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/__init__.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/database.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/database.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/dicts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/dicts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/read_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/read_data.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/tools.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/tools.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/data/write_data.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/data/write_data.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/half_way.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/half_way.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/future_version/in_thoughts.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/future_version/in_thoughts.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/initialize/initialize.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/initialize/initialize.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import pickle
 import os
 
 
-def initialize():
+def ini():
     user_file = os.path.expanduser("~") + "/"
     # æ—¥é¢‘æ•°æ®è·¯å¾„
     daily_data_file = input("è¯·è®¾ç½®æ—¥é¢‘æ•°æ®å­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/')ï¼š")
     while "/" not in daily_data_file:
         print("è¯·ä¸è¦è¾“å…¥åæ–œæ ''ï¼Œè¯·æ›¿æ¢ä¸º'/'ï¼Œå¹¶ä»¥'/'ç»“å°¾")
         daily_data_file = input("è¯·è®¾ç½®æ—¥é¢‘æ•°æ®å­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/')ï¼š")
     if daily_data_file[-1] != "/":
@@ -35,23 +35,31 @@
     # æœ€ç»ˆå› å­æˆæœè·¯å¾„
     final_factor_file = input("è¯·è®¾ç½®æœ€ç»ˆå› å­æˆæœå­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/')ï¼š")
     while "/" not in final_factor_file:
         print("è¯·ä¸è¦è¾“å…¥åæ–œæ ''ï¼Œè¯·æ›¿æ¢ä¸º'/'ï¼Œå¹¶ä»¥'/'ç»“å°¾")
         final_factor_file = input("è¯·è®¾ç½®æœ€ç»ˆå› å­æˆæœå­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/')ï¼š")
     if final_factor_file[-1] != "/":
         final_factor_file = final_factor_file + "/"
+    # è‚¡ç¥¨é€ç¬”æ•°æ®è·¯å¾„
+    tick_by_tick_data=input("è¯·è®¾ç½®è‚¡ç¥¨é€ç¬”æ•°æ®å­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/)ï¼š")
+    while "/" not in tick_by_tick_data:
+        print("è¯·ä¸è¦è¾“å…¥åæ–œæ ''ï¼Œè¯·æ›¿æ¢ä¸º'/'ï¼Œå¹¶ä»¥'/'ç»“å°¾")
+        tick_by_tick_data=input("è¯·è®¾ç½®è‚¡ç¥¨é€ç¬”æ•°æ®å­˜æ”¾è·¯å¾„(è¯·æœ€ç»ˆä»¥æ–œæ ç»“å°¾ï¼Œè¯·ä¸è¦è¾“å…¥åæ–œæ '',è¯·éƒ½æ›¿æ¢ä¸º'/)ï¼š")
+    if tick_by_tick_data[-1]!='/':
+        tick_by_tick_data=tick_by_tick_data+'/'
     # æ•°ç«‹æ–¹token
     api_token = input("è¯·è¾“å…¥æ‚¨çš„æ•°ç«‹æ–¹tokenï¼š")
     save_dict = {
         "daily_data_file": daily_data_file,
         "factor_data_file": factor_data_file,
         "barra_data_file": barra_data_file,
         "update_data_file": update_data_file,
         "final_factor_file": final_factor_file,
         "api_token": api_token,
+        'tick_by_tick_data': tick_by_tick_data,
     }
     save_dict_file = open(user_file + "paths.settings", "wb")
     pickle.dump(save_dict, save_dict_file)
     save_dict_file.close()
     from loguru import logger
 
     logger.success("æ­å–œä½ ï¼Œå›æµ‹æ¡†æ¶åˆå§‹åŒ–å®Œæˆï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨äº†ğŸ‘")
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/comment.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/labor/comment.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/labor/process.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/labor/process.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,93 +1,103 @@
-__updated__ = "2022-11-05 00:16:56"
+__updated__ = "2023-06-28 16:10:41"
 
 import warnings
 
 warnings.filterwarnings("ignore")
 import numpy as np
 import pandas as pd
 import knockknock as kk
 import os
-import tqdm
+import tqdm.auto
 import scipy.stats as ss
+from scipy.optimize import linprog
 import statsmodels.formula.api as smf
 import matplotlib as mpl
 
 mpl.rcParams.update(mpl.rcParamsDefault)
 import matplotlib.pyplot as plt
 
-plt.style.use(["science", "no-latex", "notebook"])
 plt.rcParams["axes.unicode_minus"] = False
 
 from functools import reduce, lru_cache, partial
 from dateutil.relativedelta import relativedelta
 from loguru import logger
 import datetime
 from collections.abc import Iterable
 import plotly.express as pe
 import plotly.io as pio
 from plotly.tools import FigureFactory as FF
-import plotly.graph_objects as go
-import plotly.tools as plyoo
 import pyfinance.ols as po
 from texttable import Texttable
 from xpinyin import Pinyin
+import tradetime as tt
 import cufflinks as cf
+import deprecation
+from mpire import WorkerPool
+from scipy.optimize import minimize
+from pure_ocean_breeze import __version__
 
 cf.set_config_file(offline=True)
-from typing import Callable, Union
-from pure_ocean_breeze.legacy_version.v3p4.data.read_data import (
+from typing import Callable, Union, Dict, List, Tuple
+from pure_ocean_breeze.data.read_data import (
     read_daily,
     read_market,
     get_industry_dummies,
     read_swindustry_prices,
     read_zxindustry_prices,
     database_read_final_factors,
+    read_index_single,
 )
-from pure_ocean_breeze.legacy_version.v3p4.state.homeplace import HomePlace
+from pure_ocean_breeze.state.homeplace import HomePlace
 
-homeplace = HomePlace()
-from pure_ocean_breeze.legacy_version.v3p4.state.states import STATES, is_notebook
-from pure_ocean_breeze.legacy_version.v3p4.data.database import *
-from pure_ocean_breeze.legacy_version.v3p4.data.dicts import INDUS_DICT
-from pure_ocean_breeze.legacy_version.v3p4.data.tools import (
+try:
+    homeplace = HomePlace()
+except Exception:
+    print("æ‚¨æš‚æœªåˆå§‹åŒ–ï¼ŒåŠŸèƒ½å°†å—é™")
+from pure_ocean_breeze.state.states import STATES
+from pure_ocean_breeze.state.decorators import do_on_dfs
+from pure_ocean_breeze.data.database import *
+from pure_ocean_breeze.data.dicts import INDUS_DICT
+from pure_ocean_breeze.data.tools import (
     indus_name,
     drop_duplicates_index,
     to_percent,
     to_group,
+    standardlize,
+    select_max,
+    select_min,
+    merge_many,
 )
-from pure_ocean_breeze.legacy_version.v3p4.labor.comment import (
+from pure_ocean_breeze.labor.comment import (
     comments_on_twins,
     make_relative_comments,
     make_relative_comments_plot,
 )
 
 
+@do_on_dfs
 def daily_factor_on300500(
     fac: pd.DataFrame,
     hs300: bool = 0,
     zz500: bool = 0,
-    zz800: bool = 0,
     zz1000: bool = 0,
     gz2000: bool = 0,
     other: bool = 0,
 ) -> pd.DataFrame:
     """è¾“å…¥æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå°†å…¶é™å®šåœ¨æŸæŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨æ± å†…ï¼Œ
-    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯800ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡
+    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œä»¥åŠè¿™å››ç§æŒ‡æ•°æˆåˆ†è‚¡çš„ç»„åˆå åŠ ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡
 
     Parameters
     ----------
     fac : pd.DataFrame
         æœªé™å®šè‚¡ç¥¨æ± çš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     hs300 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºæ²ªæ·±300, by default 0
     zz500 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯500, by default 0
-    zz800 : bool, optional
-        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯800, by default 0
     zz1000 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯1000, by default 0
     gz2000 : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºå›½è¯2000, by default 0
     other : bool, optional
         é™å®šè‚¡ç¥¨æ± ä¸ºé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡, by default 0
 
@@ -99,142 +109,105 @@
     Raises
     ------
     `ValueError`
         å¦‚æœæœªæŒ‡å®šä»»ä½•ä¸€ç§æŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œå°†æŠ¥é”™
     """
     last = fac.resample("M").last()
     homeplace = HomePlace()
+    dummies = []
     if fac.shape[0] / last.shape[0] > 2:
         if hs300:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz500:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz800:
-            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
-            df1 = df1.set_index(list(df1.columns)[0])
-            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
-            df2 = df2.set_index(list(df2.columns)[0])
-            df = df1 + df2
-            df = df.replace(0, np.nan)
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz1000:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
-            df = df * fac
-            df = df.dropna(how="all")
-        elif gz2000:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
-            df = df * fac
-            df = df.dropna(how="all")
-        elif other:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
+            dummies.append(df)
+        if zz500:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
+            dummies.append(df)
+        if zz1000:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
+            dummies.append(df)
+        if gz2000:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
+            dummies.append(df)
+        if other:
             tr = read_daily(tr=1).fillna(0).replace(0, 1)
             tr = np.sign(tr)
             df1 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df1 = df1.set_index(list(df1.columns)[0])
             df2 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df2 = df2.set_index(list(df2.columns)[0])
             df3 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df3 = df3.set_index(list(df3.columns)[0])
             df = (1 - df1) * (1 - df2) * (1 - df3) * tr
             df = df.replace(0, np.nan) * fac
             df = df.dropna(how="all")
-        else:
+        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
             raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
     else:
         if hs300:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
             df = df.resample("M").last()
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz500:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
+            dummies.append(df)
+        if zz500:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
             df = df.resample("M").last()
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz800:
-            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
-            df1 = df1.set_index(list(df1.columns)[0])
-            df1 = df1.resample("M").last()
-            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
-            df2 = df2.set_index(list(df2.columns)[0])
-            df2 = df2.resample("M").last()
-            df = df1 + df2
-            df = df.replace(0, np.nan)
-            df = df * fac
-            df = df.dropna(how="all")
-        elif zz1000:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
+            dummies.append(df)
+        if zz1000:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
             df = df.resample("M").last()
-            df = df * fac
-            df = df.dropna(how="all")
-        elif gz2000:
-            df = pd.read_feather(
-                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
-            ).replace(0, np.nan)
-            df = df.set_index(list(df.columns)[0])
+            dummies.append(df)
+        if gz2000:
+            df = pd.read_parquet(
+                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
+            ).fillna(0)
             df = df.resample("M").last()
-            df = df * fac
-            df = df.dropna(how="all")
-        elif other:
+            dummies.append(df)
+        if other:
             tr = read_daily(tr=1).fillna(0).replace(0, 1).resample("M").last()
             tr = np.sign(tr)
             df1 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df1 = df1.set_index(list(df1.columns)[0])
             df1 = df1.resample("M").last()
             df2 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df2 = df2.set_index(list(df2.columns)[0])
             df2 = df2.resample("M").last()
             df3 = (
-                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
+                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
             ).fillna(0)
-            df3 = df3.set_index(list(df3.columns)[0])
             df3 = df3.resample("M").last()
             df = (1 - df1) * (1 - df2) * (1 - df3)
             df = df.replace(0, np.nan) * fac
             df = df.dropna(how="all")
-        else:
+        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
             raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
+    if len(dummies) > 0:
+        dummies = sum(dummies).replace(0, np.nan)
+        df = (dummies * fac).dropna(how="all")
     return df
 
 
+@do_on_dfs
 def daily_factor_on_industry(
     df: pd.DataFrame, swindustry: bool = 0, zxindustry: bool = 0
 ) -> dict:
     """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨
 
     Parameters
     ----------
@@ -265,29 +238,33 @@
         swindustry=swindustry,
         zxindustry=zxindustry,
     )
     ress = {k: v * df for k, v in ress.items()}
     return ress
 
 
+@do_on_dfs
 def group_test_on_industry(
     df: pd.DataFrame,
     group_num: int = 10,
+    trade_cost_double_side: float = 0,
     net_values_writer: pd.ExcelWriter = None,
     swindustry: bool = 0,
     zxindustry: bool = 0,
 ) -> pd.DataFrame:
     """åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•æ¯ä¸ªè¡Œä¸šçš„åˆ†ç»„å›æµ‹
 
     Parameters
     ----------
     df : pd.DataFrame
         å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
     group_num : int, optional
         åˆ†ç»„æ•°é‡, by default 10
+    trade_cost_double_side : float, optional
+        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
     net_values_writer : pd.ExcelWriter, optional
         ç”¨äºå­˜å‚¨å„ä¸ªè¡Œä¸šåˆ†ç»„åŠå¤šç©ºå¯¹å†²å‡€å€¼åºåˆ—çš„excelæ–‡ä»¶, by default None
     swindustry : bool, optional
         é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
     zxindustry : bool, optional
         é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
 
@@ -301,38 +278,41 @@
     ks = []
     vs = []
     if swindustry:
         for k, v in dfs.items():
             shen = pure_moonnight(
                 v,
                 groups_num=group_num,
+                trade_cost_double_side=trade_cost_double_side,
                 net_values_writer=net_values_writer,
                 sheetname=INDUS_DICT[k],
                 plt_plot=0,
             )
             ks.append(k)
             vs.append(shen.shen.total_comments.T)
         vs = pd.concat(vs)
         vs.index = [INDUS_DICT[i] for i in ks]
     else:
         for k, v in dfs.items():
             shen = pure_moonnight(
                 v,
                 groups_num=group_num,
+                trade_cost_double_side=trade_cost_double_side,
                 net_values_writer=net_values_writer,
                 sheetname=k,
                 plt_plot=0,
             )
             ks.append(k)
             vs.append(shen.shen.total_comments.T)
         vs = pd.concat(vs)
         vs.index = ks
     return vs
 
 
+@do_on_dfs
 def rankic_test_on_industry(
     df: pd.DataFrame,
     excel_name: str = "è¡Œä¸šrankic.xlsx",
     png_name: str = "è¡Œä¸šrankicå›¾.png",
     swindustry: bool = 0,
     zxindustry: bool = 0,
 ) -> pd.DataFrame:
@@ -354,30 +334,32 @@
     Returns
     -------
     pd.DataFrame
         è¡Œä¸šåç§°ä¸å¯¹åº”çš„Rank IC
     """
     vs = group_test_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)
     rankics = vs[["RankIC"]].T
-    rankics.to_excel(excel_name)
+    if excel_name is not None:
+        rankics.to_excel(excel_name)
     rankics.plot(kind="bar")
     plt.show()
     plt.savefig(png_name)
     return rankics
 
 
+@do_on_dfs
 def long_test_on_industry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
-) -> list[dict]:
+) -> List[dict]:
     """å¯¹æ¯ä¸ªç”³ä¸‡/ä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -390,32 +372,32 @@
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     swindustry : bool, optional
         åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
     zxindusrty : bool, optional
         åœ¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
     Returns
     -------
-    list[dict]
+    List[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
     fac = decap_industry(df, monthly=True)
 
     if swindustry:
-        industry_dummy = pd.read_feather(
-            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
+        industry_dummy = pd.read_parquet(
+            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
         ).fillna(0)
         indus = read_swindustry_prices()
     else:
-        industry_dummy = pd.read_feather(
-            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.feather"
+        industry_dummy = pd.read_parquet(
+            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.parquet"
         ).fillna(0)
         indus = read_zxindustry_prices()
     inds = list(industry_dummy.columns)
     ret_next = (
         read_daily(close=1).resample("M").last()
         / read_daily(open=1).resample("M").first()
         - 1
@@ -450,24 +432,18 @@
         fi = fi.T.apply(sing).T
         fi = fi.replace(0, np.nan)
         fi = fi * ret_next
         ret_long = fi.mean(axis=1)
         return ret_long
 
     ret_longs = {k: [] for k in nums}
-    if is_notebook():
-        for num in tqdm.tqdm_notebook(nums):
-            for code in inds[2:]:
-                df = save_ind(code, num).to_frame(code)
-                ret_longs[num] = ret_longs[num] + [df]
-    else:
-        for num in tqdm.tqdm(nums):
-            for code in inds[2:]:
-                df = save_ind(code, num).to_frame(code)
-                ret_longs[num] = ret_longs[num] + [df]
+    for num in tqdm.auto.tqdm(nums):
+        for code in inds[2:]:
+            df = save_ind(code, num).to_frame(code)
+            ret_longs[num] = ret_longs[num] + [df]
 
     indus = indus.resample("M").last().pct_change()
 
     if swindustry:
         coms = {
             k: indus_name(pd.concat(v, axis=1).dropna(how="all").T).T
             for k, v in ret_longs.items()
@@ -548,22 +524,18 @@
                     raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
                 return tuple(thr.index)
 
             fi = fi.T.apply(sing)
             return fi
 
         stocks_longs = {k: {} for k in nums}
-        if is_notebook():
-            for num in tqdm.tqdm_notebook(nums):
-                for code in inds[2:]:
-                    stocks_longs[num][code] = save_ind_stocks(code, num)
-        else:
-            for num in tqdm.tqdm(nums):
-                for code in inds[2:]:
-                    stocks_longs[num][code] = save_ind_stocks(code, num)
+
+        for num in tqdm.auto.tqdm(nums):
+            for code in inds[2:]:
+                stocks_longs[num][code] = save_ind_stocks(code, num)
 
         for num in nums:
             w1 = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šä¹°{num}åªçš„è‚¡ç¥¨åå•.xlsx")
             for k, v in stocks_longs[num].items():
                 v = v.T
                 v.index = v.index.strftime("%Y/%m/%d")
                 v.to_excel(w1, sheet_name=INDUS_DICT[k])
@@ -571,21 +543,22 @@
             w1.close()
 
         return [coms_finals, rets_save, stocks_longs]
     else:
         return [coms_finals, rets_save]
 
 
+@do_on_dfs
 def long_test_on_swindustry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
-) -> list[dict]:
+) -> List[dict]:
     """å¯¹æ¯ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -594,15 +567,15 @@
         å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
     neg : bool, optional
         å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
     save_stock_list : bool, optional
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     Returns
     -------
-    list[dict]
+    List[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
@@ -613,21 +586,22 @@
         neg=neg,
         save_stock_list=save_stock_list,
         swindustry=1,
     )
     return res
 
 
+@do_on_dfs
 def long_test_on_zxindustry(
     df: pd.DataFrame,
     nums: list,
     pos: bool = 0,
     neg: bool = 0,
     save_stock_list: bool = 0,
-) -> list[dict]:
+) -> List[dict]:
     """å¯¹æ¯ä¸ªä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Parameters
     ----------
     df : pd.DataFrame
         ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
     nums : list
@@ -636,15 +610,15 @@
         å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
     neg : bool, optional
         å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
     save_stock_list : bool, optional
         æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
     Returns
     -------
-    list[dict]
+    List[dict]
         è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•
 
     Raises
     ------
     IOError
         poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
     """
@@ -655,14 +629,15 @@
         neg=neg,
         save_stock_list=save_stock_list,
         zxindustry=1,
     )
     return res
 
 
+@do_on_dfs
 @kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
 def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
     """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–
 
     Parameters
     ----------
     df : pd.DataFrame
@@ -678,18 +653,15 @@
         å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
 
     Raises
     ------
     `NotImplementedError`
         å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
     """
-    if is_notebook():
-        tqdm.tqdm_notebook().pandas()
-    else:
-        tqdm.tqdm.pandas()
+    tqdm.auto.tqdm.pandas()
     share = read_daily(sharenum=1)
     undi_close = read_daily(close=1, unadjust=1)
     cap = (share * undi_close).stack().reset_index()
     cap.columns = ["date", "code", "cap"]
     cap.cap = ss.boxcox(cap.cap)[0]
 
     def single(x):
@@ -710,14 +682,15 @@
     elif monthly:
         df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
     else:
         raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
     return df
 
 
+@do_on_dfs
 @kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
 def decap_industry(
     df: pd.DataFrame,
     daily: bool = 0,
     monthly: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
@@ -786,51 +759,49 @@
         df.fac = df.fac - ols_w * df.cap - ols_b
         for k, v in ols_bs.items():
             df.fac = df.fac - v * df[k]
         df = df[["fac"]]
         return df
 
     if swindustry:
-        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
+        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
     else:
-        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"
+        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"
 
     if monthly:
         industry_dummy = (
-            pd.read_feather(homeplace.daily_data_file + file_name)
+            pd.read_parquet(homeplace.daily_data_file + file_name)
             .fillna(0)
             .set_index("date")
             .groupby("code")
             .resample("M")
             .last()
         )
         industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
         industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
         col = ["code", "date"] + industry_ws
     elif daily:
-        industry_dummy = pd.read_feather(homeplace.daily_data_file + file_name).fillna(
+        industry_dummy = pd.read_parquet(homeplace.daily_data_file + file_name).fillna(
             0
         )
         industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
         col = ["date", "code"] + industry_ws
     else:
         raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
     industry_dummy.columns = col
     df = pd.merge(df, industry_dummy, on=["date", "code"])
     df = df.set_index(["date", "code"])
-    if is_notebook():
-        tqdm.tqdm_notebook().pandas()
-    else:
-        tqdm.tqdm.pandas()
+    tqdm.auto.tqdm.pandas()
     df = df.groupby(["date"]).progress_apply(neutralize_factors)
     df = df.unstack()
     df.columns = [i[1] for i in list(df.columns)]
     return df
 
 
+@do_on_dfs
 def deboth(df: pd.DataFrame) -> pd.DataFrame:
     """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–
 
     Parameters
     ----------
     df : pd.DataFrame
         æœªä¸­æ€§åŒ–çš„å› å­
@@ -840,17 +811,33 @@
     `pd.DataFrame`
         è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
     """
     shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
     return shen()
 
 
+@do_on_dfs
+def boom_one(
+    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
+) -> pd.DataFrame:
+    if min_periods is None:
+        min_periods = int(backsee * 0.5)
+    if not daily:
+        df_mean = (
+            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
+        )
+    else:
+        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
+    return df_mean
+
+
+@do_on_dfs
 def boom_four(
     df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
-) -> tuple[pd.DataFrame]:
+) -> Tuple[pd.DataFrame]:
     """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­
 
     Parameters
     ----------
     df : pd.DataFrame
         åŸæ—¥é¢‘å› å­
     backsee : int, optional
@@ -858,15 +845,15 @@
     daily : bool, optional
         ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
     min_periods : int, optional
         rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ
 
     Returns
     -------
-    `tuple[pd.DataFrame]`
+    `Tuple[pd.DataFrame]`
         6ä¸ªå› å­çš„å…ƒç»„
     """
     if min_periods is None:
         min_periods = int(backsee * 0.5)
     if not daily:
         df_mean = (
             df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
@@ -882,14 +869,42 @@
         twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
         rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
         twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
         rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
     return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus
 
 
+def boom_fours(
+    dfs: List[pd.DataFrame],
+    backsee: Union[int, List[int]] = 20,
+    daily: Union[bool, List[bool]] = 0,
+    min_periods: Union[int, List[int]] = None,
+) -> List[List[pd.DataFrame]]:
+    """å¯¹å¤šä¸ªå› å­ï¼Œæ¯ä¸ªå› å­éƒ½è¿›è¡Œboom_fourçš„æ“ä½œ
+
+    Parameters
+    ----------
+    dfs : List[pd.DataFrame]
+        å¤šä¸ªå› å­çš„dataframeç»„æˆçš„list
+    backsee : Union[int,List[int]], optional
+        æ¯ä¸ªå› å­å›çœ‹æœŸæ•°, by default 20
+    daily : Union[bool,List[bool]], optional
+        æ¯ä¸ªå› å­æ˜¯å¦é€æ—¥è®¡ç®—, by default 0
+    min_periods : Union[int,List[int]], optional
+        æ¯ä¸ªå› å­è®¡ç®—çš„æœ€å°æœŸ, by default None
+
+    Returns
+    -------
+    List[List[pd.DataFrame]]
+        æ¯ä¸ªå› å­è¿›è¡Œboom_fouråçš„ç»“æœ
+    """
+    return boom_four(df=dfs, backsee=backsee, daily=daily, min_periods=min_periods)
+
+
+@do_on_dfs
 def add_cross_standardlize(*args: list) -> pd.DataFrame:
     """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 
 
     Returns
     -------
     `pd.DataFrame`
         åˆæˆåçš„å› å­
@@ -897,37 +912,42 @@
     fms = [pure_fallmount(i) for i in args]
     one = fms[0]
     others = fms[1:]
     final = one + others
     return final()
 
 
+@do_on_dfs
 def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
     """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥
 
     Parameters
     ----------
     df : pd.DataFrame
         indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©
 
     Returns
     -------
     `pd.DataFrame`
         ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
     """
     """"""
-    trs = read_daily(tr=1)
+    start = df.index.min()
+    start = start - pd.tseries.offsets.MonthBegin()
+    start = datetime.datetime.strftime(start, "%Y%m%d")
+    trs = read_daily(tr=1, start=start)
     trs = trs.assign(tradeends=list(trs.index))
     trs = trs[["tradeends"]]
     trs = trs.resample("M").last()
     df = pd.concat([trs, df], axis=1)
     df = df.set_index(["tradeends"])
     return df
 
 
+@do_on_dfs
 def market_kind(
     df: pd.DataFrame,
     zhuban: bool = 0,
     chuangye: bool = 0,
     kechuang: bool = 0,
     beijing: bool = 0,
 ) -> pd.DataFrame:
@@ -1001,38 +1021,41 @@
         è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼
 
     Returns
     -------
     `float`
         å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
     """
-    corr = show_x_with_func(fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1])
+    if method == "spearman":
+        corr = show_x_with_func(fac1, fac2, lambda x: x.rank().corr().iloc[0, 1])
+    else:
+        corr = show_x_with_func(fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1])
     if show_series:
         return corr
     else:
         if plt_plot:
             corr.plot(rot=60)
             plt.show()
         return corr.mean()
 
 
 def show_corrs(
-    factors: list[pd.DataFrame],
-    factor_names: list[str] = None,
+    factors: List[pd.DataFrame],
+    factor_names: List[str] = None,
     print_bool: bool = True,
     show_percent: bool = True,
     method: str = "spearman",
 ) -> pd.DataFrame:
     """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§
 
     Parameters
     ----------
-    factors : list[pd.DataFrame]
+    factors : List[pd.DataFrame]
         æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
-    factor_names : list[str], optional
+    factor_names : List[str], optional
         ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
     print_bool : bool, optional
         æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
     show_percent : bool, optional
         æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
     method : str, optional
         è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
@@ -1054,16 +1077,17 @@
     corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
     np.fill_diagonal(corrs.to_numpy(), 1)
     if show_percent:
         pcorrs = corrs.applymap(to_percent)
     else:
         pcorrs = corrs.copy()
     if print_bool:
-        print(pcorrs)
-    return corrs
+        return pcorrs
+    else:
+        return corrs
 
 
 def show_cov(
     fac1: pd.DataFrame,
     fac2: pd.DataFrame,
     plt_plot: bool = 1,
     show_series: bool = 0,
@@ -1126,34 +1150,31 @@
     befo1.columns = ["date", "code", "befo"]
     twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
     corr = twins.groupby("date").apply(the_func)
     return corr
 
 
 def show_covs(
-    factors: list[pd.DataFrame],
-    factor_names: list[str] = None,
+    factors: List[pd.DataFrame],
+    factor_names: List[str] = None,
     print_bool: bool = True,
     show_percent: bool = True,
-    method: str = "spearman",
 ) -> pd.DataFrame:
     """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§
 
     Parameters
     ----------
-    factors : list[pd.DataFrame]
+    factors : List[pd.DataFrame]
         æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
-    factor_names : list[str], optional
+    factor_names : List[str], optional
         ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
     print_bool : bool, optional
         æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
     show_percent : bool, optional
         æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
-    method : str, optional
-        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
 
     Returns
     -------
     `pd.DataFrame`
         ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
     """
     corrs = []
@@ -1173,82 +1194,166 @@
         pcorrs = corrs.copy()
     if print_bool:
         print(pcorrs)
     return corrs
 
 
 def de_cross(
-    y: pd.DataFrame, xs: Union[list[pd.DataFrame], pd.DataFrame]
+    y: pd.DataFrame, xs: Union[List[pd.DataFrame], pd.DataFrame]
 ) -> pd.DataFrame:
     """ä½¿ç”¨è‹¥å¹²å› å­å¯¹æŸä¸ªå› å­è¿›è¡Œæ­£äº¤åŒ–å¤„ç†
 
     Parameters
     ----------
     y : pd.DataFrame
         ç ”ç©¶çš„ç›®æ ‡ï¼Œå›å½’ä¸­çš„y
-    xs : Union[list[pd.DataFrame],pd.DataFrame]
+    xs : Union[List[pd.DataFrame],pd.DataFrame]
         ç”¨äºæ­£äº¤åŒ–çš„è‹¥å¹²å› å­ï¼Œå›å½’ä¸­çš„x
 
     Returns
     -------
     pd.DataFrame
         æ­£äº¤åŒ–ä¹‹åçš„å› å­
     """
     if not isinstance(xs, list):
         xs = [xs]
     y = pure_fallmount(y)
     xs = [pure_fallmount(i) for i in xs]
     return (y - xs)()
 
 
+@do_on_dfs
 def show_corrs_with_old(
-    df: pd.DataFrame = None, method: str = "spearman"
+    df: pd.DataFrame = None, method: str = "spearman", only_new: bool = 1
 ) -> pd.DataFrame:
     """è®¡ç®—æ–°å› å­å’Œå·²æœ‰å› å­çš„ç›¸å…³ç³»æ•°
 
     Parameters
     ----------
     df : pd.DataFrame, optional
         æ–°å› å­, by default None
     method : str, optional
         æ±‚ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default 'spearman'
+    only_new : bool, optional
+        ä»…è®¡ç®—æ–°å› å­ä¸æ—§å› å­ä¹‹é—´çš„ç›¸å…³ç³»æ•°, by default 1
 
     Returns
     -------
     pd.DataFrame
         ç›¸å…³ç³»æ•°çŸ©é˜µ
     """
     if df is not None:
         df0 = df.resample("M").last()
         if df.shape[0] / df0.shape[0] > 2:
             daily = 1
         else:
             daily = 0
+    nums = os.listdir(homeplace.final_factor_file)
+    nums = sorted(
+        set(
+            [
+                int(i.split("å¤šå› å­")[1].split("_æœˆ")[0])
+                for i in nums
+                if i.endswith("æœˆ.parquet")
+            ]
+        )
+    )
     olds = []
-    for i in range(1, 100):
+    for i in nums:
         try:
             if daily:
                 old = database_read_final_factors(order=i)[0]
             else:
                 old = database_read_final_factors(order=i)[0].resample("M").last()
             olds.append(old)
         except Exception:
             break
     if df is not None:
-        olds = [df] + olds
-        corrs = show_corrs(
-            olds, ["new"] + [f"old{i}" for i in range(1, len(olds))], method=method
-        )
+        if only_new:
+            corrs = [
+                to_percent(show_corr(df, i, plt_plot=0, method=method)) for i in olds
+            ]
+            corrs = pd.Series(corrs, index=[f"old{i}" for i in nums])
+            corrs = corrs.to_frame(f"{method}ç›¸å…³ç³»æ•°").T
+        else:
+            olds = [df] + olds
+            corrs = show_corrs(olds, ["new"] + [f"old{i}" for i in nums], method=method)
     else:
-        corrs = show_corrs(
-            olds, [f"old{i}" for i in range(1, len(olds))], method=method
-        )
+        corrs = show_corrs(olds, [f"old{i}" for i in nums], method=method)
     return corrs
 
 
+@do_on_dfs
+def remove_unavailable(df: pd.DataFrame) -> pd.DataFrame:
+    """å¯¹æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå‰”é™¤stè‚¡ã€ä¸æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨å’Œä¸Šå¸‚ä¸è¶³60å¤©çš„è‚¡ç¥¨
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+        å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
+
+    Returns
+    -------
+    pd.DataFrame
+        å‰”é™¤åçš„å› å­å€¼
+    """
+    df0 = df.resample("M").last()
+    if df.shape[0] / df0.shape[0] > 2:
+        daily = 1
+    else:
+        daily = 0
+    if daily:
+        state = read_daily(state=1).replace(0, np.nan)
+        st = read_daily(st=1)
+        age = read_daily(age=1)
+        st = (1 - st).replace(0, np.nan)
+        age = ((age >= 60) + 0).replace(0, np.nan)
+        df = df * age * st * state
+    else:
+        moon = pure_moon(no_read_indu=1)
+        moon.set_basic_data()
+        moon.judge_month()
+        df = moon.tris_monthly * df
+    return df
+
+
+class frequency_controller(object):
+    def __init__(self, freq: str):
+        self.homeplace = HomePlace()
+        self.freq = freq
+
+        if freq == "M":
+            self.counts_one_year = 12
+            self.time_shift = pd.DateOffset(months=1)
+            self.states_files = (
+                self.homeplace.daily_data_file + "states_monthly.parquet"
+            )
+            self.sts_files = self.homeplace.daily_data_file + "sts_monthly.parquet"
+            self.comment_name = "æœˆ"
+            self.days_in = 20
+        elif freq == "W":
+            self.counts_one_year = 52
+            self.time_shift = pd.DateOffset(weeks=1)
+            self.states_files = self.homeplace.daily_data_file + "states_weekly.parquet"
+            self.sts_files = self.homeplace.daily_data_file + "sts_weekly.parquet"
+            self.comment_name = "å‘¨"
+            self.days_in = 5
+        else:
+            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")
+
+    def next_end(self, x):
+        """æ‰¾åˆ°ä¸‹ä¸ªå‘¨æœŸçš„æœ€åä¸€å¤©"""
+        if self.freq == "M":
+            return x + pd.DateOffset(months=1) + pd.tseries.offsets.MonthEnd()
+        elif self.freq == "W":
+            return x + pd.DateOffset(weeks=1)
+        else:
+            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")
+
+
 class pure_moon(object):
     __slots__ = [
         "homeplace",
         "sts_monthly_file",
         "states_monthly_file",
         "factors",
         "codes",
@@ -1298,35 +1403,56 @@
         "__factors_out",
         "ics",
         "rankics",
         "factor_turnover_rates",
         "factor_turnover_rate",
         "group_rets_std",
         "group_rets_stds",
+        "group_rets_skews",
+        "group_rets_skew",
         "wind_out",
         "swindustry_dummy",
         "zxindustry_dummy",
         "closes2_monthly",
         "rets_monthly_last",
+        "freq_ctrl",
+        "freq",
+        "factor_cover",
+        "factor_cross_skew",
+        "factor_cross_skew_after_neu",
+        "pos_neg_rate",
+        "corr_itself",
+        "factor_cross_stds",
+        "corr_itself_shift2",
+        "rets_all",
+        "inner_long_ret_yearly",
+        "inner_short_ret_yearly",
+        "inner_long_net_values",
+        "inner_short_net_values",
     ]
 
     @classmethod
     @lru_cache(maxsize=None)
     def __init__(
         cls,
+        freq: str = "M",
         no_read_indu: bool = 0,
         swindustry_dummy: pd.DataFrame = None,
         zxindustry_dummy: pd.DataFrame = None,
         read_in_swindustry_dummy: bool = 0,
     ):
         cls.homeplace = HomePlace()
+        cls.freq = freq
+        cls.freq_ctrl = frequency_controller(freq)
         # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
-        cls.sts_monthly_file = homeplace.daily_data_file + "sts_monthly.feather"
+        # week_here
+        cls.sts_monthly_file = cls.freq_ctrl.sts_files
         # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
-        cls.states_monthly_file = homeplace.daily_data_file + "states_monthly.feather"
+        # week_here
+        cls.states_monthly_file = cls.freq_ctrl.states_files
 
         if swindustry_dummy is not None:
             cls.swindustry_dummy = swindustry_dummy
         if zxindustry_dummy is not None:
             cls.zxindustry_dummy = zxindustry_dummy
 
         def deal_dummy(industry_dummy):
@@ -1336,37 +1462,37 @@
             industry_dummy.columns = col
             industry_dummy = industry_dummy[
                 industry_dummy.date >= pd.Timestamp("20100101")
             ]
             return industry_dummy
 
         if (swindustry_dummy is None) and (zxindustry_dummy is None):
-
             if not no_read_indu:
                 if read_in_swindustry_dummy:
+                    # week_here
                     cls.swindustry_dummy = (
-                        pd.read_feather(
-                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
+                        pd.read_parquet(
+                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
                         )
                         .fillna(0)
                         .set_index("date")
                         .groupby("code")
-                        .resample("M")
+                        .resample(freq)
                         .last()
                     )
                     cls.swindustry_dummy = deal_dummy(cls.swindustry_dummy)
-
+                # week_here
                 cls.zxindustry_dummy = (
-                    pd.read_feather(
-                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"
+                    pd.read_parquet(
+                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"
                     )
                     .fillna(0)
                     .set_index("date")
                     .groupby("code")
-                    .resample("M")
+                    .resample(freq)
                     .last()
                     .fillna(0)
                 )
 
                 cls.zxindustry_dummy = deal_dummy(cls.zxindustry_dummy)
 
     @property
@@ -1374,47 +1500,83 @@
         return self.__factors_out
 
     def __call__(self):
         """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
         return self.factors_out
 
     @classmethod
+    @lru_cache(maxsize=None)
     def set_basic_data(
         cls,
-        age: pd.DataFrame,
-        st: pd.DataFrame,
-        state: pd.DataFrame,
-        open: pd.DataFrame,
-        close: pd.DataFrame,
-        capital: pd.DataFrame,
+        ages: pd.DataFrame = None,
+        sts: pd.DataFrame = None,
+        states: pd.DataFrame = None,
+        opens: pd.DataFrame = None,
+        closes: pd.DataFrame = None,
+        capitals: pd.DataFrame = None,
+        opens_average_first_day: bool = 0,
+        total_cap: bool = 0,
     ):
+        if ages is None:
+            ages = read_daily(age=1, start=20100101)
+        if sts is None:
+            sts = read_daily(st=1, start=20100101)
+        if states is None:
+            states = read_daily(state=1, start=20100101)
+        if opens is None:
+            if opens_average_first_day:
+                opens = read_daily(vwap=1, start=20100101)
+            else:
+                opens = read_daily(open=1, start=20100101)
+        if closes is None:
+            closes = read_daily(close=1, start=20100101)
+        if capitals is None:
+            if total_cap:
+                capitals = (
+                    read_daily(total_cap=1, start=20100101).resample(cls.freq).last()
+                )
+            else:
+                capitals = (
+                    read_daily(flow_cap=1, start=20100101).resample(cls.freq).last()
+                )
         # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
-        cls.ages = age
+        cls.ages = ages
         # stæ—¥å­æ ‡å¿—æ–‡ä»¶
-        cls.sts = st.fillna(0)
+        cls.sts = sts.fillna(0)
         # cls.sts = 1 - cls.sts.fillna(0)
         # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
-        cls.states = state
+        cls.states = states
         # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
-        cls.opens = open
+        cls.opens = opens
         # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
-        cls.closes = close
+        cls.closes = closes
         # æœˆåº•æµé€šå¸‚å€¼æ•°æ®
-        cls.capital = capital
-        cls.opens = cls.opens.replace(0, np.nan)
-        cls.closes = cls.closes.replace(0, np.nan)
+        cls.capital = capitals
+        if cls.opens is not None:
+            cls.opens = cls.opens.replace(0, np.nan)
+        if cls.closes is not None:
+            cls.closes = cls.closes.replace(0, np.nan)
 
-    def set_factor_df_date_as_index(self, df):
+    def set_factor_df_date_as_index(self, df: pd.DataFrame):
         """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
-        df = df.reset_index()
-        df.columns = ["date"] + list(df.columns)[1:]
-        self.factors = df
-        self.factors = self.factors.set_index("date")
-        self.factors = self.factors.resample("M").last()
-        self.factors = self.factors.reset_index()
+        # week_here
+        self.factors = df.resample(self.freq).last().dropna(how="all")
+        self.factor_cover = np.sign(self.factors.abs() + 1).sum().sum()
+        opens = self.opens[self.opens.index >= self.factors.index.min()]
+        total = np.sign(opens.resample(self.freq).last()).sum().sum()
+        self.factor_cover = min(self.factor_cover / total, 1)
+        self.factor_cross_skew = self.factors.skew(axis=1).mean()
+        pos_num = ((self.factors > 0) + 0).sum().sum()
+        neg_num = ((self.factors < 0) + 0).sum().sum()
+        self.pos_neg_rate = pos_num / (neg_num + pos_num)
+        self.corr_itself = show_corr(self.factors, self.factors.shift(1), plt_plot=0)
+        self.corr_itself_shift2 = show_corr(
+            self.factors, self.factors.shift(2), plt_plot=0
+        )
+        self.factor_cross_stds = self.factors.std(axis=1)
 
     @classmethod
     def judge_month_st(cls, df):
         """æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ"""
         st_count = len(df[df == 1])
         normal_count = len(df[df != 1])
         if st_count >= normal_count:
@@ -1433,62 +1595,93 @@
             return 1
 
     @classmethod
     def read_add(cls, pridf, df, func):
         """ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ """
         if pridf.index.max() > df.index.max():
             df_add = pridf[pridf.index > df.index.max()]
-            df_add = df_add.resample("M").apply(func)
-            df = pd.concat([df, df_add])
-            return df
+            if df_add.shape[0] > int(cls.freq_ctrl.days_in / 2):
+                df_1 = df_add.index.max()
+                year = df_1.year
+                month = df_1.month
+                last = tt.date.get_close(year=year, m=month).pd_date()
+                if (last == df_1)[0]:
+                    # week_here
+                    df_add = df_add.resample(cls.freq).apply(func)
+                    df = pd.concat([df, df_add])
+                    return df
+                else:
+                    df_add = df_add[
+                        df_add.index < pd.Timestamp(year=year, month=month, day=1)
+                    ]
+                    if df_add.shape[0] > 0:
+                        df_add = df_add.resample(cls.freq).apply(func)
+                        df = pd.concat([df, df_add])
+                        return df
+                    else:
+                        return df
+            else:
+                return df
         else:
             return df
 
     @classmethod
     def daily_to_monthly(cls, pridf, path, func):
         """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
         è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
         try:
-            month_df = pd.read_feather(path)
-            month_df = month_df.set_index(list(month_df.columns)[0])
+            month_df = pd.read_parquet(path)
             month_df = cls.read_add(pridf, month_df, func)
-            month_df.reset_index().to_feather(path)
+            month_df.to_parquet(path)
         except Exception as e:
             if not STATES["NO_LOG"]:
                 logger.error("error occurs when read state files")
                 logger.error(e)
             print("state file rewritingâ€¦â€¦")
-            month_df = pridf.resample("M").apply(func)
-            month_df.reset_index().to_feather(path)
+            # week_here
+            df_1 = pridf.index.max()
+            year = df_1.year
+            month = df_1.month
+            last = tt.date.get_close(year=year, m=month).pd_date()
+            if not (last == df_1)[0]:
+                pridf = pridf[pridf.index < pd.Timestamp(year=year, month=month, day=1)]
+            month_df = pridf.resample(cls.freq).apply(func)
+            month_df.to_parquet(path)
         return month_df
 
     @classmethod
     @lru_cache(maxsize=None)
     def judge_month(cls):
         """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""
-
-        cls.sts_monthly = cls.daily_to_monthly(
-            cls.sts, cls.sts_monthly_file, cls.judge_month_st
-        )
-        cls.states_monthly = cls.daily_to_monthly(
-            cls.states, cls.states_monthly_file, cls.judge_month_state
-        )
-        cls.ages_monthly = cls.ages.resample("M").last()
-        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(
-            -1, 0
-        )
-        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
-        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)
+        if cls.freq == "M":
+            cls.sts_monthly = cls.daily_to_monthly(
+                cls.sts, cls.sts_monthly_file, cls.judge_month_st
+            )
+            cls.states_monthly = cls.daily_to_monthly(
+                cls.states, cls.states_monthly_file, cls.judge_month_state
+            )
+            # week_here
+            cls.ages_monthly = (cls.ages.resample(cls.freq).last() > 60) + 0
+            cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
+            cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)
+        else:
+            cls.tris_monthly = (
+                (1 - cls.sts).resample(cls.freq).last().ffill(limit=2)
+                * cls.states.resample(cls.freq).last().ffill(limit=2)
+                * ((cls.ages.resample(cls.freq).last() > 60) + 0)
+            ).replace(0, np.nan)
 
     @classmethod
     @lru_cache(maxsize=None)
     def get_rets_month(cls):
         """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
-        cls.opens_monthly = cls.opens.resample("M").first()
-        cls.closes_monthly = cls.closes.resample("M").last()
+        # week_here
+        cls.opens_monthly = cls.opens.resample(cls.freq).first()
+        # week_here
+        cls.closes_monthly = cls.closes.resample(cls.freq).last()
         cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
         cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
         cls.rets_monthly = cls.rets_monthly.stack().reset_index()
         cls.rets_monthly.columns = ["date", "code", "ret"]
 
     @classmethod
     def neutralize_factors(cls, df):
@@ -1527,24 +1720,27 @@
         else:
             cls.cap["cap_size"] = np.log(cls.cap["cap_size"])
 
     def get_neutral_factors(
         self, zxindustry_dummies=0, swindustry_dummies=0, only_cap=0
     ):
         """å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
-        self.factors = self.factors.set_index("date")
-        self.factors.index = self.factors.index + pd.DateOffset(months=1)
-        self.factors = self.factors.resample("M").last()
-        last_date = self.tris_monthly.index.max() + pd.DateOffset(months=1)
-        last_date = last_date + pd.tseries.offsets.MonthEnd()
+        # week_here
+        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
+        # week_here
+        self.factors = self.factors.resample(self.freq).last()
+        # week_here
+        last_date = self.freq_ctrl.next_end(self.tris_monthly.index.max())
         add_tail = pd.DataFrame(1, index=[last_date], columns=self.tris_monthly.columns)
         tris_monthly = pd.concat([self.tris_monthly, add_tail])
         self.factors = self.factors * tris_monthly
-        self.factors.index = self.factors.index - pd.DateOffset(months=1)
-        self.factors = self.factors.resample("M").last()
+        # week_here
+        self.factors.index = self.factors.index - self.freq_ctrl.time_shift
+        # week_here
+        self.factors = self.factors.resample(self.freq).last()
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
         self.factors = pd.merge(
             self.factors, self.cap, how="inner", on=["date", "code"]
         )
         if not only_cap:
             if swindustry_dummies:
@@ -1557,31 +1753,33 @@
                 )
         self.factors = self.factors.set_index(["date", "code"])
         self.factors = self.factors.groupby(["date"]).apply(self.neutralize_factors)
         self.factors = self.factors.reset_index()
 
     def deal_with_factors(self):
         """åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®"""
-        self.factors = self.factors.set_index("date")
         self.__factors_out = self.factors.copy()
-        self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
-        self.factors.index = self.factors.index + pd.DateOffset(months=1)
-        self.factors = self.factors.resample("M").last()
+        # week_here
+        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
+        # week_here
+        self.factors = self.factors.resample(self.freq).last()
         self.factors = self.factors * self.tris_monthly
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
 
     def deal_with_factors_after_neutralize(self):
         """ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•"""
         self.factors = self.factors.set_index(["date", "code"])
         self.factors = self.factors.unstack()
         self.__factors_out = self.factors.copy()
         self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
-        self.factors.index = self.factors.index + pd.DateOffset(months=1)
-        self.factors = self.factors.resample("M").last()
+        # week_here
+        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
+        # week_here
+        self.factors = self.factors.resample(self.freq).last()
         self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
         self.factors = self.factors.stack().reset_index()
         self.factors.columns = ["date", "code", "fac"]
 
     @classmethod
     def find_limit(cls, df, up=1):
         """è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
@@ -1600,35 +1798,38 @@
         """æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ"""
         """æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨"""
         cls.opens_monthly_shift = cls.opens_monthly.copy()
         cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
         cls.rets_monthly_begin = (
             cls.opens_monthly_shift - cls.closes_monthly
         ) / cls.closes_monthly
-        cls.closes2_monthly = cls.closes.shift(1).resample("M").last()
+        # week_here
+        cls.closes2_monthly = cls.closes.shift(1).resample(cls.freq).last()
         cls.rets_monthly_last = (
             cls.closes_monthly - cls.closes2_monthly
         ) / cls.closes2_monthly
         cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
         cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)
 
     def get_ic_rankic(cls, df):
         """è®¡ç®—ICå’ŒRankIC"""
         df1 = df[["ret", "fac"]]
         ic = df1.corr(method="pearson").iloc[0, 1]
-        rankic = df1.corr(method="spearman").iloc[0, 1]
+        rankic = df1.rank().corr().iloc[0, 1]
         df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic]})
         return df2
 
     def get_icir_rankicir(cls, df):
         """è®¡ç®—ICIRå’ŒRankICIR"""
         ic = df.ic.mean()
         rankic = df.rankic.mean()
-        icir = ic / np.std(df.ic) * (12 ** (0.5))
-        rankicir = rankic / np.std(df.rankic) * (12 ** (0.5))
+        # week_here
+        icir = ic / np.std(df.ic) * (cls.freq_ctrl.counts_one_year ** (0.5))
+        # week_here
+        rankicir = rankic / np.std(df.rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
         return pd.DataFrame(
             {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
             index=["è¯„ä»·æŒ‡æ ‡"],
         )
 
     def get_ic_icir_and_rank(cls, df):
         """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
@@ -1637,15 +1838,15 @@
         cls.rankics = df1.rankic
         cls.ics = cls.ics.reset_index(drop=True, level=1).to_frame()
         cls.rankics = cls.rankics.reset_index(drop=True, level=1).to_frame()
         df2 = cls.get_icir_rankicir(df1)
         df2 = df2.T
         dura = (df.date.max() - df.date.min()).days / 365
         t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
-        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankICå‡å€¼tå€¼"])
+        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankIC.t"])
         df4 = pd.concat([df2, df3])
         return df4
 
     @classmethod
     def get_groups(cls, df, groups_num):
         """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
         if "group" in list(df.columns):
@@ -1663,32 +1864,25 @@
         if len(l) < df.shape[0]:
             l = l + [groups_num] * (df.shape[0] - len(l))
         l = l[: df.shape[0]]
         df.insert(0, "group", l)
         return df
 
     @classmethod
-    def next_month_end(cls, x):
-        """æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©"""
-        x1 = x = x + relativedelta(months=1)
-        while x1.month == x.month:
-            x1 = x1 + relativedelta(days=1)
-        return x1 - relativedelta(days=1)
-
-    @classmethod
     def limit_old_to_new(cls, limit, data):
         """è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
         æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»"""
         data1 = data.copy()
         data1 = data1.reset_index()
         data1.columns = ["data_index"] + list(data1.columns)[1:]
         old = pd.merge(limit, data1, how="inner", on=["date", "code"])
         old = old.set_index("data_index")
         old = old[["group", "date", "code"]]
-        old.date = list(map(cls.next_month_end, list(old.date)))
+        # week_here
+        old.date = list(map(cls.freq_ctrl.next_end, list(old.date)))
         return old
 
     def get_data(self, groups_num):
         """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
         self.data = pd.merge(
             self.rets_monthly, self.factors, how="inner", on=["date", "code"]
         )
@@ -1696,91 +1890,124 @@
         self.data = self.data.groupby("date").apply(
             lambda x: self.get_groups(x, groups_num)
         )
         self.wind_out = self.data.copy()
         self.factor_turnover_rates = self.data.pivot(
             index="date", columns="code", values="group"
         )
-        self.factor_turnover_rates = self.factor_turnover_rates.diff()
-        change = ((np.abs(np.sign(self.factor_turnover_rates)) == 1) + 0).sum(axis=1)
-        still = ((self.factor_turnover_rates == 0) + 0).sum(axis=1)
-        self.factor_turnover_rates = change / (change + still)
-        self.factor_turnover_rate = self.factor_turnover_rates.mean()
+        rates = []
+        for i in range(1, groups_num + 1):
+            son = (self.factor_turnover_rates == i) + 0
+            son1 = son.diff()
+            # self.factor_turnover_rates = self.factor_turnover_rates.diff()
+            change = ((np.abs(np.sign(son1)) == 1) + 0).sum(axis=1)
+            still = (((son1 == 0) + 0) * son).sum(axis=1)
+            rate = change / (change + still)
+            rates.append(rate.to_frame(f"group{i}"))
+        rates = pd.concat(rates, axis=1).fillna(0)
+        self.factor_turnover_rates = rates
         self.data = self.data.reset_index(drop=True)
         limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
         limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
         self.data = self.data.drop(limit_ups_object.index)
         rets_monthly_limit_downs = pd.merge(
             self.rets_monthly, limit_downs_object, how="inner", on=["date", "code"]
         )
         self.data = pd.concat([self.data, rets_monthly_limit_downs])
 
-    def select_data_time(self, time_start, time_end):
-        """ç­›é€‰ç‰¹å®šçš„æ—¶é—´æ®µ"""
-        if time_start:
-            self.data = self.data[self.data.date >= time_start]
-        if time_end:
-            self.data = self.data[self.data.date <= time_end]
-
     def make_start_to_one(self, l):
         """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
         min_date = self.factors.date.min()
         add_date = min_date - relativedelta(days=min_date.day)
         add_l = pd.Series([1], index=[add_date])
         l = pd.concat([add_l, l])
         return l
 
     def to_group_ret(self, l):
         """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
-        ret = l[-1] ** (12 / len(l)) - 1
+        # week_here
+        ret = l[-1] ** (self.freq_ctrl.counts_one_year / len(l)) - 1
         return ret
 
-    def get_group_rets_net_values(self, groups_num=10, value_weighted=False):
+    def get_group_rets_net_values(
+        self, groups_num=10, value_weighted=False, trade_cost_double_side=0
+    ):
         """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
         if value_weighted:
             cap_value = self.capital.copy()
-            cap_value = cap_value.resample("M").last().shift(1)
+            # week_here
+            cap_value = cap_value.resample(self.freq).last().shift(1)
             cap_value = cap_value * self.tris_monthly
             # cap_value=np.log(cap_value)
             cap_value = cap_value.stack().reset_index()
             cap_value.columns = ["date", "code", "cap_value"]
             self.data = pd.merge(self.data, cap_value, on=["date", "code"])
 
             def in_g(df):
                 df.cap_value = df.cap_value / df.cap_value.sum()
                 df.ret = df.ret * df.cap_value
                 return df.ret.sum()
 
             self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
+            self.rets_all = self.data.groupby(["date"]).apply(in_g)
             self.group_rets_std = "å¸‚å€¼åŠ æƒæš‚æœªè®¾ç½®è¯¥åŠŸèƒ½ï¼Œæ•¬è¯·æœŸå¾…ğŸŒ™"
         else:
             self.group_rets = self.data.groupby(["date", "group"]).apply(
                 lambda x: x.ret.mean()
             )
-            self.group_rets_stds = self.data.groupby(["date", "group"]).apply(
-                lambda x: x.ret.std()
+            self.rets_all = self.data.groupby(["date"]).apply(lambda x: x.ret.mean())
+            self.group_rets_stds = self.data.groupby(["date", "group"]).ret.std()
+            self.group_rets_std = (
+                self.group_rets_stds.reset_index().groupby("group").mean()
+            )
+            self.group_rets_skews = self.data.groupby(["date", "group"]).ret.skew()
+            self.group_rets_skew = (
+                self.group_rets_skews.reset_index().groupby("group").mean()
             )
-            self.group_rets_std = self.group_rets_stds.groupby("group").mean()
         # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
         self.group_rets = self.group_rets.unstack()
         self.group_rets = self.group_rets[
             self.group_rets.index <= self.factors.date.max()
         ]
         self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
         self.group_rets = self.group_rets.add_prefix("group")
+        self.group_rets = (
+            self.group_rets - self.factor_turnover_rates * trade_cost_double_side
+        )
+        self.rets_all = (
+            self.rets_all
+            - self.factor_turnover_rates.mean(axis=1) * trade_cost_double_side
+        ).dropna()
         self.long_short_rets = (
             self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
         )
-        self.long_short_net_values = (self.long_short_rets + 1).cumprod()
+        self.inner_rets_long = self.group_rets.group1 - self.rets_all
+        self.inner_rets_short = (
+            self.rets_all - self.group_rets["group" + str(groups_num)]
+        )
+        self.long_short_net_values = self.make_start_to_one(
+            (self.long_short_rets + 1).cumprod()
+        )
         if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
             self.long_short_rets = (
                 self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
             )
-            self.long_short_net_values = (self.long_short_rets + 1).cumprod()
-        self.long_short_net_values = self.make_start_to_one(self.long_short_net_values)
+            self.long_short_net_values = self.make_start_to_one(
+                (self.long_short_rets + 1).cumprod()
+            )
+            self.inner_rets_long = (
+                self.group_rets["group" + str(groups_num)] - self.rets_all
+            )
+            self.inner_rets_short = self.rets_all - self.group_rets.group1
+        self.inner_long_net_values = self.make_start_to_one(
+            (self.inner_rets_long + 1).cumprod()
+        )
+        self.inner_short_net_values = self.make_start_to_one(
+            (self.inner_rets_short + 1).cumprod()
+        )
         self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
         self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
         self.group_net_values = self.group_net_values.cumprod()
         self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
         a = groups_num ** (0.5)
         # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
         if a == int(a):
@@ -1794,18 +2021,34 @@
                 index=list(range(1, int(a) + 1)),
             )
             print("è¿™æ˜¯self.square_rets", self.square_rets)
 
     def get_long_short_comments(self, on_paper=False):
         """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
         åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
+        # week_here
         self.long_short_ret_yearly = (
-            self.long_short_net_values[-1] ** (12 / len(self.long_short_net_values)) - 1
+            self.long_short_net_values[-1]
+            ** (self.freq_ctrl.counts_one_year / len(self.long_short_net_values))
+            - 1
+        )
+        self.inner_long_ret_yearly = (
+            self.inner_long_net_values[-1]
+            ** (self.freq_ctrl.counts_one_year / len(self.inner_long_net_values))
+            - 1
+        )
+        self.inner_short_ret_yearly = (
+            self.inner_short_net_values[-1]
+            ** (self.freq_ctrl.counts_one_year / len(self.inner_short_net_values))
+            - 1
+        )
+        # week_here
+        self.long_short_vol_yearly = np.std(self.long_short_rets) * (
+            self.freq_ctrl.counts_one_year**0.5
         )
-        self.long_short_vol_yearly = np.std(self.long_short_rets) * (12**0.5)
         self.long_short_info_ratio = (
             self.long_short_ret_yearly / self.long_short_vol_yearly
         )
         self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
         self.long_short_win_ratio = self.long_short_win_times / len(
             self.long_short_rets
         )
@@ -1820,102 +2063,163 @@
                         self.long_short_ret_yearly,
                         self.long_short_vol_yearly,
                         self.long_short_info_ratio,
                         self.long_short_win_ratio,
                         self.max_retreat,
                     ]
                 },
-                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "æ”¶ç›Šæ³¢åŠ¨æ¯”", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
+                # week_here
+                index=[
+                    "å¹´åŒ–æ”¶ç›Šç‡",
+                    "å¹´åŒ–æ³¢åŠ¨ç‡",
+                    "æ”¶ç›Šæ³¢åŠ¨æ¯”",
+                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
+                    "æœ€å¤§å›æ’¤ç‡",
+                ],
             )
         else:
             self.long_short_comments = pd.DataFrame(
                 {
                     "è¯„ä»·æŒ‡æ ‡": [
                         self.long_short_ret_yearly,
                         self.long_short_vol_yearly,
                         self.long_short_info_ratio,
                         self.long_short_win_ratio,
                         self.max_retreat,
                     ]
                 },
-                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
+                # week_here
+                index=[
+                    "å¹´åŒ–æ”¶ç›Šç‡",
+                    "å¹´åŒ–æ³¢åŠ¨ç‡",
+                    "ä¿¡æ¯æ¯”ç‡",
+                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
+                    "æœ€å¤§å›æ’¤ç‡",
+                ],
             )
 
-    def get_total_comments(self):
-        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
+    def get_total_comments(self, groups_num):
+        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
+        rankic = self.rankics.mean()
+        rankic_win = self.rankics[self.rankics * rankic > 0]
+        rankic_win_ratio = rankic_win.dropna().shape[0] / self.rankics.dropna().shape[0]
+        self.factor_cross_skew_after_neu = self.__factors_out.skew(axis=1).mean()
+        if self.ic_icir_and_rank.iloc[2, 0] > 0:
+            self.factor_turnover_rate = self.factor_turnover_rates[
+                f"group{groups_num}"
+            ].mean()
+        else:
+            self.factor_turnover_rate = self.factor_turnover_rates["group1"].mean()
         self.total_comments = pd.concat(
             [
                 self.ic_icir_and_rank,
+                pd.DataFrame(
+                    {"è¯„ä»·æŒ‡æ ‡": [rankic_win_ratio]},
+                    index=["RankICèƒœç‡"],
+                ),
                 self.long_short_comments,
-                pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [self.factor_turnover_rate]}, index=["æœˆå‡æ¢æ‰‹ç‡"]),
+                # week_here
+                pd.DataFrame(
+                    {
+                        "è¯„ä»·æŒ‡æ ‡": [
+                            self.factor_turnover_rate,
+                            self.factor_cover,
+                            self.pos_neg_rate,
+                            self.factor_cross_skew,
+                            self.inner_long_ret_yearly,
+                            self.inner_long_ret_yearly
+                            / (
+                                self.inner_long_ret_yearly + self.inner_short_ret_yearly
+                            ),
+                            self.corr_itself,
+                        ]
+                    },
+                    index=[
+                        f"å¤šå¤´{self.freq_ctrl.comment_name}å‡æ¢æ‰‹",
+                        "å› å­è¦†ç›–ç‡",
+                        "å› å­æ­£å€¼å æ¯”",
+                        "å› å­æˆªé¢ååº¦",
+                        "å¤šå¤´è¶…å‡æ”¶ç›Š",
+                        "å¤šå¤´æ”¶ç›Šå æ¯”",
+                        "ä¸€é˜¶è‡ªç›¸å…³æ€§",
+                    ],
+                ),
             ]
         )
 
-    def plot_net_values(self, y2, filename, iplot=1, ilegend=1):
+    def plot_net_values(self, y2, filename, iplot=1, ilegend=1, without_breakpoint=0):
         """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
         if not iplot:
             fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(33, 8))
             self.group_net_values.plot(secondary_y=y2, rot=60, ax=ax[0])
             self.group_net_values.plot(secondary_y=y2, ax=ax[0])
             b = self.rankics.copy()
             b.index = [int(i.year) if i.month == 1 else "" for i in list(b.index)]
             b.plot(kind="bar", rot=60, ax=ax[1])
-            self.factor_turnover_rates.plot(rot=60, ax=ax[2])
+            self.factor_cross_stds.plot(rot=60, ax=ax[2])
 
             filename_path = filename + ".png"
             if not STATES["NO_SAVE"]:
                 plt.savefig(filename_path)
         else:
-            tris = pd.concat(
-                [self.group_net_values, self.factor_turnover_rates, self.rankics],
-                axis=1,
-            ).rename(columns={0: "turnover_rate"})
+            tris = (
+                pd.concat(
+                    [self.group_net_values, self.factor_cross_stds, self.rankics],
+                    axis=1,
+                )
+                .rename(columns={0: "å› å­æˆªé¢æ ‡å‡†å·®"})
+                .dropna()
+            )
+            if without_breakpoint:
+                tris = tris.dropna()
             figs = cf.figures(
                 tris,
                 [
                     dict(kind="line", y=list(self.group_net_values.columns)),
-                    dict(kind="line", y="turnover_rate"),
+                    dict(kind="bar", y="å› å­æˆªé¢æ ‡å‡†å·®"),
                     dict(kind="bar", y="rankic"),
                 ],
                 asList=True,
             )
             comments = (
                 self.total_comments.applymap(lambda x: round(x, 4))
                 .rename(index={"RankICå‡å€¼tå€¼": "RankIC.t"})
                 .reset_index()
             )
             here = pd.concat(
                 [
-                    comments.iloc[:5, :].reset_index(drop=True),
-                    comments.iloc[5:, :].reset_index(drop=True),
+                    comments.iloc[:6, :].reset_index(drop=True),
+                    comments.iloc[6:12, :].reset_index(drop=True),
+                    comments.iloc[12:, :].reset_index(drop=True),
                 ],
                 axis=1,
             )
-            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ"]
+            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ", "å…¶ä»–æŒ‡æ ‡", "ç»“æœ"]
             # here=here.to_numpy().tolist()+[['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']]
             table = FF.create_table(here.iloc[::-1])
             table.update_yaxes(matches=None)
             # table=go.Figure([go.Table(header=dict(values=list(here.columns)),cells=dict(values=here.to_numpy().tolist()))])
             figs.append(table)
             figs = [figs[-1]] + figs[:-1]
             figs[1].update_layout(
                 legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
             )
             base_layout = cf.tools.get_base_layout(figs)
 
             sp = cf.subplots(
                 figs,
-                shape=(2, 10),
+                shape=(2, 11),
                 base_layout=base_layout,
                 vertical_spacing=0.15,
                 horizontal_spacing=0.03,
                 shared_yaxes=False,
                 specs=[
                     [
-                        {"rowspan": 2, "colspan": 3},
+                        {"rowspan": 2, "colspan": 4},
+                        None,
                         None,
                         None,
                         {"rowspan": 2, "colspan": 4},
                         None,
                         None,
                         None,
                         {"colspan": 3},
@@ -1926,20 +2230,21 @@
                         None,
                         None,
                         None,
                         None,
                         None,
                         None,
                         None,
+                        None,
                         {"colspan": 3},
                         None,
                         None,
                     ],
                 ],
-                subplot_titles=["å‡€å€¼æ›²çº¿", "æœˆæ¢æ‰‹ç‡", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
+                subplot_titles=["å‡€å€¼æ›²çº¿", "å› å­æˆªé¢æ ‡å‡†å·®", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
             )
             sp["layout"].update(showlegend=ilegend)
             # los=sp['layout']['annotations']
             # los[0]['font']['color']='#000000'
             # los[1]['font']['color']='#000000'
             # los[2]['font']['color']='#000000'
             # los[3]['font']['color']='#000000'
@@ -1979,41 +2284,40 @@
     @classmethod
     @lru_cache(maxsize=None)
     def prerpare(cls):
         """é€šç”¨æ•°æ®å‡†å¤‡"""
         cls.judge_month()
         cls.get_rets_month()
 
-    @kk.desktop_sender(title="å˜¿ï¼Œå›æµ‹ç»“æŸå•¦ï½ğŸ—“")
     def run(
         self,
         groups_num=10,
         neutralize=False,
         boxcox=False,
+        trade_cost_double_side=0,
         value_weighted=False,
         y2=False,
         plt_plot=True,
         plotly_plot=False,
         filename="åˆ†ç»„å‡€å€¼å›¾",
-        time_start=None,
-        time_end=None,
         print_comments=True,
         comments_writer=None,
         net_values_writer=None,
         rets_writer=None,
         comments_sheetname=None,
         net_values_sheetname=None,
         rets_sheetname=None,
         on_paper=False,
         sheetname=None,
         zxindustry_dummies=0,
         swindustry_dummies=0,
         only_cap=0,
         iplot=1,
-        ilegend=1,
+        ilegend=0,
+        without_breakpoint=0,
     ):
         """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
         if comments_writer and not (comments_sheetname or sheetname):
             raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
         if net_values_writer and not (net_values_sheetname or sheetname):
             raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
         if rets_writer and not (rets_sheetname or sheetname):
@@ -2033,20 +2337,21 @@
                 only_cap=only_cap,
             )
             self.deal_with_factors_after_neutralize()
         else:
             self.deal_with_factors()
         self.get_limit_ups_downs()
         self.get_data(groups_num)
-        self.select_data_time(time_start, time_end)
         self.get_group_rets_net_values(
-            groups_num=groups_num, value_weighted=value_weighted
+            groups_num=groups_num,
+            value_weighted=value_weighted,
+            trade_cost_double_side=trade_cost_double_side,
         )
         self.get_long_short_comments(on_paper=on_paper)
-        self.get_total_comments()
+        self.get_total_comments(groups_num=groups_num)
         if on_paper:
             group1_ttest = ss.ttest_1samp(self.group_rets.group1, 0).pvalue
             group10_ttest = ss.ttest_1samp(
                 self.group_rets[f"group{groups_num}"], 0
             ).pvalue
             group_long_short_ttest = ss.ttest_1samp(self.long_short_rets, 0).pvalue
             group1_ret = self.group_rets.group1.mean()
@@ -2074,24 +2379,29 @@
             )
             self.total_comments = pd.concat([papers, self.total_comments])
 
         if plt_plot:
             if not STATES["NO_PLOT"]:
                 if filename:
                     self.plot_net_values(
-                        y2=y2, filename=filename, iplot=iplot, ilegend=bool(ilegend)
+                        y2=y2,
+                        filename=filename,
+                        iplot=iplot,
+                        ilegend=bool(ilegend),
+                        without_breakpoint=without_breakpoint,
                     )
                 else:
                     self.plot_net_values(
                         y2=y2,
                         filename=self.factors_file.split(".")[-2].split("/")[-1]
                         + str(groups_num)
                         + "åˆ†ç»„",
                         iplot=iplot,
                         ilegend=bool(ilegend),
+                        without_breakpoint=without_breakpoint,
                     )
                 plt.show()
         if plotly_plot:
             if not STATES["NO_PLOT"]:
                 if filename:
                     self.plotly_net_values(filename=filename)
                 else:
@@ -2099,16 +2409,18 @@
                         filename=self.factors_file.split(".")[-2].split("/")[-1]
                         + str(groups_num)
                         + "åˆ†ç»„"
                     )
         if print_comments:
             if not STATES["NO_COMMENT"]:
                 tb = Texttable()
-                tb.set_cols_width([8] * 4 + [12] + [8] * 2 + [7] * 2 + [8] + [10])
-                tb.set_cols_dtype(["f"] * 11)
+                tb.set_cols_width(
+                    [8] * 5 + [9] + [8] * 2 + [7] * 2 + [8] + [8] + [9] + [10] * 5
+                )
+                tb.set_cols_dtype(["f"] * 18)
                 tb.header(list(self.total_comments.T.columns))
                 tb.add_rows(self.total_comments.T.to_numpy(), header=False)
                 print(tb.draw())
         if sheetname:
             if comments_writer:
                 if not on_paper:
                     total_comments = self.total_comments.copy()
@@ -2116,18 +2428,25 @@
                     tc[0] = str(round(tc[0] * 100, 2)) + "%"
                     tc[1] = str(round(tc[1], 2))
                     tc[2] = str(round(tc[2] * 100, 2)) + "%"
                     tc[3] = str(round(tc[3], 2))
                     tc[4] = str(round(tc[4], 2))
                     tc[5] = str(round(tc[5] * 100, 2)) + "%"
                     tc[6] = str(round(tc[6] * 100, 2)) + "%"
-                    tc[7] = str(round(tc[7], 2))
-                    tc[8] = str(round(tc[8] * 100, 2)) + "%"
+                    tc[7] = str(round(tc[7] * 100, 2)) + "%"
+                    tc[8] = str(round(tc[8], 2))
                     tc[9] = str(round(tc[9] * 100, 2)) + "%"
                     tc[10] = str(round(tc[10] * 100, 2)) + "%"
+                    tc[11] = str(round(tc[11] * 100, 2)) + "%"
+                    tc[12] = str(round(tc[12] * 100, 2)) + "%"
+                    tc[13] = str(round(tc[13] * 100, 2)) + "%"
+                    tc[14] = str(round(tc[14], 2))
+                    tc[15] = str(round(tc[15], 2))
+                    tc[16] = str(round(tc[16] * 100, 2)) + "%"
+                    tc[17] = str(round(tc[17] * 100, 2)) + "%"
                     new_total_comments = pd.DataFrame(
                         {sheetname: tc}, index=total_comments.index
                     )
                     new_total_comments.T.to_excel(comments_writer, sheet_name=sheetname)
                 else:
                     self.total_comments.rename(columns={"è¯„ä»·æŒ‡æ ‡": sheetname}).to_excel(
                         comments_writer, sheet_name=sheetname
@@ -2153,18 +2472,25 @@
                 tc[0] = str(round(tc[0] * 100, 2)) + "%"
                 tc[1] = str(round(tc[1], 2))
                 tc[2] = str(round(tc[2] * 100, 2)) + "%"
                 tc[3] = str(round(tc[3], 2))
                 tc[4] = str(round(tc[4], 2))
                 tc[5] = str(round(tc[5] * 100, 2)) + "%"
                 tc[6] = str(round(tc[6] * 100, 2)) + "%"
-                tc[7] = str(round(tc[7], 2))
-                tc[8] = str(round(tc[8] * 100, 2)) + "%"
+                tc[7] = str(round(tc[7] * 100, 2)) + "%"
+                tc[8] = str(round(tc[8], 2))
                 tc[9] = str(round(tc[9] * 100, 2)) + "%"
                 tc[10] = str(round(tc[10] * 100, 2)) + "%"
+                tc[11] = str(round(tc[11] * 100, 2)) + "%"
+                tc[12] = str(round(tc[12] * 100, 2)) + "%"
+                tc[13] = str(round(tc[13] * 100, 2)) + "%"
+                tc[14] = str(round(tc[14], 2))
+                tc[15] = str(round(tc[15], 2))
+                tc[16] = str(round(tc[16] * 100, 2)) + "%"
+                tc[17] = str(round(tc[17] * 100, 2)) + "%"
                 new_total_comments = pd.DataFrame(
                     {comments_sheetname: tc}, index=total_comments.index
                 )
                 new_total_comments.T.to_excel(
                     comments_writer, sheet_name=comments_sheetname
                 )
             if net_values_writer and net_values_sheetname:
@@ -2181,25 +2507,28 @@
                 group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                 group_rets.columns = [
                     f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                 ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                 group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)
 
 
+@do_on_dfs
 class pure_moonnight(object):
     """å°è£…é€‰è‚¡æ¡†æ¶"""
 
     __slots__ = ["shen"]
 
     def __init__(
         self,
         factors: pd.DataFrame,
         groups_num: int = 10,
+        freq: str = "M",
         neutralize: bool = 0,
         boxcox: bool = 1,
+        trade_cost_double_side: float = 0,
         value_weighted: bool = 0,
         y2: bool = 0,
         plt_plot: bool = 1,
         plotly_plot: bool = 0,
         filename: str = "åˆ†ç»„å‡€å€¼å›¾",
         time_start: int = None,
         time_end: int = None,
@@ -2221,46 +2550,53 @@
         closes: pd.DataFrame = None,
         capitals: pd.DataFrame = None,
         swindustry_dummy: pd.DataFrame = None,
         zxindustry_dummy: pd.DataFrame = None,
         no_read_indu: bool = 0,
         only_cap: bool = 0,
         iplot: bool = 1,
-        ilegend: bool = 1,
+        ilegend: bool = 0,
+        without_breakpoint: bool = 0,
+        opens_average_first_day: bool = 0,
+        total_cap: bool = 0,
     ) -> None:
         """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
         æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
         å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
         æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
         ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„
 
         Parameters
         ----------
         factors : pd.DataFrame
             è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
         groups_num : int, optional
             åˆ†ç»„æ•°é‡, by default 10
+        freq : str, optional
+            å›æµ‹é¢‘ç‡, by default 'M'
         neutralize : bool, optional
             å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
         boxcox : bool, optional
             å¯¹æµé€šå¸‚å€¼åšæˆªé¢boxcoxå˜æ¢ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 1
+        trade_cost_double_side : float, optional
+            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
         value_weighted : bool, optional
             æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
         y2 : bool, optional
             ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
         plt_plot : bool, optional
             å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
         plotly_plot : bool, optional
             å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
         filename : str, optional
             åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
         time_start : int, optional
-            å›æµ‹èµ·å§‹æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
+            å›æµ‹èµ·å§‹æ—¶é—´, by default None
         time_end : int, optional
-            å›æµ‹ç»ˆæ­¢æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
+            å›æµ‹ç»ˆæ­¢æ—¶é—´, by default None
         print_comments : bool, optional
             æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
         comments_writer : pd.ExcelWriter, optional
             ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
         net_values_writer : pd.ExcelWriter, optional
             ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
         rets_writer : pd.ExcelWriter, optional
@@ -2301,98 +2637,163 @@
             ä¸è¯»å…¥è¡Œä¸šæ•°æ®, by default 0
         only_cap : bool, optional
             ä»…åšå¸‚å€¼ä¸­æ€§åŒ–, by default 0
         iplot : bool, optional
             ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»“æœ, by default 1
         ilegend : bool, optional
             ä½¿ç”¨cufflinksç»˜å›¾æ—¶ï¼Œæ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹, by default 1
+        without_breakpoint : bool, optional
+            ç”»å›¾çš„æ—¶å€™æ˜¯å¦å»é™¤é—´æ–­ç‚¹, by default 0
+        opens_average_first_day : bool, optional
+            ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
+        total_cap : bool, optional
+            åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
         """
 
         if not isinstance(factors, pd.DataFrame):
             factors = factors()
-        start = datetime.datetime.strftime(factors.index.min(), "%Y%m%d")
-        if ages is None:
-            ages = read_daily(age=1, start=start)
-        if sts is None:
-            sts = read_daily(st=1, start=start)
-        if states is None:
-            states = read_daily(state=1, start=start)
-        if opens is None:
-            opens = read_daily(open=1, start=start)
-        if closes is None:
-            closes = read_daily(close=1, start=start)
-        if capitals is None:
-            capitals = read_daily(flow_cap=1, start=start).resample("M").last()
         if comments_writer is None and sheetname is not None:
-            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
+            from pure_ocean_breeze.state.states import COMMENTS_WRITER
 
             comments_writer = COMMENTS_WRITER
         if net_values_writer is None and sheetname is not None:
-            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
+            from pure_ocean_breeze.state.states import NET_VALUES_WRITER
 
             net_values_writer = NET_VALUES_WRITER
         if not on_paper:
-            from pure_ocean_breeze.legacy_version.v3p4.state.states import ON_PAPER
+            from pure_ocean_breeze.state.states import ON_PAPER
 
             on_paper = ON_PAPER
-        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_START
+        if time_start is None:
+            from pure_ocean_breeze.state.states import MOON_START
 
-        if MOON_START is not None:
-            factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
-        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_END
+            if MOON_START is not None:
+                factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
+        else:
+            factors = factors[factors.index >= pd.Timestamp(str(time_start))]
+        if time_end is None:
+            from pure_ocean_breeze.state.states import MOON_END
 
-        if MOON_END is not None:
-            factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
+            if MOON_END is not None:
+                factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
+        else:
+            factors = factors[factors.index <= pd.Timestamp(str(time_end))]
         if boxcox + neutralize == 0:
             no_read_indu = 1
         if only_cap + no_read_indu > 0:
             only_cap = no_read_indu = 1
         if iplot:
             print_comments = 0
-        self.shen = pure_moon(
-            no_read_indu=no_read_indu,
-            swindustry_dummy=swindustry_dummy,
-            zxindustry_dummy=zxindustry_dummy,
-            read_in_swindustry_dummy=swindustry_dummies,
-        )
+        if total_cap:
+            if opens_average_first_day:
+                if freq == "M":
+                    self.shen = pure_moon_b(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+                elif freq == "W":
+                    self.shen = pure_week_b(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+            else:
+                if freq == "M":
+                    self.shen = pure_moon_c(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+                elif freq == "W":
+                    self.shen = pure_week_c(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+        else:
+            if opens_average_first_day:
+                if freq == "M":
+                    self.shen = pure_moon(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+                elif freq == "W":
+                    self.shen = pure_week(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+            else:
+                if freq == "M":
+                    self.shen = pure_moon_a(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
+                elif freq == "W":
+                    self.shen = pure_week_a(
+                        freq=freq,
+                        no_read_indu=no_read_indu,
+                        swindustry_dummy=swindustry_dummy,
+                        zxindustry_dummy=zxindustry_dummy,
+                        read_in_swindustry_dummy=swindustry_dummies,
+                    )
         self.shen.set_basic_data(
-            age=ages,
-            st=sts,
-            state=states,
-            open=opens,
-            close=closes,
-            capital=capitals,
+            ages=ages,
+            sts=sts,
+            states=states,
+            opens=opens,
+            closes=closes,
+            capitals=capitals,
+            opens_average_first_day=opens_average_first_day,
+            total_cap=total_cap,
         )
         self.shen.set_factor_df_date_as_index(factors)
         self.shen.prerpare()
         self.shen.run(
             groups_num=groups_num,
             neutralize=neutralize,
             boxcox=boxcox,
+            trade_cost_double_side=trade_cost_double_side,
             value_weighted=value_weighted,
             y2=y2,
             plt_plot=plt_plot,
             plotly_plot=plotly_plot,
             filename=filename,
-            time_start=time_start,
-            time_end=time_end,
             print_comments=print_comments,
             comments_writer=comments_writer,
             net_values_writer=net_values_writer,
             rets_writer=rets_writer,
             comments_sheetname=comments_sheetname,
             net_values_sheetname=net_values_sheetname,
             rets_sheetname=rets_sheetname,
             on_paper=on_paper,
             sheetname=sheetname,
             swindustry_dummies=swindustry_dummies,
             zxindustry_dummies=zxindustry_dummies,
             only_cap=only_cap,
             iplot=iplot,
             ilegend=ilegend,
+            without_breakpoint=without_breakpoint,
         )
 
     def __call__(self) -> pd.DataFrame:
         """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®
 
         Returns
         -------
@@ -2417,24 +2818,64 @@
             net = self.shen.group_net_values[i]
             com = comments_on_twins(net, ret)
             com = com.to_frame(i)
             coms.append(com)
         df = pd.concat(coms, axis=1)
         return df.T
 
+    def comment_yearly(self) -> pd.DataFrame:
+        """å¯¹å›æµ‹çš„æ¯å¹´è¡¨ç°ç»™å‡ºè¯„ä»·
+
+        Returns
+        -------
+        pd.DataFrame
+            å„å¹´åº¦çš„æ”¶ç›Šç‡
+        """
+        df = self.shen.group_net_values.resample("Y").last().pct_change()
+        df.index = df.index.year
+        return df
+
+
+class pure_week(pure_moon):
+    ...
+
+
+class pure_moon_a(pure_moon):
+    ...
+
+
+class pure_week_a(pure_moon):
+    ...
+
+
+class pure_moon_b(pure_moon):
+    ...
+
+
+class pure_week_b(pure_moon):
+    ...
+
+
+class pure_moon_c(pure_moon):
+    ...
+
+
+class pure_week_c(pure_moon):
+    ...
+
 
 class pure_fall(object):
     # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
     def __init__(self, daily_path: str) -> None:
         """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶
 
         Parameters
         ----------
         daily_path : str
-            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.feather'ç»“å°¾
+            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.parquet'ç»“å°¾
         """
         self.homeplace = HomePlace()
         # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
         self.daily_factors = None
         self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path
 
     def __call__(self, monthly=False):
@@ -2443,160 +2884,14 @@
             return self.monthly_factors.copy()
         else:
             try:
                 return self.daily_factors.copy()
             except Exception:
                 return self.monthly_factors.copy()
 
-    def __add__(self, selfas):
-        """å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ """
-        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
-        fac2s = []
-        if not isinstance(selfas, Iterable):
-            if not STATES["NO_LOG"]:
-                logger.warning(f"{selfas} is changed into Iterable")
-            selfas = (selfas,)
-        for selfa in selfas:
-            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
-            fac2s.append(fac2)
-        for i in fac2s:
-            fac1 = fac1 + i
-        new_pure = pure_fall()
-        new_pure.monthly_factors = fac1
-        return new_pure
-
-    def __mul__(self, selfas):
-        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
-        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
-        fac1 = fac1 - fac1.min()
-        fac2s = []
-        if not isinstance(selfas, Iterable):
-            if not STATES["NO_LOG"]:
-                logger.warning(f"{selfas} is changed into Iterable")
-            selfas = (selfas,)
-        for selfa in selfas:
-            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
-            fac2 = fac2 - fac2.min()
-            fac2s.append(fac2)
-        for i in fac2s:
-            fac1 = fac1 * i
-        new_pure = pure_fall()
-        new_pure.monthly_factors = fac1
-        return new_pure
-
-    def __truediv__(self, selfa):
-        """ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡"""
-        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
-        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
-        fac = fac1 - fac2
-        new_pure = pure_fall()
-        new_pure.monthly_factors = fac
-        return new_pure
-
-    def __floordiv__(self, selfa):
-        """ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤"""
-        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
-        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
-        fac1 = fac1 - fac1.min()
-        fac2 = fac2 - fac2.min()
-        fac = fac1 / fac2
-        fac = fac.replace(np.inf, np.nan)
-        new_pure = pure_fall()
-        new_pure.monthly_factors = fac
-        return new_pure
-
-    @kk.desktop_sender(title="å˜¿ï¼Œæ­£äº¤åŒ–ç»“æŸå•¦ï½ğŸ¬")
-    def __sub__(self, selfa):
-        """ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
-        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯"""
-        if is_notebook():
-            tqdm.tqdm_notebook().pandas()
-        else:
-            tqdm.tqdm.pandas()
-        if not isinstance(selfa, Iterable):
-            if not STATES["NO_LOG"]:
-                logger.warning(f"{selfa} is changed into Iterable")
-            selfa = (selfa,)
-        fac_main = self.wide_to_long(self.monthly_factors, "fac")
-        fac_helps = [i.monthly_factors for i in selfa]
-        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
-        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
-        fac_helps = pd.concat(fac_helps, axis=1)
-        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
-        facs = facs.groupby("date").progress_apply(
-            lambda x: self.de_in_group(x, help_names)
-        )
-        facs = facs.unstack()
-        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
-        return facs
-
-    def __gt__(self, selfa):
-        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
-        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
-        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
-        x = self.monthly_factors.copy()
-        y = selfa.monthly_factors.copy()
-        x = x.stack().reset_index()
-        y = y.stack().reset_index()
-        x.columns = ["date", "code", "fac"]
-        y.columns = ["date", "code", "fac"]
-        shen = pure_moon()
-        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
-        x = (
-            x.reset_index(drop=True)
-            .drop(columns=["fac"])
-            .rename(columns={"group": "groupx"})
-        )
-        xy = pd.merge(x, y, on=["date", "code"])
-        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
-        xy = (
-            xy.reset_index(drop=True)
-            .drop(columns=["fac"])
-            .rename(columns={"group": "groupy"})
-        )
-        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
-        xy = xy[["date", "code", "fac"]]
-        xy = xy.set_index(["date", "code"]).unstack()
-        xy.columns = [i[1] for i in list(xy.columns)]
-        new_pure = pure_fall()
-        new_pure.monthly_factors = xy
-        return new_pure
-
-    def __rshift__(self, selfa):
-        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
-        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
-        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
-        x = self.monthly_factors.copy()
-        y = selfa.monthly_factors.copy()
-        x = x.stack().reset_index()
-        y = y.stack().reset_index()
-        x.columns = ["date", "code", "fac"]
-        y.columns = ["date", "code", "fac"]
-        shen = pure_moon()
-        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
-        x = (
-            x.reset_index(drop=True)
-            .drop(columns=["fac"])
-            .rename(columns={"group": "groupx"})
-        )
-        xy = pd.merge(x, y, on=["date", "code"])
-        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
-        xy = (
-            xy.reset_index(drop=True)
-            .drop(columns=["fac"])
-            .rename(columns={"group": "groupy"})
-        )
-        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
-        xy = xy[["date", "code", "fac"]]
-        xy = xy.set_index(["date", "code"]).unstack()
-        xy.columns = [i[1] for i in list(xy.columns)]
-        new_pure = pure_fall()
-        new_pure.monthly_factors = xy
-        return new_pure
-
     def wide_to_long(self, df, i):
         """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
         df = df.stack().reset_index()
         df.columns = ["date", "code", i]
         df = df.set_index(["date", "code"])
         return df
 
@@ -2616,97 +2911,15 @@
         åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
         è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
         """
         df = df.T
         df = (df - df.mean()) / df.std()
         df = df.T
         return df
-
-    def get_single_day_factor(self, func: Callable, day: int) -> pd.DataFrame:
-        """è®¡ç®—å•æ—¥çš„å› å­å€¼ï¼Œé€šè¿‡sqlæ•°æ®åº“ï¼Œè¯»å–å•æ—¥çš„æ•°æ®ï¼Œç„¶åè®¡ç®—å› å­å€¼"""
-        sql = sqlConfig("minute_data_stock_alter")
-        df = sql.get_data(str(day))
-        the_func = partial(func)
-        df = df.groupby(["code"]).apply(the_func).to_frame()
-        df.columns = [str(day)]
-        df = df.T
-        df.index = pd.to_datetime(df.index, format="%Y%m%d")
-        return df
-
-    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
-    def get_daily_factors_alter(self, func: Callable) -> None:
-        """ç”¨mysqlé€æ—¥æ›´æ–°åˆ†é’Ÿæ•°æ®æ„é€ çš„å› å­
-
-        Parameters
-        ----------
-        func : Callable
-            æ„é€ åˆ†é’Ÿæ•°æ®ä½¿ç”¨çš„å‡½æ•°
-
-        Raises
-        ------
-        `IOError`
-            å¦‚æœæ²¡æœ‰å†å²å› å­æ•°æ®ï¼Œå°†æŠ¥é”™
-        """
-        """é€šè¿‡minute_data_stock_alteræ•°æ®åº“ä¸€å¤©ä¸€å¤©è®¡ç®—å› å­å€¼"""
-        try:
-            try:
-                self.daily_factors = pd.read_feather(self.daily_factors_path)
-            except Exception:
-                self.daily_factors_path = self.daily_factors_path.split("æ—¥é¢‘_")
-                self.daily_factors_path = (
-                    self.daily_factors_path[0] + self.daily_factors_path[1]
-                )
-                self.daily_factors = pd.read_feather(self.daily_factors_path)
-            self.daily_factors = self.daily_factors.rename(
-                columns={list(self.daily_factors.columns)[0]: "date"}
-            )
-            self.daily_factors = self.daily_factors.drop_duplicates(
-                subset=["date"], keep="last"
-            )
-            self.daily_factors = self.daily_factors.set_index("date")
-            sql = sqlConfig("minute_data_stock_alter")
-            now_minute_datas = sql.show_tables(full=False)
-            now_minute_data = now_minute_datas[-1]
-            now_minute_data = pd.Timestamp(now_minute_data)
-            if self.daily_factors.index.max() < now_minute_data:
-                if not STATES["NO_LOG"]:
-                    logger.info(
-                        f"ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦"
-                    )
-                old_end = datetime.datetime.strftime(
-                    self.daily_factors.index.max(), "%Y%m%d"
-                )
-                now_minute_datas = [i for i in now_minute_datas if i > old_end]
-                dfs = []
-                for c in tqdm.tqdm(now_minute_datas, desc="æ¡‚æ£¹å…®å…°æ¡¨ï¼Œå‡»ç©ºæ˜å…®é‚æµå…‰ğŸŒŠ"):
-                    df = self.get_single_day_factor(func, c)
-                    dfs.append(df)
-                dfs = pd.concat(dfs)
-                dfs = dfs.sort_index()
-                self.daily_factors = pd.concat([self.daily_factors, dfs])
-                self.daily_factors = self.daily_factors.dropna(how="all")
-                self.daily_factors = self.daily_factors[
-                    self.daily_factors.index >= pd.Timestamp(str(STATES["START"]))
-                ]
-            self.daily_factors = self.daily_factors.reset_index()
-            self.daily_factors = self.daily_factors.rename(
-                columns={list(self.daily_factors.columns)[0]: "date"}
-            )
-            self.daily_factors = self.daily_factors.drop_duplicates(
-                subset=["date"], keep="first"
-            )
-            self.daily_factors = self.daily_factors.set_index("date")
-            self.daily_factors.reset_index().to_feather(self.daily_factors_path)
-            if not STATES["NO_LOG"]:
-                logger.success("æ›´æ–°å·²å®Œæˆ")
-
-        except Exception:
-            raise IOError(
-                "æ‚¨è¿˜æ²¡æœ‰è¯¥å› å­çš„åˆçº§æ•°æ®ï¼Œæš‚æ—¶ä¸èƒ½æ›´æ–°ã€‚è¯·å…ˆä½¿ç”¨pure_fall_frequentæˆ–pure_fall_flexibleè®¡ç®—å†å²å› å­å€¼ã€‚"
-            )
+    
 
 
 class pure_fallmount(pure_fall):
     """ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨"""
 
     def __init__(self, monthly_factors):
         """è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡"""
@@ -2756,18 +2969,15 @@
             fac1 = fac1 * i
         new_pure = pure_fall()
         new_pure.monthly_factors = fac1
         return new_pure
 
     def __sub__(self, selfa):
         """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
-        if is_notebook():
-            tqdm.tqdm_notebook().pandas()
-        else:
-            tqdm.tqdm.pandas()
+        tqdm.auto.tqdm.pandas()
         if not isinstance(selfa, Iterable):
             if not STATES["NO_LOG"]:
                 logger.warning(f"{selfa} is changed into Iterable")
             selfa = (selfa,)
         fac_main = self.wide_to_long(self.monthly_factors, "fac")
         fac_helps = [i.monthly_factors for i in selfa]
         help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
@@ -2847,112 +3057,115 @@
 
 class pure_fall_frequent(object):
     """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""
 
     def __init__(
         self,
         factor_file: str,
+        project: str = None,
         startdate: int = None,
         enddate: int = None,
+        questdb_host: str = "127.0.0.1",
         kind: str = "stock",
         clickhouse: bool = 0,
         questdb: bool = 0,
+        questdb_web_port: str = "9001",
         ignore_history_in_questdb: bool = 0,
         groupby_target: list = ["date", "code"],
     ) -> None:
         """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®
 
         Parameters
         ----------
         factor_file : str
-            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºfeatheræ–‡ä»¶ï¼Œä»¥'.feather'ç»“å°¾
+            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
+        project : str, optional
+            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
         startdate : int, optional
             èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
         enddate : int, optional
             æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
+        questdb_host: str, optional
+            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
         kind : str, optional
             ç±»å‹ä¸ºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°ä¸º'index', by default "stock"
         clickhouse : bool, optional
             ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
         questdb : bool, optional
             ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
+        questdb_web_port : str, optional
+            questdbçš„web_port, by default '9001'
         ignore_history_in_questdb : bool, optional
             æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
         groupby_target: list, optional
             groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
             åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
         """
         homeplace = HomePlace()
         self.kind = kind
         self.groupby_target = groupby_target
         if clickhouse == 0 and questdb == 0:
             clickhouse = 1
         self.clickhouse = clickhouse
         self.questdb = questdb
+        self.questdb_web_port = questdb_web_port
         if clickhouse == 1:
             # è¿æ¥clickhouse
             self.chc = ClickHouseClient("minute_data")
         elif questdb == 1:
-            self.chc = Questdb()
+            self.chc = Questdb(host=questdb_host, web_port=questdb_web_port)
         # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
         pinyin = Pinyin()
         self.factor_file_pinyin = pinyin.get_pinyin(
-            factor_file.replace(".feather", ""), ""
+            factor_file.replace(".parquet", ""), ""
         )
-        self.factor_steps = Questdb()
+        self.factor_steps = Questdb(host=questdb_host, web_port=questdb_web_port)
+        if project is not None:
+            if not os.path.exists(homeplace.factor_data_file + project):
+                os.makedirs(homeplace.factor_data_file + project)
+            else:
+                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
+        else:
+            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
         # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
-        factor_file = homeplace.factor_data_file + factor_file
+        if project is not None:
+            factor_file = homeplace.factor_data_file + project + "/" + factor_file
+        else:
+            factor_file = homeplace.factor_data_file + factor_file
         self.factor_file = factor_file
         # è¯»å…¥ä¹‹å‰çš„å› å­
         if os.path.exists(factor_file):
-            factor_old = pd.read_feather(self.factor_file)
-            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
-            factor_old = factor_old.drop_duplicates(subset=["date"])
-            factor_old = factor_old.set_index("date")
+            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
             self.factor_old = factor_old
             # å·²ç»ç®—å¥½çš„æ—¥å­
             dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
             self.dates_old = dates_old
         elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
             self.factor_steps.get_data("show tables").table
         ):
             logger.info(
                 f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
             )
-            factor_old = self.factor_steps.get_data(
-                f"select * from {self.factor_file_pinyin}"
-            )
-            # åˆ¤æ–­ä¸€ä¸‹æ¯å¤©æ˜¯å¦ç”Ÿæˆå¤šä¸ªæ•°æ®ï¼Œå•ä¸ªæ•°æ®å°±ä»¥floatå½¢å¼å­˜å‚¨ï¼Œå¤šä¸ªæ•°æ®ä»¥listå½¢å¼å­˜å‚¨
-            if "f0" in list(factor_old.columns):
-                factor_old = factor_old[factor_old.f0 != "date"]
-                factor_old.columns = ["date", "code", "fac"]
-                try:
-                    factor_old.fac = factor_old.fac.apply(
-                        lambda x: [float(i) for i in x[1:-1].split(" ") if i != ""]
-                    )
-                except Exception:
-                    factor_old.fac = factor_old.fac.apply(
-                        lambda x: [float(i) for i in x[1:-1].split(", ") if i != ""]
-                    )
+            factor_old = self.factor_steps.get_data_with_tuple(
+                f"select * from '{self.factor_file_pinyin}'"
+            ).drop_duplicates(subset=["date", "code"])
             factor_old = factor_old.pivot(index="date", columns="code", values="fac")
-            factor_old.index = pd.to_datetime(factor_old.index)
             factor_old = factor_old.sort_index()
-            factor_old = drop_duplicates_index(factor_old)
             self.factor_old = factor_old
             # å·²ç»ç®—å¥½çš„æ—¥å­
             dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
             self.dates_old = dates_old
         elif ignore_history_in_questdb and self.factor_file_pinyin in list(
             self.factor_steps.get_data("show tables").table
         ):
             logger.info(
                 f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
             )
             factor_old = self.factor_steps.do_order(
-                f"drop table {self.factor_file_pinyin}"
+                f"drop table '{self.factor_file_pinyin}'"
             )
             self.factor_old = None
             self.dates_old = []
             logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
         else:
             self.factor_old = None
             self.dates_old = []
@@ -3002,14 +3215,25 @@
         Returns
         -------
         `pd.DataFrame`
             ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
         """
         return self.factor.copy()
 
+    def forward_dates(self, dates, many_days):
+        dates_index = [self.dates_all.index(i) for i in dates]
+
+        def value(x, a):
+            if x >= 0:
+                return a[x]
+            else:
+                return None
+
+        return [value(i - many_days, self.dates_all) for i in dates_index]
+
     def select_one_calculate(
         self,
         date: pd.Timestamp,
         func: Callable,
         fields: str = "*",
         show_time: bool = 0,
     ) -> None:
@@ -3035,162 +3259,174 @@
             df = df.sort_values(["date", "num"])
         df = df.groupby(self.groupby_target).apply(the_func)
         if self.groupby_target == ["date", "code"]:
             df = df.to_frame("fac").reset_index()
             df.columns = ["date", "code", "fac"]
         else:
             df = df.reset_index()
-        df = df.pivot(columns="code", index="date", values="fac")
-        df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-        return df
+        if (df is not None) and (df.shape[0] > 0):
+            df = df.pivot(columns="code", index="date", values="fac")
+            df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+            to_save = df.stack().reset_index()
+            to_save.columns = ["date", "code", "fac"]
+            self.factor_steps.write_via_df(
+                to_save, self.factor_file_pinyin, tuple_col="fac"
+            )
+            return df
 
     def select_many_calculate(
         self,
-        dates: list[pd.Timestamp],
+        dates: List[pd.Timestamp],
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        tqdm_inside: bool = 0,
+        many_days: int = 1,
+        n_jobs: int = 1,
     ) -> None:
         the_func = partial(func)
-        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
-        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
-        dates_new_len = len(dates)
-        cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
-        if cut_points[-1] == cut_points[-2]:
-            cut_points = cut_points[:-1]
-        cut_first = cut_points[0]
-        cuts = tuple(zip(cut_points[:-1], cut_points[1:]))
-        print(f"å…±{len(cuts)}æ®µ")
         factor_new = []
-        df_first = self.select_one_calculate(
-            date=dates[0],
-            func=func,
-            fields=fields,
-            show_time=show_time,
-        )
-        factor_new.append(df_first)
-        to_save = df_first.stack().reset_index()
-        to_save.columns = ["date", "code", "fac"]
-        self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
-
-        if tqdm_inside == 1:
-            # å¼€å§‹è®¡ç®—å› å­å€¼
-            for date1, date2 in cuts:
+        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
+        if many_days == 1:
+            # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
+            dates_new_len = len(dates)
+            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
+            if cut_points[-1] == cut_points[-2]:
+                cut_points = cut_points[:-1]
+            cuts = tuple(zip(cut_points[:-many_days], cut_points[many_days:]))
+            df_first = self.select_one_calculate(
+                date=dates[0],
+                func=func,
+                fields=fields,
+                show_time=show_time,
+            )
+            factor_new.append(df_first)
+
+            def cal_one(date1, date2):
                 if self.clickhouse == 1:
                     sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                 else:
-                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
+                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]} order by code,date,num"
                 if show_time:
                     df = self.chc.get_data_show_time(sql_order)
                 else:
                     df = self.chc.get_data(sql_order)
                 if self.clickhouse == 1:
                     df = ((df.set_index("code")) / 100).reset_index()
                 else:
                     df.num = df.num.astype(int)
                     df.date = df.date.astype(int)
                     df = df.sort_values(["date", "num"])
-                if is_notebook():
-                    tqdm.tqdm_notebook().pandas()
+                df = df.groupby(self.groupby_target).apply(the_func)
+                if self.groupby_target == ["date", "code"]:
+                    df = df.to_frame("fac").reset_index()
+                    df.columns = ["date", "code", "fac"]
                 else:
-                    tqdm.tqdm.pandas()
-                df = df.groupby(self.groupby_target).progress_apply(the_func)
-                df = df.to_frame("fac").reset_index()
-                df.columns = ["date", "code", "fac"]
+                    df = df.reset_index()
                 df = df.pivot(columns="code", index="date", values="fac")
                 df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                factor_new.append(df)
                 to_save = df.stack().reset_index()
                 to_save.columns = ["date", "code", "fac"]
-                self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
-        else:
-            if is_notebook():
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in tqdm.tqdm_notebook(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
-                    if self.clickhouse == 1:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
-                    else:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1]} and date<={dates[date2]} order by code,date,num"
-                    if show_time:
-                        df = self.chc.get_data_show_time(sql_order)
-                    else:
-                        df = self.chc.get_data(sql_order)
-                    if self.clickhouse == 1:
-                        df = ((df.set_index("code")) / 100).reset_index()
-                    df = df.groupby(self.groupby_target).apply(the_func)
-                    if self.groupby_target == ["date", "code"]:
-                        df = df.to_frame("fac").reset_index()
-                        df.columns = ["date", "code", "fac"]
-                    else:
-                        df = df.reset_index()
-                    df = df.pivot(columns="code", index="date", values="fac")
-                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                    factor_new.append(df)
-                    to_save = df.stack().reset_index()
-                    to_save.columns = ["date", "code", "fac"]
-                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
+                self.factor_steps.write_via_df(
+                    to_save, self.factor_file_pinyin, tuple_col="fac"
+                )
+                return df
+
+            if n_jobs > 1:
+                with WorkerPool(n_jobs=n_jobs) as pool:
+                    factor_new_more = pool.map(cal_one, cuts, progress_bar=True)
+                factor_new = factor_new + factor_new_more
             else:
                 # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in tqdm.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
+                for date1, date2 in tqdm.auto.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
+                    df = cal_one(date1, date2)
+                    factor_new.append(df)
+        else:
+
+            def cal_two(date1, date2):
+                if date1 is not None:
                     if self.clickhouse == 1:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{date1*100} and date<={date2*100} order by code,date,num"
                     else:
-                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
+                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{date1} and cast(date as int)<={date2} order by code,date,num"
                     if show_time:
                         df = self.chc.get_data_show_time(sql_order)
                     else:
                         df = self.chc.get_data(sql_order)
                     if self.clickhouse == 1:
                         df = ((df.set_index("code")) / 100).reset_index()
                     else:
                         df.num = df.num.astype(int)
                         df.date = df.date.astype(int)
                         df = df.sort_values(["date", "num"])
-                    df = df.groupby(self.groupby_target).apply(the_func)
-                    if self.groupby_target == ["date", "code"]:
-                        df = df.to_frame("fac").reset_index()
-                        df.columns = ["date", "code", "fac"]
+                    if self.groupby_target == [
+                        "date",
+                        "code",
+                    ] or self.groupby_target == ["code"]:
+                        df = df.groupby(["code"]).apply(the_func).reset_index()
                     else:
-                        df = df.reset_index()
+                        df = the_func(df)
+                    df = df.assign(date=date2)
+                    df.columns = ["code", "fac", "date"]
                     df = df.pivot(columns="code", index="date", values="fac")
                     df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                    factor_new.append(df)
                     to_save = df.stack().reset_index()
                     to_save.columns = ["date", "code", "fac"]
-                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
-        factor_new = pd.concat(factor_new)
-        return factor_new
+                    self.factor_steps.write_via_df(
+                        to_save, self.factor_file_pinyin, tuple_col="fac"
+                    )
+                    return df
+
+            pairs = self.forward_dates(dates, many_days=many_days)
+            cuts2 = tuple(zip(pairs, dates))
+            if n_jobs > 1:
+                with WorkerPool(n_jobs=n_jobs) as pool:
+                    factor_new_more = pool.map(cal_two, cuts2, progress_bar=True)
+                factor_new = factor_new + factor_new_more
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in tqdm.auto.tqdm(cuts2, desc="çŸ¥ä¸å¯ä¹éª¤å¾—ï¼Œæ‰˜é—å“äºæ‚²é£ã€‚"):
+                    df = cal_two(date1, date2)
+                    factor_new.append(df)
+
+        if len(factor_new) > 0:
+            factor_new = pd.concat(factor_new)
+            return factor_new
+        else:
+            return None
 
     def select_any_calculate(
         self,
-        dates: list[pd.Timestamp],
+        dates: List[pd.Timestamp],
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        tqdm_inside: bool = 0,
+        many_days: int = 1,
+        n_jobs: int = 1,
     ) -> None:
-        if len(dates) == 1:
+        if len(dates) == 1 and many_days == 1:
             res = self.select_one_calculate(
                 dates[0],
                 func=func,
                 fields=fields,
                 show_time=show_time,
             )
         else:
             res = self.select_many_calculate(
                 dates=dates,
                 func=func,
                 fields=fields,
                 chunksize=chunksize,
                 show_time=show_time,
-                tqdm_inside=tqdm_inside,
+                many_days=many_days,
+                n_jobs=n_jobs,
             )
+        if res is not None:
+            self.factor_new.append(res)
         return res
 
     @staticmethod
     def for_cross_via_str(func):
         """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
         ä¾‹å¦‚
         ```python
@@ -3224,31 +3460,45 @@
                 )
         ```
         ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
         """
 
         def full_run(df, *args, **kwargs):
             res = func(df, *args, **kwargs)
-            res = pd.concat(res, axis=1)
-            res.columns = [f"fac{i}" for i in range(len(res.columns))]
-            res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
-            res = res[["fac"]].reset_index()
-            res.columns = ["code", "fac"]
-            return res
+            if isinstance(res, pd.Series):
+                res = res.reset_index()
+                res.columns = ["code", "fac"]
+                return res
+            elif isinstance(res, pd.DataFrame):
+                res.columns = [f"fac{i}" for i in range(len(res.columns))]
+                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
+                res = res[["fac"]].reset_index()
+                res.columns = ["code", "fac"]
+                return res
+            elif res is None:
+                ...
+            else:
+                res = pd.concat(res, axis=1)
+                res.columns = [f"fac{i}" for i in range(len(res.columns))]
+                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
+                res = res[["fac"]].reset_index()
+                res.columns = ["code", "fac"]
+                return res
 
         return full_run
 
     @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
     def get_daily_factors(
         self,
         func: Callable,
         fields: str = "*",
         chunksize: int = 10,
         show_time: bool = 0,
-        tqdm_inside: bool = 0,
+        many_days: int = 1,
+        n_jobs: int = 1,
     ) -> None:
         """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
         å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼
 
         Parameters
         ----------
         func : Callable
@@ -3256,292 +3506,74 @@
         fields : str, optional
             è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
             æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
         chunksize : int, optional
             æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
         show_time : bool, optional
             å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
-        tqdm_inside : bool, optional
-            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
+        many_days : int, optional
+            è®¡ç®—æŸå¤©çš„å› å­å€¼æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¹‹å‰å¤šå°‘å¤©çš„æ•°æ®
+        n_jobs : int, optional
+            å¹¶è¡Œæ•°é‡, by default 1
         """
         if len(self.dates_new) > 0:
             for interval in self.dates_new_intervals:
                 df = self.select_any_calculate(
                     dates=interval,
                     func=func,
                     fields=fields,
                     chunksize=chunksize,
                     show_time=show_time,
-                    tqdm_inside=tqdm_inside,
+                    many_days=many_days,
+                    n_jobs=n_jobs,
                 )
-                self.factor_new.append(df)
-            self.factor_new = pd.concat(self.factor_new)
-            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
-            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
-            self.factor = self.factor.dropna(how="all")
-            self.factor = self.factor.reset_index()
-            self.factor = self.factor.rename(
-                columns={list(self.factor.columns)[0]: "date"}
-            )
-            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
-            self.factor = self.factor.set_index("date")
-            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
-            # å­˜å…¥æœ¬åœ°
-            self.factor.reset_index().to_feather(self.factor_file)
-            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
-            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
-            self.factor_steps.do_order(f"drop table {self.factor_file_pinyin}")
-            logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
-
-        else:
-            self.factor = self.factor_old
-            self.factor = self.factor.reset_index()
-            self.factor = self.factor.rename(
-                columns={list(self.factor.columns)[0]: "date"}
-            )
-            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
-            self.factor = self.factor.set_index("date")
-            # å­˜å…¥æœ¬åœ°
-            self.factor.reset_index().to_feather(self.factor_file)
-            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
-            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
-
+            if len(self.factor_new)>0:
+                self.factor_new = pd.concat(self.factor_new)
+                # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+                self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
+                self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
+                new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+                # å­˜å…¥æœ¬åœ°
+                self.factor.to_parquet(self.factor_file)
+                logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
+                # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
+                try:
+                    self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+                    logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
+                except Exception:
+                    logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")
+            else:
+                logger.warning('ç”±äºæŸç§åŸå› ï¼Œæ›´æ–°çš„å› å­å€¼è®¡ç®—å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ğŸ¤’')
+                # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+                self.factor = pd.concat([self.factor_old]).sort_index()
+                self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
 
-class pure_fall_flexible(object):
-    def __init__(
-        self,
-        factor_file: str,
-        startdate: int = None,
-        enddate: int = None,
-        kind: str = "stock",
-        clickhouse: bool = 0,
-        questdb: bool = 0,
-    ) -> None:
-        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼ç”¨åˆ°å¤šæ—¥çš„æ•°æ®ï¼Œæˆ–è€…ç”¨åˆ°æˆªé¢çš„æ•°æ®
-        å¯¹ä¸€æ®µæ—¶é—´çš„æˆªé¢æ•°æ®è¿›è¡Œæ“ä½œï¼Œåœ¨get_daily_factorsçš„funcå‡½æ•°ä¸­
-        è¯·å†™å…¥df=df.groupby([xxx]).apply(fff)ä¹‹ç±»çš„è¯­å¥
-        ç„¶åå•ç‹¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½œä¸ºè¦applyçš„fffï¼Œå¯ä»¥åœ¨applyä¸ŠåŠ è¿›åº¦æ¡
 
-        Parameters
-        ----------
-        factor_file : str
-            ç”¨äºå­˜å‚¨å› å­çš„æ–‡ä»¶åç§°ï¼Œè¯·ä»¥'.feather'ç»“å°¾
-        startdate : int, optional
-            è®¡ç®—å› å­çš„èµ·å§‹æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
-        enddate : int, optional
-            è®¡ç®—å› å­çš„ç»ˆæ­¢æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
-        kind : str, optional
-            æŒ‡å®šè®¡ç®—è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°åˆ™ä¸º'index', by default "stock"
-        clickhouse : bool, optional
-            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
-        questdb : bool, optional
-            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
-        """
-        homeplace = HomePlace()
-        self.kind = kind
-        if clickhouse == 0 and questdb == 0:
-            clickhouse = 1
-        self.clickhouse = clickhouse
-        self.questdb = questdb
-        if clickhouse == 1:
-            # è¿æ¥clickhouse
-            self.chc = ClickHouseClient("minute_data")
-        elif questdb:
-            self.chc = Questdb()
-        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
-        factor_file = homeplace.factor_data_file + factor_file
-        self.factor_file = factor_file
-        # è¯»å…¥ä¹‹å‰çš„å› å­
-        if os.path.exists(factor_file):
-            factor_old = pd.read_feather(self.factor_file)
-            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
-            factor_old = factor_old.drop_duplicates(subset=["date"])
-            factor_old = factor_old.set_index("date")
-            self.factor_old = factor_old
-            # å·²ç»ç®—å¥½çš„æ—¥å­
-            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
-            self.dates_old = dates_old
-        else:
-            self.factor_old = None
-            self.dates_old = []
-            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
-        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
-        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
-        if startdate is None:
-            ...
-        else:
-            dates_all = [i for i in dates_all if i >= startdate]
-        if enddate is None:
-            ...
         else:
-            dates_all = [i for i in dates_all if i <= enddate]
-        self.dates_all = dates_all
-        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
-        self.dates_new = sorted([i for i in dates_all if i not in dates_old])
-
-    def __call__(self) -> pd.DataFrame:
-        """ç›´æ¥è¿”å›å› å­å€¼çš„pd.DataFrame
-
-        Returns
-        -------
-        `pd.DataFrame`
-            è®¡ç®—å‡ºçš„å› å­å€¼
-        """
-        return self.factor.copy()
-
-    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
-    def get_daily_factors(
-        self,
-        func: Callable,
-        fields: str = "*",
-        chunksize: int = 250,
-        show_time: bool = 0,
-        tqdm_inside: bool = 0,
-    ) -> None:
-        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
-        ä¾ç…§å®šä¹‰çš„å‡½æ•°è®¡ç®—å› å­å€¼
-
-        Parameters
-        ----------
-        func : Callable
-            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
-        fields : str, optional
-            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
-            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
-        chunksize : int, optional
-            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
-        show_time : bool, optional
-            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
-        tqdm_inside : bool, optional
-            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
-        """
-        the_func = partial(func)
-        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
-        dates_new_len = len(self.dates_new)
-        if dates_new_len > 0:
-            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
-            if cut_points[-1] == cut_points[-2]:
-                cut_points = cut_points[:-1]
-            self.cut_points = cut_points
-            self.factor_new = []
-            # å¼€å§‹è®¡ç®—å› å­å€¼
-            if tqdm_inside:
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in cut_points:
-                    if self.clickhouse == 1:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
-                    else:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]}"
-                    if show_time:
-                        df = self.chc.get_data_show_time(sql_order)
-                    else:
-                        df = self.chc.get_data(sql_order)
-                    if self.clickhouse == 1:
-                        df = ((df.set_index("code")) / 100).reset_index()
-                    if is_notebook():
-                        tqdm.tqdm_notebook().pandas()
-                    else:
-                        tqdm.tqdm.pandas()
-                    df = the_func(df)
-                    if isinstance(df, pd.Series):
-                        df = df.reset_index()
-                    df.columns = ["date", "code", "fac"]
-                    df = df.pivot(columns="code", index="date", values="fac")
-                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                    self.factor_new.append(df)
-            else:
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                for date1, date2 in tqdm.tqdm(cut_points, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
-                    if self.clickhouse == 1:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
-                    else:
-                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
-                    if show_time:
-                        df = self.chc.get_data_show_time(sql_order)
-                    else:
-                        df = self.chc.get_data(sql_order)
-                    if self.clickhouse == 1:
-                        df = ((df.set_index("code")) / 100).reset_index()
-                    df = the_func(df)
-                    if isinstance(df, pd.Series):
-                        df = df.reset_index()
-                    df.columns = ["date", "code", "fac"]
-                    df = df.pivot(columns="code", index="date", values="fac")
-                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                    self.factor_new.append(df)
-            self.factor_new = pd.concat(self.factor_new)
-            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
-            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
-            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
-            # å­˜å…¥æœ¬åœ°
-            self.factor.reset_index().to_feather(self.factor_file)
-            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
-        elif dates_new_len == 1:
-            print("å…±1å¤©")
-            if tqdm_inside:
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                if self.clickhouse == 1:
-                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
-                else:
-                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
-                if show_time:
-                    df = self.chc.get_data_show_time(sql_order)
-                else:
-                    df = self.chc.get_data(sql_order)
-                if self.clickhouse == 1:
-                    df = ((df.set_index("code")) / 100).reset_index()
-                tqdm.tqdm.pandas()
-                df = the_func(df)
-                if isinstance(df, pd.Series):
-                    df = df.reset_index()
-                df.columns = ["date", "code", "fac"]
-                df = df.pivot(columns="code", index="date", values="fac")
-                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-            else:
-                # å¼€å§‹è®¡ç®—å› å­å€¼
-                if self.clickhouse == 1:
-                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
-                else:
-                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
-                if show_time:
-                    df = self.chc.get_data_show_time(sql_order)
-                else:
-                    df = self.chc.get_data(sql_order)
-                if self.clickhouse == 1:
-                    df = ((df.set_index("code")) / 100).reset_index()
-                df = the_func(df)
-                if isinstance(df, pd.Series):
-                    df = df.reset_index()
-                df.columns = ["date", "code", "fac"]
-                df = df.pivot(columns="code", index="date", values="fac")
-                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
-                self.factor_new.append(df)
-            self.factor_new = df
-            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
-            self.factor = (
-                pd.concat([self.factor_old, self.factor_new])
-                .sort_index()
-                .drop_duplicates()
-            )
-            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            self.factor = drop_duplicates_index(self.factor_old)
             # å­˜å…¥æœ¬åœ°
-            self.factor.reset_index().to_feather(self.factor_file)
-            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
-        else:
-            self.factor = self.factor_old
+            self.factor.to_parquet(self.factor_file)
             new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
             logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
 
+    def drop_table(self):
+        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
+        try:
+            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
+        except Exception:
+            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")
+
 
 class pure_coldwinter(object):
     # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­
 
     def __init__(
         self,
-        facs_dict: dict = None,
+        facs_dict: Dict = None,
         momentum: bool = 1,
         earningsyield: bool = 1,
         growth: bool = 1,
         liquidity: bool = 1,
         size: bool = 1,
         leverage: bool = 1,
         beta: bool = 1,
@@ -3549,15 +3581,15 @@
         residualvolatility: bool = 1,
         booktoprice: bool = 1,
     ) -> None:
         """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­
 
         Parameters
         ----------
-        facs_dict : dict, optional
+        facs_dict : Dict, optional
             é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
         momentum : bool, optional
             æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
         earningsyield : bool, optional
             æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
         growth : bool, optional
             æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
@@ -3575,21 +3607,19 @@
             æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
         booktoprice : bool, optional
             æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
         """
         self.homeplace = HomePlace()
         # barraå› å­æ•°æ®
         styles = os.listdir(self.homeplace.barra_data_file)
-        styles = [i for i in styles if i.endswith(".feather")]
+        styles = [i for i in styles if i.endswith(".parquet")]
         barras = {}
         for s in styles:
             k = s.split(".")[0]
-            v = pd.read_feather(self.homeplace.barra_data_file + s)
-            v.columns = ["date"] + list(v.columns)[1:]
-            v = v.set_index("date")
+            v = pd.read_parquet(self.homeplace.barra_data_file + s)
             barras[k] = v
         rename_dict = {
             "fac": "å› å­è‡ªèº«",
             "size": "å¸‚å€¼",
             "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
             "booktoprice": "ä¼°å€¼",
             "earningsyield": "ç›ˆåˆ©",
@@ -3685,15 +3715,15 @@
             [self.factors] + list(self.barras.values()), axis=1
         ).dropna()
 
     # DONE: ä¿®æ”¹é£æ ¼å› å­å±•ç¤ºé¡ºåºè‡³æŠ¥å‘Šçš„é¡ºåº
     def get_corr(self):
         """è®¡ç®—æ¯ä¸€æœŸçš„ç›¸å…³ç³»æ•°ï¼Œå†æ±‚å¹³å‡å€¼"""
         self.corr_by_step = self.corr_pri.groupby(["date"]).apply(
-            lambda x: x.corr().head(1)
+            lambda x: x.rank().corr().head(1)
         )
         self.__corr = self.corr_by_step.mean()
         self.__corr = self.__corr.rename(index=self.rename_dict)
         self.__corr = self.__corr.to_frame("ç›¸å…³ç³»æ•°").T
 
         self.__corr = self.__corr[self.sort_names]
         self.__corr = self.__corr.T
@@ -3736,21 +3766,22 @@
         self.get_monthly_barras_industrys()
         self.get_wide_barras_industrys()
         self.get_corr_pri_ols_pri()
         self.get_corr()
         self.get_snow_fac()
 
 
+@do_on_dfs
 class pure_snowtrain(object):
     """ç›´æ¥è¿”å›çº¯å‡€å› å­"""
 
     def __init__(
         self,
         factors: pd.DataFrame,
-        facs_dict: dict = None,
+        facs_dict: Dict = None,
         momentum: bool = 1,
         earningsyield: bool = 1,
         growth: bool = 1,
         liquidity: bool = 1,
         size: bool = 1,
         leverage: bool = 1,
         beta: bool = 1,
@@ -3760,36 +3791,36 @@
     ) -> None:
         """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­
 
         Parameters
         ----------
         factors : pd.DataFrame
             è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-        facs_dict : dict, optional
-            _description_, by default None
+        facs_dict : Dict, optional
+            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
         momentum : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
         earningsyield : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
         growth : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
         liquidity : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
         size : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
         leverage : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
         beta : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
         nonlinearsize : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
         residualvolatility : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
         booktoprice : bool, optional
-            _description_, by default 1
+            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
         """
         self.winter = pure_coldwinter(
             facs_dict=facs_dict,
             momentum=momentum,
             earningsyield=earningsyield,
             growth=growth,
             liquidity=liquidity,
@@ -3810,23 +3841,34 @@
         Returns
         -------
         pd.DataFrame
             çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
         """
         return self.winter.snow_fac.copy()
 
+    def show_corr(self) -> pd.DataFrame:
+        """å±•ç¤ºå› å­ä¸barraé£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
+
+        Returns
+        -------
+        pd.DataFrame
+            ç›¸å…³ç³»æ•°è¡¨æ ¼
+        """
+        return self.corr.T.applymap(lambda x: to_percent(x))
+
 
 class pure_newyear(object):
     """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""
 
     def __init__(
         self,
         facx: pd.DataFrame,
         facy: pd.DataFrame,
         group_num_single: int,
+        trade_cost_double_side: float = 0,
         namex: str = "ä¸»",
         namey: str = "æ¬¡",
     ) -> None:
         """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
         ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡
 
         Parameters
@@ -3835,27 +3877,33 @@
             é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
             indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
         facy : pd.DataFrame
             åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
             indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
         group_num_single : int
             å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
+        trade_cost_double_side : float, optional
+            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
         namex : str, optional
             facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
         namey : str, optional
             facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
         """
         homex = pure_fallmount(facx)
         homey = pure_fallmount(facy)
         if group_num_single == 5:
             homexy = homex > homey
         elif group_num_single == 10:
             homexy = homex >> homey
         shen = pure_moonnight(
-            homexy(), group_num_single**2, plt_plot=False, print_comments=False
+            homexy(),
+            group_num_single**2,
+            trade_cost_double_side=trade_cost_double_side,
+            plt_plot=False,
+            print_comments=False,
         )
         sq = shen.shen.square_rets.copy()
         sq.index = [namex + str(i) for i in list(sq.index)]
         sq.columns = [namey + str(i) for i in list(sq.columns)]
         self.square_rets = sq
 
     def __call__(self) -> pd.DataFrame:
@@ -3865,290 +3913,528 @@
         -------
         `pd.DataFrame`
             æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
         """
         return self.square_rets.copy()
 
 
-class pure_dawn(object):
-    """
-    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
-    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
-    ä½¿ç”¨ä¸¾ä¾‹
-    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
-    ```python
-    class Cut(pure_dawn):
-
-    def cut(self,df):
-        df=df.sort_values('fac1')
-        df=df.assign(fac3=df.fac1*df.fac2)
-        ret0=df.fac2.iloc[:4].mean()
-        ret1=df.fac2.iloc[4:8].mean()
-        ret2=df.fac2.iloc[8:12].mean()
-        ret3=df.fac2.iloc[12:16].mean()
-        ret4=df.fac2.iloc[16:].mean()
-        aret0=df.fac3.iloc[:4].mean()
-        aret1=df.fac3.iloc[4:8].mean()
-        aret2=df.fac3.iloc[8:12].mean()
-        aret3=df.fac3.iloc[12:16].mean()
-        aret4=df.fac3.iloc[16:].mean()
-        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4
-
-    cut=Cut(ct,ret_inday)
-    cut.run(cut.cut)
-
-    cut0=get_value(cut(),0)
-    cut1=get_value(cut(),1)
-    cut2=get_value(cut(),2)
-    cut3=get_value(cut(),3)
-    cut4=get_value(cut(),4)
-    ```
-    """
-
-    def __init__(self, fac1: pd.DataFrame, fac2: pd.DataFrame, *args: list) -> None:
-        """å‡ ä¸ªå› å­çš„æ“ä½œï¼Œæ¯ä¸ªæœˆæ“ä½œä¸€æ¬¡
-
-        Parameters
-        ----------
-        fac1 : pd.DataFrame
-            å› å­å€¼1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-        fac2 : pd.DataFrame
-            å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
-        """
-        self.fac1 = fac1
-        self.fac1 = self.fac1.stack().reset_index()
-        self.fac1.columns = ["date", "code", "fac1"]
-        self.fac2 = fac2
-        self.fac2 = self.fac2.stack().reset_index()
-        self.fac2.columns = ["date", "code", "fac2"]
-        fac_all = pd.merge(self.fac1, self.fac2, on=["date", "code"])
-        for i, fac in enumerate(args):
-            fac = fac.stack().reset_index()
-            fac.columns = ["date", "code", f"fac{i+3}"]
-            fac_all = pd.merge(fac_all, fac, on=["date", "code"])
-        fac_all = fac_all.sort_values(["date", "code"])
-        self.fac = fac_all.copy()
-
-    def __call__(self) -> pd.DataFrame:
-        """è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼
-
-        Returns
-        -------
-        `pd.DataFrame`
-            æœ€ç»ˆå› å­å€¼
-        """
-        return self.fac.copy()
-
-    def get_fac_long_and_tradedays(self):
-        """å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨"""
-        self.tradedays = sorted(list(set(self.fac.date)))
-
-    def get_month_starts_and_ends(self, backsee=20):
-        """è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥"""
-        self.month_ends = [
-            i
-            for i, j in zip(self.tradedays[:-1], self.tradedays[1:])
-            if i.month != j.month
-        ]
-        self.month_ends.append(self.tradedays[-1])
-        self.month_starts = [
-            self.find_begin(self.tradedays, i, backsee=backsee) for i in self.month_ends
-        ]
-        self.month_starts[0] = self.tradedays[0]
-
-    def find_begin(self, tradedays, end_day, backsee=20):
-        """æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20"""
-        end_day_index = tradedays.index(end_day)
-        start_day_index = end_day_index - backsee + 1
-        start_day = tradedays[start_day_index]
-        return start_day
-
-    def make_monthly_factors_single_code(self, df, func):
-        """
-        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
-        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
-        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
-        """
-        res = {}
-        for start, end in zip(self.month_starts, self.month_ends):
-            this_month = df[(df.date >= start) & (df.date <= end)]
-            res[end] = func(this_month)
-        dates = list(res.keys())
-        corrs = list(res.values())
-        part = pd.DataFrame({"date": dates, "corr": corrs})
-        return part
-
-    def get_monthly_factor(self, func):
-        """è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­"""
-        if is_notebook():
-            tqdm.tqdm_notebook().pandas()
-        else:
-            tqdm.tqdm.pandas(desc="when the dawn comes, tonight will be a memory too.")
-        self.fac = self.fac.groupby(["code"]).progress_apply(
-            lambda x: self.make_monthly_factors_single_code(x, func)
-        )
-        self.fac = (
-            self.fac.reset_index(level=1, drop=True)
-            .reset_index()
-            .set_index(["date", "code"])
-            .unstack()
-        )
-        self.fac.columns = [i[1] for i in list(self.fac.columns)]
-        self.fac = self.fac.resample("M").last()
-
-    @kk.desktop_sender(title="å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›")
-    def run(self, func: Callable, backsee: int = 20) -> None:
-        """æ‰§è¡Œè®¡ç®—çš„æ¡†æ¶ï¼Œäº§ç”Ÿå› å­å€¼
-
-        Parameters
-        ----------
-        func : Callable
-            æ¯ä¸ªæœˆè¦è¿›è¡Œçš„æ“ä½œ
-        backsee : int, optional
-            å›çœ‹æœŸï¼Œå³æ¯ä¸ªæœˆæœˆåº•å¯¹è¿‡å»å¤šå°‘å¤©è¿›è¡Œè®¡ç®—, by default 20
-        """
-        self.get_fac_long_and_tradedays()
-        self.get_month_starts_and_ends(backsee=backsee)
-        self.get_monthly_factor(func)
-
-
+@do_on_dfs
 def follow_tests(
     fac: pd.DataFrame,
+    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
+    index_member_value_weighted: bool = 1,
     comments_writer: pd.ExcelWriter = None,
     net_values_writer: pd.ExcelWriter = None,
     pos: bool = 0,
     neg: bool = 0,
     swindustry: bool = 0,
     zxindustry: bool = 0,
-    nums: list[int] = [3],
+    nums: List[int] = [3],
+    opens_average_first_day: bool = 0,
+    total_cap: bool = 0,
 ):
     """å› å­å®Œæˆå…¨Aæµ‹è¯•åï¼Œè¿›è¡Œçš„ä¸€äº›å¿…è¦çš„åç»­æµ‹è¯•ï¼ŒåŒ…æ‹¬å„ä¸ªåˆ†ç»„è¡¨ç°ã€ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–ã€3510çš„å¤šç©ºå’Œå¤šå¤´ã€å„ä¸ªè¡Œä¸šRank ICã€å„ä¸ªè¡Œä¸šä¹°3åªè¶…é¢è¡¨ç°
 
     Parameters
     ----------
     fac : pd.DataFrame
         è¦è¿›è¡Œåç»­æµ‹è¯•çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
+    trade_cost_double_side : float, optional
+        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
+    index_member_value_weighted : bool, optional
+        æˆåˆ†è‚¡å¤šå¤´é‡‡å–æµé€šå¸‚å€¼åŠ æƒ
     comments_writer : pd.ExcelWriter, optional
         å†™å…¥è¯„ä»·æŒ‡æ ‡çš„excel, by default None
     net_values_writer : pd.ExcelWriter, optional
         å†™å…¥å‡€å€¼åºåˆ—çš„excel, by default None
     pos : bool, optional
         å› å­çš„æ–¹å‘ä¸ºæ­£, by default 0
     neg : bool, optional
         å› å­çš„æ–¹å‘ä¸ºè´Ÿ, by default 0
     swindustry : bool, optional
         ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
     zxindustry : bool, optional
         ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
-    nums : list[int], optional
+    nums : List[int], optional
         å„ä¸ªè¡Œä¸šä¹°å‡ åªè‚¡ç¥¨, by default [3]
+    opens_average_first_day : bool, optional
+        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
+    total_cap : bool, optional
+        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Raises
     ------
     IOError
         å¦‚æœæœªæŒ‡å®šå› å­æ­£è´Ÿæ–¹å‘ï¼Œå°†æŠ¥é”™
     """
     if comments_writer is None:
-        from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
+        from pure_ocean_breeze.state.states import COMMENTS_WRITER
 
         comments_writer = COMMENTS_WRITER
     if net_values_writer is None:
-        from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
+        from pure_ocean_breeze.state.states import NET_VALUES_WRITER
 
         net_values_writer = NET_VALUES_WRITER
 
-    shen = pure_moonnight(fac)
-    shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
+    shen = pure_moonnight(
+        fac,
+        opens_average_first_day=opens_average_first_day,
+    )
+    if (
+        shen.shen.group_net_values.group1.iloc[-1]
+        > shen.shen.group_net_values.group10.iloc[-1]
+    ):
+        neg = 1
+    else:
+        pos = 1
+    if comments_writer is not None:
+        shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
+    print(shen.comments_ten())
     """ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–"""
     pure_fac = pure_snowtrain(fac)
-    pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
+    if comments_writer is not None:
+        pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
+    print(pure_fac.corr)
     shen = pure_moonnight(
         pure_fac(),
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="çº¯å‡€",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
     )
     """3510å¤šç©ºå’Œå¤šå¤´"""
     # 300
     fi300 = daily_factor_on300500(fac, hs300=1)
     shen = pure_moonnight(
         fi300,
+        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="300å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
     )
     if pos:
-        make_relative_comments(shen.shen.group_rets.group10, hs300=1).to_excel(
-            comments_writer, sheet_name="300è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1).to_excel(
-            net_values_writer, sheet_name="300è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group10, hs300=1).to_excel(
+                comments_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    hs300=1,
+                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group10, hs300=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1).to_excel(
+                net_values_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    hs300=1,
+                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1)
     elif neg:
-        make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
-            comments_writer, sheet_name="300è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
-            net_values_writer, sheet_name="300è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
+                comments_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    hs300=1,
+                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
+                net_values_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    hs300=1,
+                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
     else:
         raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
     # 500
     fi500 = daily_factor_on300500(fac, zz500=1)
     shen = pure_moonnight(
         fi500,
+        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="500å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
     )
     if pos:
-        make_relative_comments(shen.shen.group_rets.group10, zz500=1).to_excel(
-            comments_writer, sheet_name="500è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1).to_excel(
-            net_values_writer, sheet_name="500è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group10, zz500=1).to_excel(
+                comments_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    zz500=1,
+                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group10, zz500=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1).to_excel(
+                net_values_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    zz500=1,
+                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1)
     else:
-        make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
-            comments_writer, sheet_name="500è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
-            net_values_writer, sheet_name="500è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
+                comments_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz500=1,
+                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
+                net_values_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz500=1,
+                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
     # 1000
     fi1000 = daily_factor_on300500(fac, zz1000=1)
     shen = pure_moonnight(
         fi1000,
+        value_weighted=index_member_value_weighted,
         comments_writer=comments_writer,
         net_values_writer=net_values_writer,
         sheetname="1000å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
     )
     if pos:
-        make_relative_comments(shen.shen.group_rets.group10, zz1000=1).to_excel(
-            comments_writer, sheet_name="1000è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group10, zz1000=1).to_excel(
-            net_values_writer, sheet_name="1000è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group10, zz1000=1).to_excel(
+                comments_writer, sheet_name="1000è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    zz1000=1,
+                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group10, zz1000=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(
+                shen.shen.group_rets.group10, zz1000=1
+            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group10
+                    - shen.shen.factor_turnover_rates.group10 * i,
+                    zz1000=1,
+                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group10, zz1000=1)
     else:
-        make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
-            comments_writer, sheet_name="1000è¶…é¢"
-        )
-        make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
-            net_values_writer, sheet_name="1000è¶…é¢"
-        )
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
+                comments_writer, sheet_name="1000è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz1000=1,
+                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
+                net_values_writer, sheet_name="1000è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz1000=1,
+                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)
     # å„è¡Œä¸šRank IC
     rankics = rankic_test_on_industry(fac, comments_writer)
     # ä¹°3åªè¶…é¢è¡¨ç°
     rets = long_test_on_industry(
         fac, nums, pos=pos, neg=neg, swindustry=swindustry, zxindustry=zxindustry
     )
     logger.success("å› å­åç»­çš„å¿…è¦æµ‹è¯•å…¨éƒ¨å®Œæˆ")
 
 
+def test_on_index_four_out(
+    fac: pd.DataFrame,
+    value_weighted: bool = 1,
+    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
+    group_num: int = 10,
+    boxcox: bool = 1,
+    comments_writer: pd.ExcelWriter = None,
+    net_values_writer: pd.ExcelWriter = None,
+    opens_average_first_day: bool = 0,
+    total_cap: bool = 0,
+):
+    if comments_writer is None:
+        from pure_ocean_breeze.state.states import COMMENTS_WRITER
+
+        comments_writer = COMMENTS_WRITER
+    if net_values_writer is None:
+        from pure_ocean_breeze.state.states import NET_VALUES_WRITER
+
+        net_values_writer = NET_VALUES_WRITER
+
+    shen = pure_moonnight(
+        fac,
+        opens_average_first_day=opens_average_first_day,
+    )
+    if (
+        shen.shen.group_net_values.group1.iloc[-1]
+        > shen.shen.group_net_values10.iloc[-1]
+    ):
+        neg = 1
+        pos = 0
+    else:
+        pos = 1
+        neg = 0
+
+    """3510å¤šç©ºå’Œå¤šå¤´"""
+    # 300
+    fi300 = daily_factor_on300500(fac, hs300=1)
+    shen = pure_moonnight(
+        fi300,
+        value_weighted=value_weighted,
+        groups_num=group_num,
+        boxcox=boxcox,
+        comments_writer=comments_writer,
+        net_values_writer=net_values_writer,
+        sheetname="300å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
+    )
+    if pos:
+        if comments_writer is not None:
+            make_relative_comments(
+                shen.shen.group_rets[f"group{group_num}"], hs300=1
+            ).to_excel(comments_writer, sheet_name="300è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    hs300=1,
+                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], hs300=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], hs300=1
+            ).to_excel(net_values_writer, sheet_name="300è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    hs300=1,
+                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], hs300=1
+            )
+    elif neg:
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
+                comments_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    hs300=1,
+                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
+                net_values_writer, sheet_name="300è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    hs300=1,
+                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
+    else:
+        raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
+    # 500
+    fi500 = daily_factor_on300500(fac, zz500=1)
+    shen = pure_moonnight(
+        fi500,
+        value_weighted=value_weighted,
+        groups_num=group_num,
+        boxcox=boxcox,
+        comments_writer=comments_writer,
+        net_values_writer=net_values_writer,
+        sheetname="500å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
+    )
+    if pos:
+        if comments_writer is not None:
+            make_relative_comments(
+                shen.shen.group_rets[f"group{group_num}"], zz500=1
+            ).to_excel(comments_writer, sheet_name="500è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    zz500=1,
+                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz500=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], zz500=1
+            ).to_excel(net_values_writer, sheet_name="500è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    zz500=1,
+                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], zz500=1
+            )
+    else:
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
+                comments_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz500=1,
+                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
+                net_values_writer, sheet_name="500è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz500=1,
+                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
+    # 1000
+    fi1000 = daily_factor_on300500(fac, zz1000=1)
+    shen = pure_moonnight(
+        fi1000,
+        value_weighted=value_weighted,
+        groups_num=group_num,
+        boxcox=boxcox,
+        comments_writer=comments_writer,
+        net_values_writer=net_values_writer,
+        sheetname="1000å¤šç©º",
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
+    )
+    if pos:
+        if comments_writer is not None:
+            make_relative_comments(
+                shen.shen.group_rets[f"group{group_num}"], zz1000=1
+            ).to_excel(comments_writer, sheet_name="1000è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    zz1000=1,
+                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz1000=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], zz1000=1
+            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets[f"group{group_num}"]
+                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
+                    zz1000=1,
+                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(
+                shen.shen.group_rets[f"group{group_num}"], zz1000=1
+            )
+    else:
+        if comments_writer is not None:
+            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
+                comments_writer, sheet_name="1000è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz1000=1,
+                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
+        if net_values_writer is not None:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
+                net_values_writer, sheet_name="1000è¶…é¢"
+            )
+            for i in trade_cost_double_side_list:
+                make_relative_comments_plot(
+                    shen.shen.group_rets.group1
+                    - shen.shen.factor_turnover_rates.group1 * i,
+                    zz1000=1,
+                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
+        else:
+            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)
+
+
 class pure_helper(object):
     def __init__(
         self,
         df_main: pd.DataFrame,
         df_helper: pd.DataFrame,
         func: Callable = None,
         group: int = 10,
@@ -4205,30 +4491,31 @@
         return df
 
 
 class pure_fama(object):
     # @lru_cache(maxsize=None)
     def __init__(
         self,
-        factors: list[pd.DataFrame],
+        factors: List[pd.DataFrame],
         minus_group: Union[list, float] = 3,
         backsee: int = 20,
         rets: pd.DataFrame = None,
         value_weighted: bool = 1,
         add_market: bool = 1,
         add_market_series: pd.Series = None,
         factors_names: list = None,
         betas_rets: bool = 0,
+        total_cap: bool = 0,
     ) -> None:
         """ä½¿ç”¨famaä¸‰å› å­çš„æ–¹æ³•ï¼Œå°†ä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œæ‹†åˆ†å‡ºå„ä¸ªå› å­å¸¦æ¥çš„æ”¶ç›Šç‡ä»¥åŠç‰¹è´¨çš„æ”¶ç›Šç‡
         åˆ†åˆ«è®¡ç®—æ¯ä¸€æœŸï¼Œå„ä¸ªå› å­æ”¶ç›Šç‡çš„å€¼ï¼Œè¶…é¢æ”¶ç›Šç‡ï¼Œå› å­çš„æš´éœ²ï¼Œä»¥åŠç‰¹è´¨æ”¶ç›Šç‡
 
         Parameters
         ----------
-        factors : list[pd.DataFrame]
+        factors : List[pd.DataFrame]
             ç”¨äºè§£é‡Šæ”¶ç›Šçš„å„ä¸ªå› å­å€¼ï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„dataframe
         minus_group : Union[list, float], optional
             æ¯ä¸€ä¸ªå› å­å°†æˆªé¢ä¸Šçš„è‚¡ç¥¨åˆ†ä¸ºå‡ ç»„, by default 3
         backsee : int, optional
             åšæ—¶åºå›å½’æ—¶ï¼Œå›çœ‹çš„å¤©æ•°, by default 20
         rets : pd.DataFrame, optional
             æ¯åªä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ”¶ç›Šç‡ï¼Œé»˜è®¤ä½¿ç”¨å½“æ—¥æ—¥é—´æ”¶ç›Šç‡, by default None
@@ -4238,14 +4525,16 @@
             æ˜¯å¦åŠ å…¥å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œé»˜è®¤ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ¯æ—¥æ—¥é—´æ”¶ç›Šç‡, by default 1
         add_market_series : bool, optional
             åŠ å…¥çš„å¸‚åœºæ”¶ç›Šç‡çš„æ•°æ®ï¼Œå¦‚æœæ²¡æŒ‡å®šï¼Œåˆ™ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ—¥é—´æ”¶ç›Šç‡, by default None
         factors_names : list, optional
             å„ä¸ªå› å­çš„åå­—ï¼Œé»˜è®¤ä¸ºfac0(å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»fac1å¼€å§‹),fac1,fac2,fac3, by default None
         betas_rets : bool, optional
             æ˜¯å¦è®¡ç®—æ¯åªä¸ªè‚¡çš„ç”±äºæš´éœ²åœ¨æ¯ä¸ªå› å­ä¸Šæ‰€å¸¦æ¥çš„æ”¶ç›Šç‡, by default 0
+        total_cap : bool, optional
+            åŠ æƒæ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
         """
         start = max(
             [int(datetime.datetime.strftime(i.index.min(), "%Y%m%d")) for i in factors]
         )
         self.backsee = backsee
         self.factors = factors
         self.factors_names = factors_names
@@ -4261,14 +4550,16 @@
         ]
         self.factors_group_long = [(i == 0) + 0 for i in self.factors_group]
         self.factors_group_short = [
             (i == (j - 1)) + 0 for i, j in zip(self.factors_group, self.minus_group)
         ]
         self.value_weighted = value_weighted
         if value_weighted:
+            if total_cap:
+                self.cap = read_daily(total_cap=1, start=start)
             self.cap = read_daily(flow_cap=1, start=start)
             self.factors_group_long = [self.cap * i for i in self.factors_group_long]
             self.factors_group_short = [self.cap * i for i in self.factors_group_short]
             self.factors_group_long = [
                 (i.T / i.T.sum()).T for i in self.factors_group_long
             ]
             self.factors_group_short = [
@@ -4305,18 +4596,15 @@
             else:
                 closes = add_market_series.to_frame("fac0")
             rets = closes / closes.shift(1) - 1
             self.__factors_rets = pd.concat([rets, self.__factors_rets], axis=1)
             if factors_names is not None:
                 factors_names = ["å¸‚åœº"] + factors_names
         self.__data = self.make_df(self.rets, self.__factors_rets)
-        if is_notebook():
-            tqdm.tqdm_notebook().pandas()
-        else:
-            tqdm.tqdm.pandas()
+        tqdm.auto.tqdm.pandas()
         self.__coefficients = (
             self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
         )
         self.__coefficients = self.__coefficients.rename(
             columns={
                 i: "co" + i for i in list(self.__coefficients.columns) if "fac" in i
             }
@@ -4429,29 +4717,29 @@
             ...
 
 
 class pure_rollingols(object):
     def __init__(
         self,
         y: pd.DataFrame,
-        xs: Union[list[pd.DataFrame], pd.DataFrame],
+        xs: Union[List[pd.DataFrame], pd.DataFrame],
         backsee: int = 20,
-        factors_names: list[str] = None,
+        factors_names: List[str] = None,
     ) -> None:
         """ä½¿ç”¨è‹¥å¹²ä¸ªdataframeï¼Œå¯¹åº”çš„è‚¡ç¥¨è¿›è¡ŒæŒ‡å®šçª—å£çš„æ—¶åºæ»šåŠ¨å›å½’
 
         Parameters
         ----------
         y : pd.DataFrame
             æ»šåŠ¨å›å½’ä¸­çš„å› å˜é‡yï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
-        xs : Union[list[pd.DataFrame], pd.DataFrame]
+        xs : Union[List[pd.DataFrame], pd.DataFrame]
             æ»šåŠ¨å›å½’ä¸­çš„è‡ªå˜é‡xiï¼Œæ¯ä¸€ä¸ªdataframeï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
         backsee : int, optional
             æ»šåŠ¨å›å½’çš„æ—¶é—´çª—å£, by default 20
-        factors_names : list[str], optional
+        factors_names : List[str], optional
             xsä¸­ï¼Œæ¯ä¸ªå› å­çš„åå­—, by default None
         """
         self.backsee = backsee
         self.y = y
         if not isinstance(xs, list):
             xs = [xs]
         self.xs = xs
@@ -4465,18 +4753,15 @@
             for j, i in enumerate(xs)
         ]
         xs = [y] + xs
         xs = reduce(lambda x, y: pd.merge(x, y, on=["date", "code"]), xs)
         xs = xs.set_index("date")
         self.__data = xs
         self.haha = xs
-        if is_notebook():
-            tqdm.tqdm_notebook().pandas()
-        else:
-            tqdm.tqdm.pandas()
+        tqdm.auto.tqdm.pandas()
         self.__coefficients = (
             self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
         )
         self.__coefficients = self.__coefficients.rename(
             columns={i: "co" + i for i in list(self.__coefficients.columns) if "x" in i}
         )
         self.__data = pd.merge(
@@ -4497,18 +4782,18 @@
         self.__alphas = self.__data.pivot(
             index="date", columns="code", values="intercept"
         )
         if factors_names is None:
             self.__betas = {
                 i: self.__data.pivot(index="date", columns="code", values=i)
                 for i in list(self.__data.columns)
-                if i.startswith("x")
+                if i.startswith("cox")
             }
         else:
-            facs = [i for i in list(self.__data.columns) if i.startswith("x")]
+            facs = [i for i in list(self.__data.columns) if i.startswith("cox")]
             self.__betas = {
                 factors_names[num]: self.__data.pivot(
                     index="date", columns="code", values=i
                 )
                 for num, i in enumerate(facs)
             }
         if len(list(self.__betas)) == 1:
@@ -4546,51 +4831,77 @@
             return pd.concat([alpha, betas], axis=1)
 
         except Exception:
             # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
             ...
 
 
+@do_on_dfs
 def test_on_300500(
     df: pd.DataFrame,
+    trade_cost_double_side: float = 0,
+    group_num: int = 10,
+    value_weighted: bool = 1,
+    boxcox: bool = 0,
     hs300: bool = 0,
     zz500: bool = 0,
     zz1000: bool = 0,
     gz2000: bool = 0,
     iplot: bool = 1,
+    opens_average_first_day: bool = 0,
+    total_cap: bool = 0,
 ) -> pd.Series:
     """å¯¹å› å­åœ¨æŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´æµ‹è¯•
 
     Parameters
     ----------
     df : pd.DataFrame
         å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
+    trade_cost_double_side : float, optional
+        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
+    group_num : int
+        åˆ†ç»„æ•°é‡, by default 10
+    value_weighted : bool
+        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
     hs300 : bool, optional
         åœ¨æ²ªæ·±300æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     zz500 : bool, optional
         åœ¨ä¸­è¯500æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     zz1000 : bool, optional
         åœ¨ä¸­è¯1000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
     gz1000 : bool, optional
         åœ¨å›½è¯2000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
-    iplot : bol,optional
+    iplot : bo0l,optional
         å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
+    opens_average_first_day : bool, optional
+        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
+    total_cap : bool, optional
+        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Returns
     -------
     pd.Series
         å¤šå¤´ç»„åœ¨è¯¥æŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
     """
     fi300 = daily_factor_on300500(
         df, hs300=hs300, zz500=zz500, zz1000=zz1000, gz2000=gz2000
     )
-    shen = pure_moonnight(fi300, iplot=iplot)
+    shen = pure_moonnight(
+        fi300,
+        value_weighted=value_weighted,
+        groups_num=group_num,
+        trade_cost_double_side=trade_cost_double_side,
+        boxcox=boxcox,
+        iplot=iplot,
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
+    )
     if (
         shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values.group10.iloc[-1]
+        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
     ):
         print(
             make_relative_comments(
                 shen.shen.group_rets.group1,
                 hs300=hs300,
                 zz500=zz500,
                 zz1000=zz1000,
@@ -4604,96 +4915,177 @@
             zz1000=zz1000,
             gz2000=gz2000,
         )
         return abrets
     else:
         print(
             make_relative_comments(
-                shen.shen.group_rets.group10,
+                shen.shen.group_rets[f"group{group_num}"],
                 hs300=hs300,
                 zz500=zz500,
                 zz1000=zz1000,
                 gz2000=gz2000,
             )
         )
         abrets = make_relative_comments_plot(
-            shen.shen.group_rets.group10,
+            shen.shen.group_rets[f"group{group_num}"],
             hs300=hs300,
             zz500=zz500,
             zz1000=zz1000,
             gz2000=gz2000,
         )
         return abrets
 
 
+@do_on_dfs
 def test_on_index_four(
-    df: pd.DataFrame, iplot: bool = 1, gz2000: bool = 0, boxcox: bool = 1
+    df: pd.DataFrame,
+    value_weighted: bool = 1,
+    group_num: int = 10,
+    trade_cost_double_side: float = 0,
+    iplot: bool = 1,
+    gz2000: bool = 0,
+    boxcox: bool = 1,
+    opens_average_first_day: bool = 0,
+    total_cap: bool = 0,
 ) -> pd.DataFrame:
     """å¯¹å› å­åŒæ—¶åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000è¿™4ä¸ªæŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´è¶…é¢æµ‹è¯•
 
     Parameters
     ----------
     df : pd.DataFrame
         å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
+    value_weighted : bool
+        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
+    group_num : int
+        åˆ†ç»„æ•°é‡, by default 10
+    trade_cost_double_side : float, optional
+        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
     iplot : bol,optional
         å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
     gz2000 : bool, optional
         æ˜¯å¦è¿›è¡Œå›½è¯2000ä¸Šçš„æµ‹è¯•, by default 0
     boxcox : bool, optional
         æ˜¯å¦è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†, by default 1
+    opens_average_first_day : bool, optional
+        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
+    total_cap : bool, optional
+        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
 
     Returns
     -------
     pd.DataFrame
         å¤šå¤´ç»„åœ¨å„ä¸ªæŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
     """
     fi300 = daily_factor_on300500(df, hs300=1)
-    shen = pure_moonnight(fi300, iplot=iplot, boxcox=boxcox)
+    shen = pure_moonnight(
+        fi300,
+        groups_num=group_num,
+        value_weighted=value_weighted,
+        trade_cost_double_side=trade_cost_double_side,
+        iplot=iplot,
+        boxcox=boxcox,
+        opens_average_first_day=opens_average_first_day,
+        total_cap=total_cap,
+    )
     if (
         shen.shen.group_net_values.group1.iloc[-1]
-        > shen.shen.group_net_values.group10.iloc[-1]
+        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
     ):
         com300, net300 = make_relative_comments(
             shen.shen.group_rets.group1, hs300=1, show_nets=1
         )
         fi500 = daily_factor_on300500(df, zz500=1)
-        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
+        shen = pure_moonnight(
+            fi500,
+            groups_num=group_num,
+            value_weighted=value_weighted,
+            trade_cost_double_side=trade_cost_double_side,
+            iplot=iplot,
+            boxcox=boxcox,
+            opens_average_first_day=opens_average_first_day,
+            total_cap=total_cap,
+        )
         com500, net500 = make_relative_comments(
             shen.shen.group_rets.group1, zz500=1, show_nets=1
         )
         fi1000 = daily_factor_on300500(df, zz1000=1)
-        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
+        shen = pure_moonnight(
+            fi1000,
+            groups_num=group_num,
+            value_weighted=value_weighted,
+            trade_cost_double_side=trade_cost_double_side,
+            iplot=iplot,
+            boxcox=boxcox,
+            opens_average_first_day=opens_average_first_day,
+            total_cap=total_cap,
+        )
         com1000, net1000 = make_relative_comments(
             shen.shen.group_rets.group1, zz1000=1, show_nets=1
         )
         if gz2000:
             fi2000 = daily_factor_on300500(df, gz2000=1)
-            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
+            shen = pure_moonnight(
+                fi2000,
+                groups_num=group_num,
+                trade_cost_double_side=trade_cost_double_side,
+                iplot=iplot,
+                boxcox=boxcox,
+                opens_average_first_day=opens_average_first_day,
+                total_cap=total_cap,
+            )
             com2000, net2000 = make_relative_comments(
                 shen.shen.group_rets.group1, gz2000=1, show_nets=1
             )
     else:
         com300, net300 = make_relative_comments(
-            shen.shen.group_rets.group10, hs300=1, show_nets=1
+            shen.shen.group_rets[f"group{group_num}"], hs300=1, show_nets=1
         )
         fi500 = daily_factor_on300500(df, zz500=1)
-        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
+        shen = pure_moonnight(
+            fi500,
+            groups_num=group_num,
+            value_weighted=value_weighted,
+            trade_cost_double_side=trade_cost_double_side,
+            iplot=iplot,
+            boxcox=boxcox,
+            opens_average_first_day=opens_average_first_day,
+            total_cap=total_cap,
+        )
         com500, net500 = make_relative_comments(
-            shen.shen.group_rets.group10, zz500=1, show_nets=1
+            shen.shen.group_rets[f"group{group_num}"], zz500=1, show_nets=1
         )
         fi1000 = daily_factor_on300500(df, zz1000=1)
-        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
+        shen = pure_moonnight(
+            fi1000,
+            groups_num=group_num,
+            value_weighted=value_weighted,
+            trade_cost_double_side=trade_cost_double_side,
+            iplot=iplot,
+            boxcox=boxcox,
+            opens_average_first_day=opens_average_first_day,
+            total_cap=total_cap,
+        )
         com1000, net1000 = make_relative_comments(
-            shen.shen.group_rets.group10, zz1000=1, show_nets=1
+            shen.shen.group_rets[f"group{group_num}"], zz1000=1, show_nets=1
         )
         if gz2000:
             fi2000 = daily_factor_on300500(df, gz2000=1)
-            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
+            shen = pure_moonnight(
+                fi2000,
+                groups_num=group_num,
+                value_weighted=value_weighted,
+                trade_cost_double_side=trade_cost_double_side,
+                iplot=iplot,
+                boxcox=boxcox,
+                opens_average_first_day=opens_average_first_day,
+                total_cap=total_cap,
+            )
             com2000, net2000 = make_relative_comments(
-                shen.shen.group_rets.group10, gz2000=1, show_nets=1
+                shen.shen.group_rets[f"group{group_num}"], gz2000=1, show_nets=1
             )
     com300 = com300.to_frame("300è¶…é¢")
     com500 = com500.to_frame("500è¶…é¢")
     com1000 = com1000.to_frame("1000è¶…é¢")
     if gz2000:
         com2000 = com2000.to_frame("2000è¶…é¢")
         coms = pd.concat([com300, com500, com1000, com2000], axis=1)
@@ -4733,15 +5125,15 @@
                 specs=[
                     [
                         None,
                         {"rowspan": 2, "colspan": 4},
                         None,
                         None,
                         None,
-                        {"rowspan": 2, "colspan": 5},
+                        {"rowspan": 2, "colspan": 3},
                         None,
                         None,
                         None,
                         None,
                     ],
                     [
                         None,
@@ -4767,15 +5159,15 @@
                 shared_yaxes=False,
                 specs=[
                     [
                         None,
                         {"rowspan": 2, "colspan": 3},
                         None,
                         None,
-                        {"rowspan": 2, "colspan": 6},
+                        {"rowspan": 2, "colspan": 3},
                         None,
                         None,
                         None,
                         None,
                         None,
                     ],
                     [
@@ -4801,24 +5193,26 @@
         tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
         tb.set_cols_dtype(["f"] * 7)
         tb.header(list(coms.T.reset_index().columns))
         tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
         print(tb.draw())
 
 
+@do_on_dfs
 class pure_star(object):
     def __init__(
         self,
         fac: pd.Series,
         code: str = None,
         price_opens: pd.Series = None,
         iplot: bool = 1,
         comments_writer: pd.ExcelWriter = None,
         net_values_writer: pd.ExcelWriter = None,
         sheetname: str = None,
+        questdb_host: str = "127.0.0.1",
     ):
         """æ‹©æ—¶å›æµ‹æ¡†æ¶ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æˆ–ä¿¡å·å€¼ï¼Œä¾æ®ä¿¡å·ä¹°å…¥å¯¹åº”çš„è‚¡ç¥¨æˆ–æŒ‡æ•°ï¼Œå¹¶è€ƒå¯Ÿç»å¯¹æ”¶ç›Šã€è¶…é¢æ”¶ç›Šå’ŒåŸºå‡†æ”¶ç›Š
         å›æµ‹æ–¹å¼ä¸ºï¼Œtæ—¥æ”¶ç›˜æ—¶è·å¾—ä¿¡å·ï¼Œt+1æ—¥å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·ä¹°å…¥ï¼Œt+2å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·å–å‡º
 
         Parameters
         ----------
         fac : pd.Series
@@ -4831,27 +5225,29 @@
             ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»©æ•ˆå’Œèµ°åŠ¿å›¾, by default 1
         comments_writer : pd.ExcelWriter, optional
             ç»©æ•ˆè¯„ä»·çš„å­˜å‚¨æ–‡ä»¶, by default None
         net_values_writer : pd.ExcelWriter, optional
             å‡€å€¼åºåˆ—çš„å­˜å‚¨æ–‡ä»¶, by default None
         sheetname : str, optional
             å­˜å‚¨æ–‡ä»¶çš„å·¥ä½œè¡¨çš„åå­—, by default None
+        questdb_host: str, optional
+            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
         """
         if code is not None:
             x1 = code.split(".")[0]
             x2 = code.split(".")[1]
             if (x1[0] == "0" or x1[:2] == "30") and x2 == "SZ":
                 kind = "stock"
             elif x1[0] == "6" and x2 == "SH":
                 kind = "stock"
             else:
                 kind = "index"
             self.kind = kind
             if kind == "index":
-                qdb = Questdb()
+                qdb = Questdb(host=questdb_host)
                 price_opens = qdb.get_data(
                     f"select date,num,close from minute_data_{kind} where code='{code}'"
                 )
                 price_opens = price_opens[price_opens.num == "1"]
                 price_opens = price_opens.set_index("date").close
                 price_opens.index = pd.to_datetime(price_opens.index, format="%Y%m%d")
             else:
@@ -4884,20 +5280,20 @@
         self.total_comments.columns = (
             self.total_nets.columns
         ) = self.total_rets.columns = ["å› å­ç»å¯¹", "å› å­è¶…é¢", "ä¹°å…¥æŒæœ‰"]
         self.total_comments = np.around(self.total_comments, 3)
         self.iplot = iplot
         self.plot()
         if comments_writer is None and sheetname is not None:
-            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER
+            from pure_ocean_breeze.state.states import COMMENTS_WRITER
 
             comments_writer = COMMENTS_WRITER
             self.total_comments.to_excel(comments_writer, sheet_name=sheetname)
         if net_values_writer is None and sheetname is not None:
-            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER
+            from pure_ocean_breeze.state.states import NET_VALUES_WRITER
 
             net_values_writer = NET_VALUES_WRITER
             self.total_nets.to_excel(net_values_writer, sheet_name=sheetname)
 
     def plot(self):
         coms = self.total_comments.copy().reset_index()
         if self.iplot:
@@ -4956,7 +5352,1373 @@
             plt.show()
             tb = Texttable()
             tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
             tb.set_cols_dtype(["f"] * 7)
             tb.header(list(coms.T.reset_index().columns))
             tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
             print(tb.draw())
+
+
+@do_on_dfs
+def get_group(df: pd.DataFrame, group_num: int = 10) -> pd.DataFrame:
+    """ä½¿ç”¨groupbyçš„æ–¹æ³•ï¼Œå°†ä¸€ç»„å› å­å€¼æ”¹ä¸ºæˆªé¢ä¸Šçš„åˆ†ç»„å€¼ï¼Œæ­¤æ–¹æ³•ç›¸æ¯”qcutçš„æ–¹æ³•æ›´åŠ ç¨³å¥ï¼Œä½†é€Ÿåº¦æ›´æ…¢ä¸€äº›
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
+    group_num : int, optional
+        åˆ†ç»„çš„æ•°é‡, by default 10
+
+    Returns
+    -------
+    pd.DataFrame
+        è½¬åŒ–ä¸ºåˆ†ç»„å€¼åçš„dfï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºåˆ†ç»„å€¼
+    """
+    a = pure_moon(no_read_indu=1)
+    df = df.stack().reset_index()
+    df.columns = ["date", "code", "fac"]
+    df = a.get_groups(df, group_num).pivot(index="date", columns="code", values="group")
+    return df
+
+
+class pure_linprog(object):
+    def __init__(
+        self,
+        facs: pd.DataFrame,
+        total_caps: pd.DataFrame = None,
+        indu_dummys: pd.DataFrame = None,
+        index_weights_hs300: pd.DataFrame = None,
+        index_weights_zz500: pd.DataFrame = None,
+        index_weights_zz1000: pd.DataFrame = None,
+        opens: pd.DataFrame = None,
+        closes: pd.DataFrame = None,
+        hs300_closes: pd.DataFrame = None,
+        zz500_closes: pd.DataFrame = None,
+        zz1000_closes: pd.DataFrame = None,
+    ) -> None:
+        """çº¿æ€§è§„åˆ’æ±‚è§£ï¼Œç›®æ ‡ä¸ºé¢„æœŸæ”¶ç›Šç‡æœ€å¤§ï¼ˆå³å› å­æ–¹å‘ä¸ºè´Ÿæ—¶ï¼Œç»„åˆå› å­å€¼æœ€å°ï¼‰
+        æ¡ä»¶ä¸ºï¼Œä¸¥æ ¼æ§åˆ¶å¸‚å€¼ä¸­æ€§ï¼ˆæ•°æ®ï¼šæ€»å¸‚å€¼çš„å¯¹æ•°ï¼›å«ä¹‰ï¼šç»„åˆåœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ä¸æŒ‡æ•°åœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ç›¸ç­‰ï¼‰
+        ä¸¥æ ¼æ§åˆ¶è¡Œä¸šä¸­æ€§ï¼ˆæ•°æ®ï¼šä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼‰ï¼Œä¸ªè‚¡åç¦»åœ¨1%ä»¥å†…ï¼Œæˆåˆ†è‚¡æƒé‡ä¹‹å’Œåœ¨80%ä»¥ä¸Š
+        åˆ†åˆ«åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä¸Šä¼˜åŒ–æ±‚è§£
+
+        Parameters
+        ----------
+        facs : pd.DataFrame
+            å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
+        total_caps : pd.DataFrame, optional
+            æ€»å¸‚å€¼æ•°æ®ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ€»å¸‚å€¼, by default None
+        indu_dummys : pd.DataFrame, optional
+            è¡Œä¸šå“‘å˜é‡ï¼ŒåŒ…å«ä¸¤åˆ—åä¸ºdateçš„æ—¶é—´å’Œcodeçš„è‚¡ç¥¨ä»£ç ï¼Œä»¥åŠ30+åˆ—è¡Œä¸šå“‘å˜é‡, by default None
+        index_weights_hs300 : pd.DataFrame, optional
+            æ²ªæ·±300æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
+        index_weights_zz500 : pd.DataFrame, optional
+            ä¸­è¯500æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
+        index_weights_zz1000 : pd.DataFrame, optional
+            ä¸­è¯1000æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
+        opens : pd.DataFrame, optional
+            æ¯æœˆæœˆåˆå¼€ç›˜ä»·æ•°æ®, by default None
+        closes : pd.DataFrame, optional
+            æ¯æœˆæœˆæœ«æ”¶ç›˜ä»·æ•°æ®, by default None
+        hs300_closes : pd.DataFrame, optional
+            æ²ªæ·±300æ¯æœˆæ”¶ç›˜ä»·æ•°æ®, by default None
+        zz500_closes : pd.DataFrame, optional
+            ä¸­è¯500æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
+        zz1000_closes : pd.DataFrame, optional
+            ä¸­è¯1000æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
+        """
+        self.facs = facs.resample("M").last()
+        if total_caps is None:
+            total_caps = standardlize(
+                np.log(read_daily(total_cap=1).resample("M").last())
+            )
+        if indu_dummys is None:
+            indu_dummys = read_daily(zxindustry_dummy_code=1)
+        if index_weights_hs300 is None:
+            index_weights_hs300 = read_daily(hs300_member_weight=1)
+        if index_weights_zz500 is None:
+            index_weights_zz500 = read_daily(zz500_member_weight=1)
+        if index_weights_zz1000 is None:
+            index_weights_zz1000 = read_daily(zz1000_member_weight=1)
+        if opens is None:
+            opens = read_daily(open=1).resample("M").first()
+        if closes is None:
+            closes = read_daily(close=1).resample("M").last()
+        if hs300_closes is None:
+            hs300_closes = read_index_single("000300.SH").resample("M").last()
+        if zz500_closes is None:
+            zz500_closes = read_index_single("000905.SH").resample("M").last()
+        if zz1000_closes is None:
+            zz1000_closes = read_index_single("000852.SH").resample("M").last()
+        self.total_caps = total_caps
+        self.indu_dummys = indu_dummys
+        self.index_weights_hs300 = index_weights_hs300
+        self.index_weights_zz500 = index_weights_zz500
+        self.index_weights_zz1000 = index_weights_zz1000
+        self.hs300_weights = []
+        self.zz500_weights = []
+        self.zz1000_weights = []
+        self.ret_next = closes / opens - 1
+        self.ret_hs300 = hs300_closes.pct_change()
+        self.ret_zz500 = zz500_closes.pct_change()
+        self.ret_zz1000 = zz1000_closes.pct_change()
+
+    def optimize_one_day(
+        self,
+        fac: pd.DataFrame,
+        flow_cap: pd.DataFrame,
+        indu_dummy: pd.DataFrame,
+        index_weight: pd.DataFrame,
+        name: str,
+    ) -> pd.DataFrame:
+        """ä¼˜åŒ–å•æœŸæ±‚è§£
+
+        Parameters
+        ----------
+        fac : pd.DataFrame
+            å•æœŸå› å­å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºå› å­å€¼
+        flow_cap : pd.DataFrame
+            æµé€šå¸‚å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæˆªé¢æ ‡å‡†åŒ–çš„æµé€šå¸‚å€¼
+        indu_dummy : pd.DataFrame
+            è¡Œä¸šå“‘å˜é‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºè¡Œä¸šä»£ç ï¼Œvaluesä¸ºå“‘å˜é‡
+        index_weight : pd.DataFrame
+            æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæƒé‡
+
+        Returns
+        -------
+        pd.DataFrame
+            å½“æœŸæœ€ä½³æƒé‡
+        """
+        if fac.shape[0] > 0 and index_weight.shape[1] > 0:
+            date = fac.columns.tolist()[0]
+            codes = list(
+                set(fac.index)
+                | set(flow_cap.index)
+                | set(indu_dummy.index)
+                | set(index_weight.index)
+            )
+            fac, flow_cap, indu_dummy, index_weight = list(
+                map(
+                    lambda x: x.reindex(codes).fillna(0).to_numpy(),
+                    [fac, flow_cap, indu_dummy, index_weight],
+                )
+            )
+            sign_index_weight = np.sign(index_weight)
+            # ä¸ªè‚¡æƒé‡å¤§äºé›¶ã€åç¦»1%
+            bounds = list(
+                zip(
+                    select_max(index_weight - 0.01, 0).flatten(),
+                    select_min(index_weight + 0.01, 1).flatten(),
+                )
+            )
+            # å¸‚å€¼ä¸­æ€§+è¡Œä¸šä¸­æ€§+æƒé‡å’Œä¸º1
+            huge = np.vstack([flow_cap.T, indu_dummy.T, np.array([1] * len(codes))])
+            target = (
+                list(flow_cap.T @ index_weight.flatten())
+                + list((indu_dummy.T @ index_weight).flatten())
+                + [np.sum(index_weight)]
+            )
+            # å†™çº¿æ€§æ¡ä»¶
+            c = fac.T.flatten().tolist()
+            a = sign_index_weight.reshape((1, -1)).tolist()
+            b = [0.8]
+            # ä¼˜åŒ–æ±‚è§£
+            res = linprog(c, a, b, huge, target, bounds)
+            if res.success:
+                return pd.DataFrame({date: res.x.tolist()}, index=codes)
+            else:
+                # raise NotImplementedError(f"{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
+                logger.warning(f"{name}åœ¨{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
+                return None
+        else:
+            return None
+
+    def optimize_many_days(self, startdate: int = 20130101):
+        dates = [i for i in self.facs.index if i >= pd.Timestamp(str(startdate))]
+        for date in tqdm.auto.tqdm(dates):
+            fac = self.facs[self.facs.index == date].T.dropna()
+            total_cap = self.total_caps[self.total_caps.index == date].T.dropna()
+            indu_dummy = self.indu_dummys[self.indu_dummys.date <= date]
+            indu_dummy = (
+                indu_dummy[indu_dummy.date == indu_dummy.date.max()]
+                .drop(columns=["date"])
+                .set_index("code")
+            )
+            index_weight_hs300 = self.index_weights_hs300[
+                self.index_weights_hs300.index == date
+            ].T.dropna()
+            index_weight_zz500 = self.index_weights_zz500[
+                self.index_weights_zz500.index == date
+            ].T.dropna()
+            index_weight_zz1000 = self.index_weights_zz1000[
+                self.index_weights_zz1000.index == date
+            ].T.dropna()
+            weight_hs300 = self.optimize_one_day(
+                fac, total_cap, indu_dummy, index_weight_hs300, "hs300"
+            )
+            weight_zz500 = self.optimize_one_day(
+                fac, total_cap, indu_dummy, index_weight_zz500, "zz500"
+            )
+            weight_zz1000 = self.optimize_one_day(
+                fac, total_cap, indu_dummy, index_weight_zz1000, "zz1000"
+            )
+            self.hs300_weights.append(weight_hs300)
+            self.zz500_weights.append(weight_zz500)
+            self.zz1000_weights.append(weight_zz1000)
+        self.hs300_weights = pd.concat(self.hs300_weights, axis=1).T
+        self.zz500_weights = pd.concat(self.zz500_weights, axis=1).T
+        self.zz1000_weights = pd.concat(self.zz1000_weights, axis=1).T
+
+    def make_contrast(self, weight, index, name) -> list[pd.DataFrame]:
+        ret = (weight.shift(1) * self.ret_next).sum(axis=1)
+        abret = ret - index
+        rets = pd.concat([ret, index, abret], axis=1).dropna()
+        rets.columns = [f"{name}å¢å¼ºç»„åˆå‡€å€¼", f"{name}æŒ‡æ•°å‡€å€¼", f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"]
+        rets = (rets + 1).cumprod()
+        rets = rets.apply(lambda x: x / x.iloc[0])
+        comments = comments_on_twins(rets[f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"], abret.dropna())
+        return comments, rets
+
+    def run(self, startdate: int = 20130101) -> pd.DataFrame:
+        """è¿è¡Œè§„åˆ’æ±‚è§£
+
+        Parameters
+        ----------
+        startdate : int, optional
+            èµ·å§‹æ—¥æœŸ, by default 20130101
+
+        Returns
+        -------
+        pd.DataFrame
+            è¶…é¢ç»©æ•ˆæŒ‡æ ‡
+        """
+        self.optimize_many_days(startdate=startdate)
+        self.hs300_comments, self.hs300_nets = self.make_contrast(
+            self.hs300_weights, self.ret_hs300, "æ²ªæ·±300"
+        )
+        self.zz500_comments, self.zz500_nets = self.make_contrast(
+            self.zz500_weights, self.ret_zz500, "ä¸­è¯500"
+        )
+        self.zz1000_comments, self.zz1000_nets = self.make_contrast(
+            self.zz1000_weights, self.ret_zz1000, "ä¸­è¯1000"
+        )
+
+        figs = cf.figures(
+            pd.concat([self.hs300_nets, self.zz500_nets, self.zz1000_nets]),
+            [
+                dict(kind="line", y=list(self.hs300_nets.columns)),
+                dict(kind="line", y=list(self.zz500_nets.columns)),
+                dict(kind="line", y=list(self.zz1000_nets.columns)),
+            ],
+            asList=True,
+        )
+        base_layout = cf.tools.get_base_layout(figs)
+
+        sp = cf.subplots(
+            figs,
+            shape=(1, 3),
+            base_layout=base_layout,
+            vertical_spacing=0.15,
+            horizontal_spacing=0.03,
+            shared_yaxes=False,
+            subplot_titles=["æ²ªæ·±300å¢å¼º", "ä¸­è¯500å¢å¼º", "ä¸­è¯1000å¢å¼º"],
+        )
+        sp["layout"].update(showlegend=True)
+        cf.iplot(sp)
+
+        self.comments = pd.concat(
+            [self.hs300_comments, self.zz500_comments, self.zz1000_comments], axis=1
+        )
+        self.comments.columns = ["æ²ªæ·±300è¶…é¢", "ä¸­è¯500è¶…é¢", "ä¸­è¯1000è¶…é¢"]
+
+        from pure_ocean_breeze.state.states import COMMENTS_WRITER, NET_VALUES_WRITER
+
+        comments_writer = COMMENTS_WRITER
+        net_values_writer = NET_VALUES_WRITER
+        if comments_writer is not None:
+            self.hs300_comments.to_excel(comments_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
+            self.zz500_comments.to_excel(comments_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
+            self.zz1000_comments.to_excel(comments_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
+        if net_values_writer is not None:
+            self.hs300_nets.to_excel(net_values_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–å‡€å€¼")
+            self.zz500_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–å‡€å€¼")
+            self.zz1000_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–å‡€å€¼")
+
+        return self.comments.T
+
+
+def symmetrically_orthogonalize(dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:
+    """å¯¹å¤šä¸ªå› å­åšå¯¹ç§°æ­£äº¤ï¼Œæ¯ä¸ªå› å­å¾—åˆ°æ­£äº¤å…¶ä»–å› å­åçš„ç»“æœ
+
+    Parameters
+    ----------
+    dfs : list[pd.DataFrame]
+        å¤šä¸ªè¦åšæ­£äº¤çš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
+
+    Returns
+    -------
+    list[pd.DataFrame]
+        å¯¹ç§°æ­£äº¤åçš„å„ä¸ªå› å­
+    """
+
+    def sing(dfs: list[pd.DataFrame], date: pd.Timestamp):
+        dds = []
+        for num, i in enumerate(dfs):
+            i = i[i.index == date]
+            i.index = [f"fac{num}"]
+            i = i.T
+            dds.append(i)
+        dds = pd.concat(dds, axis=1)
+        cov = dds.cov()
+        d, u = np.linalg.eig(cov)
+        d = np.diag(d ** (-0.5))
+        new_facs = pd.DataFrame(
+            np.dot(dds, np.dot(np.dot(u, d), u.T)), columns=dds.columns, index=dds.index
+        )
+        new_facs = new_facs.stack().reset_index()
+        new_facs.columns = ["code", "fac_number", "fac"]
+        new_facs = new_facs.assign(date=date)
+        dds = []
+        for num, i in enumerate(dfs):
+            i = new_facs[new_facs.fac_number == f"fac{num}"]
+            i = i.pivot(index="date", columns="code", values="fac")
+            dds.append(i)
+        return dds
+
+    dfs = [standardlize(i) for i in dfs]
+    date_first = max([i.index.min() for i in dfs])
+    date_last = min([i.index.max() for i in dfs])
+    dfs = [i[(i.index >= date_first) & (i.index <= date_last)] for i in dfs]
+    fac_num = len(dfs)
+    ddss = [[] for i in range(fac_num)]
+    for date in tqdm.auto.tqdm(dfs[0].index):
+        dds = sing(dfs, date)
+        for num, i in enumerate(dds):
+            ddss[num].append(i)
+    ds = []
+    for i in tqdm.auto.tqdm(ddss):
+        ds.append(pd.concat(i))
+    return ds
+
+
+def icir_weight(
+    facs: list[pd.DataFrame],
+    backsee: int = 6,
+    boxcox: bool = 0,
+    rank_corr: bool = 0,
+    only_ic: bool = 0,
+) -> pd.DataFrame:
+    """ä½¿ç”¨iciræ»šåŠ¨åŠ æƒçš„æ–¹å¼ï¼ŒåŠ æƒåˆæˆå‡ ä¸ªå› å­
+
+    Parameters
+    ----------
+    facs : list[pd.DataFrame]
+        è¦åˆæˆçš„è‹¥å¹²å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
+    backsee : int, optional
+        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
+    boxcox : bool, optional
+        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
+    rank_corr : bool, optional
+        æ˜¯å¦è®¡ç®—rankicir, by default 0
+    only_ic : bool, optional
+        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0
+
+    Returns
+    -------
+    pd.DataFrame
+        åˆæˆåçš„å› å­
+
+    Raises
+    ------
+    ValueError
+        å› å­æœŸæ•°å°‘äºå›çœ‹æœŸæ•°æ—¶å°†æŠ¥é”™
+    """
+    date_first_max = max([i.index[0] for i in facs])
+    facs = [i[i.index >= date_first_max] for i in facs]
+    date_last_min = min([i.index[-1] for i in facs])
+    facs = [i[i.index <= date_last_min] for i in facs]
+    facs = [i.shift(1) for i in facs]
+    ret = read_daily(
+        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
+    )
+    ret = ret / ret.shift(20) - 1
+    if boxcox:
+        facs = [decap_industry(i) for i in facs]
+    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
+    dates = list(facs[0].index)
+    fis = []
+    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
+        if num < backsee:
+            ...
+        else:
+            nears = [i.iloc[num - backsee : num, :] for i in facs]
+            targets = [i[i.index == date] for i in facs]
+            if rank_corr:
+                weights = [
+                    show_corr(
+                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
+                    )
+                    for i in nears
+                ]
+            else:
+                weights = [
+                    show_corr(
+                        i,
+                        ret[ret.index.isin(i.index)],
+                        plt_plot=0,
+                        show_series=1,
+                        method="pearson",
+                    )
+                    for i in nears
+                ]
+            if only_ic:
+                weights = [i.mean() for i in weights]
+            else:
+                weights = [i.mean() / i.std() for i in weights]
+            fi = sum([i * j for i, j in zip(weights, targets)])
+            fis.append(fi)
+    if len(fis) > 0:
+        return pd.concat(fis).shift(-1)
+    else:
+        raise ValueError("è¾“å…¥çš„å› å­å€¼é•¿åº¦ä¸å¤ªå¤Ÿå§ï¼Ÿ")
+
+
+def scipy_weight(
+    facs: list[pd.DataFrame],
+    backsee: int = 6,
+    boxcox: bool = 0,
+    rank_corr: bool = 0,
+    only_ic: bool = 0,
+    upper_bound: float = None,
+    lower_bound: float = 0,
+) -> pd.DataFrame:
+    """ä½¿ç”¨scipyçš„minimizeä¼˜åŒ–æ±‚è§£çš„æ–¹å¼ï¼Œå¯»æ‰¾æœ€ä¼˜çš„å› å­åˆæˆæƒé‡ï¼Œé»˜è®¤ä¼˜åŒ–æ¡ä»¶ä¸ºæœ€å¤§ICIR
+
+    Parameters
+    ----------
+    facs : list[pd.DataFrame]
+        è¦åˆæˆçš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
+    backsee : int, optional
+        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
+    boxcox : bool, optional
+        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
+    rank_corr : bool, optional
+        æ˜¯å¦è®¡ç®—rankicir, by default 0
+    only_ic : bool, optional
+        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0
+    upper_bound : float, optional
+        æ¯ä¸ªå› å­çš„æƒé‡ä¸Šé™ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œåˆ™ä¸ºæ¯ä¸ªå› å­å¹³å‡æƒé‡çš„2å€ï¼Œå³2é™¤ä»¥å› å­æ•°é‡, by default None
+    lower_bound : float, optional
+        æ¯ä¸ªå› å­çš„æƒé‡ä¸‹é™, by default 0
+
+    Returns
+    -------
+    pd.DataFrame
+        åˆæˆåçš„å› å­
+    """
+    date_first_max = max([i.index[0] for i in facs])
+    facs = [i[i.index >= date_first_max] for i in facs]
+    date_last_min = min([i.index[-1] for i in facs])
+    facs = [i[i.index <= date_last_min] for i in facs]
+    facs = [i.shift(1) for i in facs]
+    ret = read_daily(
+        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
+    )
+    ret = ret / ret.shift(20) - 1
+    if boxcox:
+        facs = [decap_industry(i) for i in facs]
+    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
+    if upper_bound is None:
+        upper_bound = 2 / len(facs)
+    dates = list(facs[0].index)
+    fis = []
+    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
+        if num <= backsee:
+            ...
+        else:
+            nears = [i.iloc[num - backsee : num, :] for i in facs]
+            targets = [i[i.index == date] for i in facs]
+            if rank_corr:
+                weights = [
+                    show_corr(
+                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
+                    )
+                    for i in nears
+                ]
+            else:
+                weights = [
+                    show_corr(
+                        i,
+                        ret[ret.index.isin(i.index)],
+                        plt_plot=0,
+                        show_series=1,
+                        method="pearson",
+                    )
+                    for i in nears
+                ]
+            if only_ic:
+                weights = [i.mean() for i in weights]
+            else:
+                weights = [i.mean() / i.std() for i in weights]
+            weights = pd.concat(weights, axis=1)
+
+            def func(x):
+                w = np.array(x).reshape((-1, 1))
+                y = weights @ w
+                return np.mean(y) / np.std(y)
+
+            cons = {"type": "eq", "fun": lambda x: np.sum(x) - 1}
+            res = minimize(
+                func,
+                np.random.rand(weights.shape[1], 1),
+                constraints=cons,
+                bounds=[(lower_bound, upper_bound)] * weights.shape[1],
+            )
+            xs = res.x.tolist()
+            fac = sum([i * j for i, j in zip(xs, targets)])
+            fis.append(fac)
+    return pd.concat(fis).shift(-1)
+
+
+# æ­¤å¤„æœªå®Œæˆï¼Œå¾…æ”¹å†™
+class pure_fall_second(object):
+    """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""
+
+    def __init__(
+        self,
+        factor_file: str,
+        project: str = None,
+        startdate: int = None,
+        enddate: int = None,
+        questdb_host: str = "127.0.0.1",
+        ignore_history_in_questdb: bool = 0,
+        groupby_target: list = ["date", "code"],
+    ) -> None:
+        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®
+
+        Parameters
+        ----------
+        factor_file : str
+            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
+        project : str, optional
+            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
+        startdate : int, optional
+            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
+        enddate : int, optional
+            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
+        questdb_host: str, optional
+            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
+        ignore_history_in_questdb : bool, optional
+            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
+        groupby_target: list, optional
+            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
+            åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
+        """
+        homeplace = HomePlace()
+        self.groupby_target = groupby_target
+        self.chc = ClickHouseClient("second_data")
+        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
+        pinyin = Pinyin()
+        self.factor_file_pinyin = pinyin.get_pinyin(
+            factor_file.replace(".parquet", ""), ""
+        )
+        self.factor_steps = Questdb(host=questdb_host)
+        if project is not None:
+            if not os.path.exists(homeplace.factor_data_file + project):
+                os.makedirs(homeplace.factor_data_file + project)
+            else:
+                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
+        else:
+            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
+        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
+        if project is not None:
+            factor_file = homeplace.factor_data_file + project + "/" + factor_file
+        else:
+            factor_file = homeplace.factor_data_file + factor_file
+        self.factor_file = factor_file
+        # è¯»å…¥ä¹‹å‰çš„å› å­
+        if os.path.exists(factor_file):
+            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
+            self.factor_old = factor_old
+            # å·²ç»ç®—å¥½çš„æ—¥å­
+            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
+            self.dates_old = dates_old
+        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
+            self.factor_steps.get_data("show tables").table
+        ):
+            logger.info(
+                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
+            )
+            factor_old = self.factor_steps.get_data_with_tuple(
+                f"select * from '{self.factor_file_pinyin}'"
+            ).drop_duplicates(subset=["date", "code"])
+            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
+            factor_old = factor_old.sort_index()
+            self.factor_old = factor_old
+            # å·²ç»ç®—å¥½çš„æ—¥å­
+            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
+            self.dates_old = dates_old
+        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
+            self.factor_steps.get_data("show tables").table
+        ):
+            logger.info(
+                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
+            )
+            factor_old = self.factor_steps.do_order(
+                f"drop table '{self.factor_file_pinyin}'"
+            )
+            self.factor_old = None
+            self.dates_old = []
+            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
+        else:
+            self.factor_old = None
+            self.dates_old = []
+            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
+        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
+        dates_all = self.chc.show_all_dates(f"second_data_stock_10s")
+        dates_all = [int(i) for i in dates_all]
+        if startdate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i >= startdate]
+        if enddate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i <= enddate]
+        self.dates_all = dates_all
+        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
+        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
+        if len(self.dates_new) == 0:
+            ...
+        elif len(self.dates_new) == 1:
+            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
+            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
+        else:
+            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
+            intervals = [[]] * len(dates)
+            interbee = 0
+            intervals[0] = intervals[0] + [dates[0]]
+            for i in range(len(dates) - 1):
+                val1 = dates[i]
+                val2 = dates[i + 1]
+                if val2 - val1 < pd.Timedelta(days=30):
+                    ...
+                else:
+                    interbee = interbee + 1
+                intervals[interbee] = intervals[interbee] + [val2]
+            intervals = [i for i in intervals if len(i) > 0]
+            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
+            for date in intervals:
+                print(f"ä»{date[0]}åˆ°{date[-1]}")
+            self.dates_new_intervals = intervals
+        self.factor_new = []
+
+    def __call__(self) -> pd.DataFrame:
+        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­
+
+        Returns
+        -------
+        `pd.DataFrame`
+            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
+        """
+        return self.factor.copy()
+
+    def forward_dates(self, dates, many_days):
+        dates_index = [self.dates_all.index(i) for i in dates]
+
+        def value(x, a):
+            if x >= 0:
+                return a[x]
+            else:
+                return None
+
+        return [value(i - many_days, self.dates_all) for i in dates_index]
+
+    def select_one_calculate(
+        self,
+        date: pd.Timestamp,
+        func: Callable,
+        fields: str = "*",
+    ) -> None:
+        the_func = partial(func)
+        if not isinstance(date, int):
+            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
+        # å¼€å§‹è®¡ç®—å› å­å€¼
+
+        sql_order = f"select {fields} from second_data.second_data_stock_10s where toYYYYMMDD(date)=date order by code,date"
+        df = self.chc.get_data(sql_order)
+        df = ((df.set_index(["code", "date"])) / 100).reset_index()
+        df = df.groupby(self.groupby_target).apply(the_func)
+        if self.groupby_target == ["date", "code"]:
+            df = df.to_frame("fac").reset_index()
+            df.columns = ["date", "code", "fac"]
+        else:
+            df = df.reset_index()
+        if (df is not None) and (df.shape[0] > 0):
+            df = df.pivot(columns="code", index="date", values="fac")
+            df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+            to_save = df.stack().reset_index()
+            to_save.columns = ["date", "code", "fac"]
+            self.factor_steps.write_via_df(
+                to_save, self.factor_file_pinyin, tuple_col="fac"
+            )
+            return df
+
+    def select_many_calculate(
+        self,
+        dates: List[pd.Timestamp],
+        func: Callable,
+        fields: str = "*",
+        chunksize: int = 10,
+        many_days: int = 1,
+        n_jobs: int = 1,
+    ) -> None:
+        the_func = partial(func)
+        factor_new = []
+        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
+        if many_days == 1:
+            # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
+            dates_new_len = len(dates)
+            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
+            if cut_points[-1] == cut_points[-2]:
+                cut_points = cut_points[:-1]
+            cuts = tuple(zip(cut_points[:-many_days], cut_points[many_days:]))
+            df_first = self.select_one_calculate(
+                date=dates[0],
+                func=func,
+                fields=fields,
+            )
+            factor_new.append(df_first)
+
+            def cal_one(date1, date2):
+                if self.clickhouse == 1:
+                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
+                else:
+                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]} order by code,date,num"
+
+                df = self.chc.get_data(sql_order)
+                if self.clickhouse == 1:
+                    df = ((df.set_index("code")) / 100).reset_index()
+                else:
+                    df.num = df.num.astype(int)
+                    df.date = df.date.astype(int)
+                    df = df.sort_values(["date", "num"])
+                df = df.groupby(self.groupby_target).apply(the_func)
+                if self.groupby_target == ["date", "code"]:
+                    df = df.to_frame("fac").reset_index()
+                    df.columns = ["date", "code", "fac"]
+                else:
+                    df = df.reset_index()
+                df = df.pivot(columns="code", index="date", values="fac")
+                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                to_save = df.stack().reset_index()
+                to_save.columns = ["date", "code", "fac"]
+                self.factor_steps.write_via_df(
+                    to_save, self.factor_file_pinyin, tuple_col="fac"
+                )
+                return df
+
+            if n_jobs > 1:
+                with WorkerPool(n_jobs=n_jobs) as pool:
+                    factor_new_more = pool.map(cal_one, cuts, progress_bar=True)
+                factor_new = factor_new + factor_new_more
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in tqdm.auto.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
+                    df = cal_one(date1, date2)
+                    factor_new.append(df)
+        else:
+
+            def cal_two(date1, date2):
+                if date1 is not None:
+                    if self.clickhouse == 1:
+                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{date1*100} and date<={date2*100} order by code,date,num"
+                    else:
+                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{date1} and cast(date as int)<={date2} order by code,date,num"
+
+                    df = self.chc.get_data(sql_order)
+                    if self.clickhouse == 1:
+                        df = ((df.set_index("code")) / 100).reset_index()
+                    else:
+                        df.num = df.num.astype(int)
+                        df.date = df.date.astype(int)
+                        df = df.sort_values(["date", "num"])
+                    if self.groupby_target == [
+                        "date",
+                        "code",
+                    ] or self.groupby_target == ["code"]:
+                        df = df.groupby(["code"]).apply(the_func).reset_index()
+                    else:
+                        df = the_func(df)
+                    df = df.assign(date=date2)
+                    df.columns = ["code", "fac", "date"]
+                    df = df.pivot(columns="code", index="date", values="fac")
+                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
+                    to_save = df.stack().reset_index()
+                    to_save.columns = ["date", "code", "fac"]
+                    self.factor_steps.write_via_df(
+                        to_save, self.factor_file_pinyin, tuple_col="fac"
+                    )
+                    return df
+
+            pairs = self.forward_dates(dates, many_days=many_days)
+            cuts2 = tuple(zip(pairs, dates))
+            if n_jobs > 1:
+                with WorkerPool(n_jobs=n_jobs) as pool:
+                    factor_new_more = pool.map(cal_two, cuts2, progress_bar=True)
+                factor_new = factor_new + factor_new_more
+            else:
+                # å¼€å§‹è®¡ç®—å› å­å€¼
+                for date1, date2 in tqdm.auto.tqdm(cuts2, desc="çŸ¥ä¸å¯ä¹éª¤å¾—ï¼Œæ‰˜é—å“äºæ‚²é£ã€‚"):
+                    df = cal_two(date1, date2)
+                    factor_new.append(df)
+
+        if len(factor_new) > 0:
+            factor_new = pd.concat(factor_new)
+            return factor_new
+        else:
+            return None
+
+    def select_any_calculate(
+        self,
+        dates: List[pd.Timestamp],
+        func: Callable,
+        fields: str = "*",
+        chunksize: int = 10,
+        show_time: bool = 0,
+        many_days: int = 1,
+        n_jobs: int = 1,
+    ) -> None:
+        if len(dates) == 1 and many_days == 1:
+            res = self.select_one_calculate(
+                dates[0],
+                func=func,
+                fields=fields,
+                show_time=show_time,
+            )
+        else:
+            res = self.select_many_calculate(
+                dates=dates,
+                func=func,
+                fields=fields,
+                chunksize=chunksize,
+                show_time=show_time,
+                many_days=many_days,
+                n_jobs=n_jobs,
+            )
+        if res is not None:
+            self.factor_new.append(res)
+        return res
+
+    @staticmethod
+    def for_cross_via_str(func):
+        """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
+        ä¾‹å¦‚
+        ```python
+        return [[0.11,0.24,0.55],[2.59,1.99,0.43],[1.32,8.88,7.77]â€¦â€¦]
+        ```
+        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸‰ä¸ªå› å­å€¼ï¼Œé‡Œå±‚çš„listæŒ‰ç…§è‚¡ç¥¨ä»£ç é¡ºåºæ’åˆ—"""
+
+        def full_run(df, *args, **kwargs):
+            codes = sorted(list(set(df.code)))
+            res = func(df, *args, **kwargs)
+            if isinstance(res[0], list):
+                kind = 1
+                res = [",".join(i) for i in res]
+            else:
+                kind = 0
+            df = pd.DataFrame({"code": codes, "fac": res})
+            if kind:
+                df.fac = df.fac.apply(lambda x: [float(i) for i in x.split(",")])
+            return df
+
+        return full_run
+
+    @staticmethod
+    def for_cross_via_zip(func):
+        """è¿”å›å€¼ä¸ºå¤šä¸ªpd.Seriesï¼Œæ¯ä¸ªpd.Seriesçš„indexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå•ä¸ªå› å­å€¼
+        ä¾‹å¦‚
+        ```python
+        return (
+                    pd.Series([1.54,8.77,9.99â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
+                    pd.Series([3.54,6.98,9.01â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
+                )
+        ```
+        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
+        """
+
+        def full_run(df, *args, **kwargs):
+            res = func(df, *args, **kwargs)
+            if isinstance(res, pd.Series):
+                res = res.reset_index()
+                res.columns = ["code", "fac"]
+                return res
+            elif isinstance(res, pd.DataFrame):
+                res.columns = [f"fac{i}" for i in range(len(res.columns))]
+                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
+                res = res[["fac"]].reset_index()
+                res.columns = ["code", "fac"]
+                return res
+            elif res is None:
+                ...
+            else:
+                res = pd.concat(res, axis=1)
+                res.columns = [f"fac{i}" for i in range(len(res.columns))]
+                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
+                res = res[["fac"]].reset_index()
+                res.columns = ["code", "fac"]
+                return res
+
+        return full_run
+
+    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
+    def get_daily_factors(
+        self,
+        func: Callable,
+        fields: str = "*",
+        chunksize: int = 10,
+        show_time: bool = 0,
+        many_days: int = 1,
+        n_jobs: int = 1,
+    ) -> None:
+        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
+        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼
+
+        Parameters
+        ----------
+        func : Callable
+            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
+        fields : str, optional
+            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
+            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
+        chunksize : int, optional
+            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
+        show_time : bool, optional
+            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
+        many_days : int, optional
+            è®¡ç®—æŸå¤©çš„å› å­å€¼æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¹‹å‰å¤šå°‘å¤©çš„æ•°æ®
+        n_jobs : int, optional
+            å¹¶è¡Œæ•°é‡ï¼Œä¸å»ºè®®è®¾ç½®ä¸ºå¤§äº2çš„æ•°ï¼Œæ­¤å¤–å½“æ­¤å‚æ•°å¤§äº1æ—¶ï¼Œè¯·ä½¿ç”¨questdbæ•°æ®åº“æ¥è¯»å–åˆ†é’Ÿæ•°æ®, by default 1
+        """
+        if len(self.dates_new) > 0:
+            for interval in self.dates_new_intervals:
+                df = self.select_any_calculate(
+                    dates=interval,
+                    func=func,
+                    fields=fields,
+                    chunksize=chunksize,
+                    show_time=show_time,
+                    many_days=many_days,
+                    n_jobs=n_jobs,
+                )
+            self.factor_new = pd.concat(self.factor_new)
+            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
+            self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            # å­˜å…¥æœ¬åœ°
+            self.factor.to_parquet(self.factor_file)
+            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
+            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
+            try:
+                self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+                logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
+            except Exception:
+                logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")
+
+        else:
+            self.factor = drop_duplicates_index(self.factor_old)
+            # å­˜å…¥æœ¬åœ°
+            self.factor.to_parquet(self.factor_file)
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
+
+    def drop_table(self):
+        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
+        try:
+            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
+        except Exception:
+            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")
+
+
+class pure_fall_nature:
+    def __init__(
+        self,
+        factor_file: str,
+        project: str = None,
+        startdate: int = None,
+        enddate: int = None,
+        questdb_host: str = "127.0.0.1",
+        ignore_history_in_questdb: bool = 0,
+        groupby_code: bool = 1,
+    ) -> None:
+        """åŸºäºè‚¡ç¥¨é€ç¬”æ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®
+
+        Parameters
+        ----------
+        factor_file : str
+            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
+        project : str, optional
+            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
+        startdate : int, optional
+            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
+        enddate : int, optional
+            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
+        questdb_host: str, optional
+            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
+        ignore_history_in_questdb : bool, optional
+            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
+        groupby_target: list, optional
+            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®, by default ['code']
+        """
+        homeplace = HomePlace()
+        self.groupby_code = groupby_code
+        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
+        pinyin = Pinyin()
+        self.factor_file_pinyin = pinyin.get_pinyin(
+            factor_file.replace(".parquet", ""), ""
+        )
+        self.factor_steps = Questdb(host=questdb_host)
+        if project is not None:
+            if not os.path.exists(homeplace.factor_data_file + project):
+                os.makedirs(homeplace.factor_data_file + project)
+            else:
+                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
+        else:
+            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
+        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
+        if project is not None:
+            factor_file = homeplace.factor_data_file + project + "/" + factor_file
+        else:
+            factor_file = homeplace.factor_data_file + factor_file
+        self.factor_file = factor_file
+        # è¯»å…¥ä¹‹å‰çš„å› å­
+        if os.path.exists(factor_file):
+            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
+            self.factor_old = factor_old
+            # å·²ç»ç®—å¥½çš„æ—¥å­
+            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
+            self.dates_old = dates_old
+        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
+            self.factor_steps.get_data("show tables").table
+        ):
+            logger.info(
+                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
+            )
+            factor_old = self.factor_steps.get_data_with_tuple(
+                f"select * from '{self.factor_file_pinyin}'"
+            ).drop_duplicates(subset=["date", "code"])
+            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
+            factor_old = factor_old.sort_index()
+            self.factor_old = factor_old
+            # å·²ç»ç®—å¥½çš„æ—¥å­
+            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
+            self.dates_old = dates_old
+        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
+            self.factor_steps.get_data("show tables").table
+        ):
+            logger.info(
+                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
+            )
+            factor_old = self.factor_steps.do_order(
+                f"drop table '{self.factor_file_pinyin}'"
+            )
+            self.factor_old = None
+            self.dates_old = []
+            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
+        else:
+            self.factor_old = None
+            self.dates_old = []
+            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
+        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
+        dates_all = os.listdir(homeplace.tick_by_tick_data)
+        dates_all = [i.split(".")[0] for i in dates_all if i.endswith(".parquet")]
+        dates_all = [i.replace("-", "") for i in dates_all]
+        dates_all = [int(i) for i in dates_all if "20" if i]
+        if startdate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i >= startdate]
+        if enddate is None:
+            ...
+        else:
+            dates_all = [i for i in dates_all if i <= enddate]
+        self.dates_all = dates_all
+        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
+        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
+        if len(self.dates_new) == 0:
+            ...
+        elif len(self.dates_new) == 1:
+            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
+            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
+        else:
+            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
+            intervals = [[]] * len(dates)
+            interbee = 0
+            intervals[0] = intervals[0] + [dates[0]]
+            for i in range(len(dates) - 1):
+                val1 = dates[i]
+                val2 = dates[i + 1]
+                if val2 - val1 < pd.Timedelta(days=30):
+                    ...
+                else:
+                    interbee = interbee + 1
+                intervals[interbee] = intervals[interbee] + [val2]
+            intervals = [i for i in intervals if len(i) > 0]
+            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
+            for date in intervals:
+                print(f"ä»{date[0]}åˆ°{date[-1]}")
+            self.dates_new_intervals = intervals
+        self.factor_new = []
+        self.age = read_daily(age=1)
+        self.state = read_daily(state=1)
+        self.closes_unadj = read_daily(close=1, unadjust=1).shift(1)
+
+    def __call__(self) -> pd.DataFrame:
+        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­
+
+        Returns
+        -------
+        `pd.DataFrame`
+            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
+        """
+        return self.factor.copy()
+
+    def select_one_calculate(
+        self,
+        date: pd.Timestamp,
+        func: Callable,
+        resample_frequency: str = None,
+        opens_in: bool = 0,
+        highs_in: bool = 0,
+        lows_in: bool = 0,
+        moneys_in: bool = 0,
+        merge_them: bool = 0,
+    ) -> None:
+        the_func = partial(func)
+        if not isinstance(date, int):
+            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
+        # å¼€å§‹è®¡ç®—å› å­å€¼
+        df = pd.read_parquet(
+            homeplace.tick_by_tick_data
+            + str(date)[:4]
+            + "-"
+            + str(date)[4:6]
+            + "-"
+            + str(date)[6:]
+            + ".parquet"
+        )
+        date = df.date.iloc[0]
+        date0 = pd.Timestamp(year=date.year, month=date.month, day=date.day)
+        age_here = self.age.loc[pd.Timestamp(pd.Timestamp(df.date.iloc[0]).date())]
+        age_here = age_here.where(age_here > 180, np.nan).dropna()
+        state_here = self.state.loc[pd.Timestamp(pd.Timestamp(df.date.iloc[0]).date())]
+        state_here = state_here.where(state_here > 0, np.nan).dropna()
+        df = df[df.code.isin(age_here.index)]
+        df = df[df.code.isin(state_here.index)]
+
+        if resample_frequency is not None:
+            date = df.date.iloc[0]
+            date0 = pd.Timestamp(year=date.year, month=date.month, day=date.day)
+            head = self.closes_unadj.loc[date0].to_frame("head_temp").T
+            df = df[df.code.isin(head.columns)]
+            price = df.drop_duplicates(subset=["code", "date"], keep="last").pivot(
+                index="date", columns="code", values="price"
+            )
+            closes = price.resample(resample_frequency).last()
+            head = head[[i for i in head.columns if i in closes.columns]]
+            price = pd.concat([head, closes])
+            closes = closes.ffill().iloc[1:, :]
+            self.closes = closes
+            names = []
+
+            if opens_in:
+                price = df.drop_duplicates(subset=["code", "date"], keep="first").pivot(
+                    index="date", columns="code", values="price"
+                )
+                opens = price.resample(resample_frequency).first()
+                opens = np.isnan(opens).replace(True, 1).replace(
+                    False, 0
+                ) * closes.shift(1) + opens.fillna(0)
+                self.opens = opens
+                names.append("open")
+            else:
+                self.opens = None
+
+            if highs_in:
+                price = (
+                    df.sort_values(["code", "date", "price"])
+                    .drop_duplicates(subset=["code", "date"], keep="last")
+                    .pivot(index="date", columns="code", values="price")
+                )
+                highs = price.resample(resample_frequency).max()
+                highs = np.isnan(highs).replace(True, 1).replace(
+                    False, 0
+                ) * closes.shift(1) + highs.fillna(0)
+                self.highs = highs
+                names.append("high")
+            else:
+                self.highs = None
+
+            if lows_in:
+                price = (
+                    df.sort_values(["code", "date", "price"])
+                    .drop_duplicates(subset=["code", "date"], keep="first")
+                    .pivot(index="date", columns="code", values="price")
+                )
+                lows = price.resample(resample_frequency).min()
+                lows = np.isnan(lows).replace(True, 1).replace(False, 0) * closes.shift(
+                    1
+                ) + lows.fillna(0)
+                self.lows = lows
+                names.append("low")
+            else:
+                self.low = None
+
+            names.append("close")
+            if moneys_in:
+                moneys = df.groupby(["code", "date"]).money.sum().reset_index()
+                moneys = moneys.pivot(index="date", columns="code", values="money")
+                moneys = moneys.resample(resample_frequency).sum().fillna(0)
+                self.moneys = moneys
+                names.append("money")
+            else:
+                self.moneys = None
+
+            if merge_them:
+                self.data = merge_many(
+                    [
+                        i
+                        for i in [
+                            self.opens,
+                            self.highs,
+                            self.lows,
+                            self.closes,
+                            self.moneys,
+                        ]
+                        if i is not None
+                    ],
+                    names,
+                )
+
+        if self.groupby_code:
+            df = df.groupby(["code"]).apply(the_func)
+        else:
+            df = the_func(df)
+            if isinstance(df, pd.DataFrame):
+                df.columns = [f"fac{i}" for i in range(len(df.columns))]
+                df = df.assign(fac=list(zip(*[df[i] for i in list(df.columns)])))
+                df = df[["fac"]]
+            elif isinstance(df, list) or isinstance(df, tuple):
+                df = pd.concat(list(df), axis=1)
+                df.columns = [f"fac{i}" for i in range(len(df.columns))]
+                df = df.assign(fac=list(zip(*[df[i] for i in list(df.columns)])))
+                df = df[["fac"]]
+        df = df.reset_index()
+        df.columns = ["code", "fac"]
+        df.insert(
+            0, "date", pd.Timestamp(year=date.year, month=date.month, day=date.day)
+        )
+        if (df is not None) and (df.shape[0] > 0):
+            self.factor_steps.write_via_df(df, self.factor_file_pinyin, tuple_col="fac")
+            df = df.pivot(columns="code", index="date", values="fac")
+            return df
+
+    @kk.desktop_sender(title="é“›é“›ï¼Œé€ç¬”æ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
+    def get_daily_factors(
+        self,
+        func: Callable,
+        n_jobs: int = 1,
+        resample_frequency: str = None,
+        opens_in: bool = 0,
+        highs_in: bool = 0,
+        lows_in: bool = 0,
+        moneys_in: bool = 0,
+        merge_them: bool = 0,
+    ) -> None:
+        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
+        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼
+
+        Parameters
+        ----------
+        func : Callable
+            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
+        n_jobs : int, optional
+            å¹¶è¡Œæ•°é‡, by default 1
+        resample_frequency : str, optional
+            å°†é€ç¬”æ•°æ®è½¬åŒ–ä¸ºç§’çº§æˆ–åˆ†é’Ÿé¢‘æ•°æ®ï¼Œå¯ä»¥å¡«å†™è¦è½¬åŒ–çš„é¢‘ç‡ï¼Œå¦‚'3s'ï¼ˆ3ç§’æ•°æ®ï¼‰ï¼Œ'1m'ï¼ˆ1åˆ†é’Ÿæ•°æ®ï¼‰ï¼Œ
+            æŒ‡å®šæ­¤å‚æ•°åï¼Œå°†è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªself.closesçš„æ”¶ç›˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæ”¶ç›˜ä»·)ï¼Œ
+            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.closes`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼, by default None
+        opens_in : bool, optional
+            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½å¼€ç›˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºå¼€ç›˜ä»·)ï¼Œ
+            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.opens`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
+        highs_in : bool, optional
+            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æœ€é«˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæœ€é«˜ä»·)ï¼Œ
+            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.highs`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
+        lows_in : bool, optional
+            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æœ€ä½ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæœ€ä½ä»·)ï¼Œ
+            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.lows`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
+        moneys_in : bool, optional
+            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æˆäº¤é¢çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæˆäº¤é¢)ï¼Œ
+            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.moneys`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
+        merge_them : bool, optional
+            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œå°†è®¡ç®—å¥½çš„å› å­å€¼åˆå¹¶åˆ°ä¸€èµ·ï¼Œç”Ÿæˆç±»ä¼¼äºåˆ†é’Ÿæ•°æ®çš„sqlå½¢å¼ï¼Œby default 0
+        """
+        if len(self.dates_new) > 0:
+            if n_jobs > 1:
+                with WorkerPool(n_jobs=n_jobs) as pool:
+                    self.factor_new = pool.map(
+                        lambda x: self.select_one_calculate(
+                            date=x,
+                            func=func,
+                            resample_frequency=resample_frequency,
+                            opens_in=opens_in,
+                            highs_in=highs_in,
+                            lows_in=lows_in,
+                            moneys_in=moneys_in,
+                            merge_them=merge_them,
+                        ),
+                        self.dates_new,
+                        progress_bar=True,
+                    )
+            else:
+                for date in tqdm.auto.tqdm(self.dates_new, "æ‚¨ç°åœ¨å¤„äºå•æ ¸è¿ç®—çŠ¶æ€ï¼Œå»ºè®®ä»…åœ¨è°ƒè¯•æ—¶ä½¿ç”¨å•æ ¸"):
+                    df = self.select_one_calculate(
+                        date=date,
+                        func=func,
+                        resample_frequency=resample_frequency,
+                        opens_in=opens_in,
+                        highs_in=highs_in,
+                        lows_in=lows_in,
+                        moneys_in=moneys_in,
+                        merge_them=merge_them,
+                    )
+                    self.factor_new.append(df)
+            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
+            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
+            self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            # å­˜å…¥æœ¬åœ°
+            self.factor.to_parquet(self.factor_file)
+            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
+            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
+            try:
+                self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+                logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
+            except Exception:
+                logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")
+
+        else:
+            self.factor = drop_duplicates_index(self.factor_old)
+            # å­˜å…¥æœ¬åœ°
+            self.factor.to_parquet(self.factor_file)
+            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
+            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")
+
+    def drop_table(self):
+        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
+        try:
+            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
+            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
+        except Exception:
+            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/mail/email.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/mail/email.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/decorators.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/decorators.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/homeplace.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/homeplace.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/state/states.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/state/states.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/legacy_version/v3p4/withs/requires.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/legacy_version/v3p4/withs/requires.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/mail/email.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/mail/email.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/decorators.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/decorators.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/homeplace.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/homeplace.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,36 +1,38 @@
 """
 åˆå§‹åŒ–æ—¶ä¿å­˜çš„è·¯å¾„
 """
 
-__updated__ = "2023-05-11 22:01:50"
+__updated__ = "2023-06-26 10:01:23"
 
 import os
 import pickle
 
 
 class HomePlace(object):
     """
     ```
     daily_data_file: æ—¥é¢‘æ•°æ®å­˜æ”¾ä½ç½®
     factor_data_file: ï¼ˆè¾…åŠ©ã€åˆçº§ï¼‰å› å­æ•°æ®å­˜æ”¾ä½ç½®
     barra_data_file: åç§å¸¸ç”¨é£æ ¼å› å­çš„å­˜æ”¾ä½ç½®
     update_data_file: æ›´æ–°è¾…åŠ©æ•°æ®çš„å­˜æ”¾ä½ç½®
     api_token: dcubeçš„api
     final_factor_file: æœ€ç»ˆå› å­æ•°æ®çš„å­˜æ”¾ä½ç½®
+    tick_by_tick_data: è‚¡ç¥¨é€ç¬”æ•°æ®çš„å­˜æ”¾ä½ç½®
     ```
     """
 
     __slots__ = [
         "daily_data_file",
         "factor_data_file",
         "barra_data_file",
         "update_data_file",
         "api_token",
         "final_factor_file",
+        'tick_by_tick_data',
     ]
 
     def __init__(self):
         user_file = os.path.expanduser("~") + "/"
         path_file = open(user_file + "paths.settings", "rb")
         paths = pickle.load(path_file)
         for k in self.__slots__:
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/state/states.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/state/states.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze/withs/requires.py` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze/withs/requires.py`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/PKG-INFO` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,47 @@
 Metadata-Version: 2.1
 Name: pure-ocean-breeze
-Version: 3.9.9
+Version: 4.0.0
 Summary: stock factor test
 Home-page: https://github.com/chen-001/pure_ocean_breeze.git
 Author: chenzongwei
 Author-email: winterwinter999@163.com
 License: MIT
 Project-URL: Documentation, https://chen-001.github.io/pure_ocean_breeze/
 Requires-Python: >=3
 Description-Content-Type: text/markdown
 Provides-Extra: windows
 Provides-Extra: macos
 Provides-Extra: linux
 License-File: LICENSE
 
 # pure_ocean_breeze 
-#### **ä¼—äººçš„å› å­å›æµ‹æ¡†æ¶**
+#### **ä¼—äººçš„å› å­æ¡†æ¶**
 ##### æˆ‘ä»¬çš„å£å·æ˜¯ï¼šé‡ä»·å› å­æ‰æ˜¯æœ€ç‰›çš„ï¼
 ***
 
 ### å…¨æ–°å¤§ç‰ˆæœ¬ğŸ“¢
+* v4.0.0 â€” 2023.06.28
+> å› å­æ¡†æ¶4.0ç‰ˆæœ¬æ¥å•¦ï¼é€ç¬”æ•°æ®&ä»»æ„ç§’çº§æ•°æ®æ¥å•¦ï¼
+
 * v3.0.0 â€” 2022.08.16
+
 >å›æµ‹æ¡†æ¶3.0ç‰ˆæœ¬æ¥å•¦ï¼ æ¨¡å—æ‹†åˆ†&è¯´æ˜æ–‡æ¡£æ¥å•¦ï¼[pure_ocean_breezeè¯´æ˜æ–‡æ¡£](https://chen-001.github.io/pure_ocean_breeze/)
 * v2.0.0 â€” 2022.07.12
 >å›æµ‹æ¡†æ¶2.0ç‰ˆæœ¬æ¥å•¦ï¼æ•°æ®åº“&è‡ªåŠ¨æ›´æ–°&æœ€ç»ˆå› å­åº“åŠŸèƒ½ä¸Šçº¿å•¦ï¼
 
 ### å®‰è£…&ä½¿ç”¨æŒ‡å—ğŸ¯
 1. å®‰è£…
 > ä½¿ç”¨`pip install pure_ocean_breeze`å‘½ä»¤è¿›è¡Œå®‰è£…
 2. åˆå§‹åŒ–
 >* åœ¨åˆæ¬¡å®‰è£…æ¡†æ¶æ—¶ï¼Œè¯·è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥å°†è·¯å¾„è®¾ç½®åˆ°è‡ªå·±çš„æ–‡ä»¶é‡Œ
 >* ä½¿ç”¨å¦‚ä¸‹è¯­å¥è¿›è¡Œåˆå§‹åŒ–
 >>```python
->>import pure_ocean_breeze.initialize.initialize
->>pure_ocean_breeze.initialize.initialize.initialize()
+>>import pure_ocean_breeze as p
+>>p.ini()
 >>```
 >* ç„¶åæ ¹æ®æç¤ºè¿›è¡Œæ“ä½œå³å¯
 >* è¯·æ³¨æ„è·¯å¾„ä¸è¦å†™åæ–œæ \ï¼Œè€Œè¦å†™æˆ/
 >* ç»è¿‡åˆå§‹åŒ–åï¼Œä»¥åå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸è®ºé‡å¯ç”µè„‘æˆ–è€…ç‰ˆæœ¬å‡çº§ï¼Œéƒ½ä¸ç”¨å†åˆå§‹åŒ–
 >* å¦‚æœæ›´æ¢äº†æ•°æ®åº“è·¯å¾„ï¼Œè¯·é‡æ–°åˆå§‹åŒ–
 3. æ—¥å¸¸è°ƒç”¨
 >* **å¯¼å…¥æ¡†æ¶**
```

### Comparing `pure_ocean_breeze-3.9.9/pure_ocean_breeze.egg-info/SOURCES.txt` & `pure_ocean_breeze-4.0.0/pure_ocean_breeze.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `pure_ocean_breeze-3.9.9/setup.py` & `pure_ocean_breeze-4.0.0/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-__updated__ = "2023-03-23 19:10:06"
+__updated__ = "2023-06-28 13:20:31"
 
 from setuptools import setup
 import setuptools
 import re
 import os
 import sys
 
@@ -13,15 +13,15 @@
 def get_version(package):
     """Return package version as listed in `__version__` in `init.py`."""
     init_py = open(os.path.join(package, "__init__.py")).read()
     return re.search("__version__ = ['\"]([^'\"]+)['\"]", init_py).group(1)
 
 install_requires=[
     "numpy",
-    "pandas",
+    "pandas<=1.5.3",
     "scipy",
     "statsmodels",
     "plotly",
     "matplotlib",
     "pyarrow",
     "loguru",
     "knockknock",
```

